{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b7bebf",
   "metadata": {},
   "source": [
    "# dwt 적용해서 feature 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9dc8080f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "717a7f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded mACStatus with shape (939896, 3)\n",
      "Loaded mScreenStatus with shape (939653, 3)\n",
      "Loaded mUsageStats with shape (45197, 3)\n",
      "Loaded mActivity with shape (961062, 3)\n",
      "Loaded mBle with shape (21830, 3)\n",
      "Loaded mWifi with shape (76336, 3)\n",
      "Loaded wHr with shape (382918, 3)\n",
      "Loaded wPedo with shape (748100, 9)\n",
      "Loaded mGps with shape (800611, 3)\n",
      "Loaded mLight with shape (96258, 3)\n",
      "Loaded wLight with shape (633741, 3)\n",
      "Loaded mAmbience with shape (476577, 3)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path for data folder\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# Main data item list\n",
    "file_names = [\n",
    "    \"mACStatus\", \"mScreenStatus\", \"mUsageStats\", \"mActivity\", \"mBle\", \"mWifi\",\n",
    "    \"wHr\", \"wPedo\", \"mGps\", \"mLight\",\"wLight\", \"mAmbience\"\n",
    "]\n",
    "\n",
    "data_files = {name: os.path.join(DATA_DIR, f\"ch2025_{name}.parquet\") for name in file_names}\n",
    "\n",
    "dfs = {}\n",
    "for name, file_path in data_files.items():\n",
    "    dfs[name] = pd.read_parquet(file_path)\n",
    "    globals()[name] = dfs[name]\n",
    "    print(f\"Loaded {name} with shape {dfs[name].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7a27c8",
   "metadata": {},
   "source": [
    "### 일단 간단하게 wLight dwt 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89c5aeb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyWavelets in /home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /home/wonjun/.local/lib/python3.10/site-packages (from PyWavelets) (1.24.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "335399a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   level_0_mean  level_0_std  level_0_energy  level_1_mean  level_1_std  \\\n",
      "0   1896.855697  3468.180769    1.050090e+10    -77.191514  2563.281570   \n",
      "1   1269.319542  3708.348697    2.020237e+10    -58.556863  2071.915189   \n",
      "2   1249.935703  3211.219235    1.123306e+10    -44.914753  1711.498931   \n",
      "3   1562.229941  3980.721287    1.777468e+10     59.418617  1897.019771   \n",
      "4   2187.230936  5155.305936    2.552798e+10    -18.043144  2808.941594   \n",
      "\n",
      "   level_1_energy  level_2_mean  level_2_std  level_2_energy  level_3_mean  \\\n",
      "0    4.419321e+09      0.685512  2250.379328    6.770846e+09      2.734507   \n",
      "1    5.649584e+09     28.815372  1420.826885    5.299376e+09    -23.675415   \n",
      "2    2.772959e+09     60.081667  1552.879161    4.552357e+09      8.183103   \n",
      "3    3.501353e+09     68.298851  1990.576473    7.684194e+09     32.763466   \n",
      "4    6.422849e+09     45.653551  2561.705812    1.064093e+10     26.037351   \n",
      "\n",
      "   ...  level_4_mean  level_4_std  level_4_energy  level_5_mean  level_5_std  \\\n",
      "0  ...     27.009144  1827.365777    1.780217e+10    -23.145428  1387.797640   \n",
      "1  ...      4.460262   946.632224    9.389676e+09     17.452703   990.190643   \n",
      "2  ...    -14.830765   981.225032    7.243855e+09      4.752362  1002.550965   \n",
      "3  ...     16.606641  1835.694274    2.604374e+10     -9.547962  1620.369074   \n",
      "4  ...     26.738470  1861.759014    2.240633e+10      1.634845  1735.352754   \n",
      "\n",
      "   level_5_energy  level_6_mean  level_6_std  level_6_energy  subject_id  \n",
      "0    2.052320e+10     10.487185  1363.602799    3.960783e+10        id01  \n",
      "1    2.054640e+10      6.760259   824.548545    2.848346e+10        id02  \n",
      "2    1.511416e+10     -5.983526   843.428638    2.139061e+10        id03  \n",
      "3    4.056424e+10    -10.185328  1355.992373    5.680480e+10        id04  \n",
      "4    3.890796e+10     -5.030875  1625.237011    6.823582e+10        id05  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "(10, 22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108030/4131239235.py:21: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  values = grp['w_light'].fillna(method='ffill').fillna(0).values\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "# ensure timestamp is datetime and sort by subject and time\n",
    "wLight['timestamp'] = pd.to_datetime(wLight['timestamp'])\n",
    "wLight = wLight.sort_values(['subject_id', 'timestamp'])\n",
    "\n",
    "# function to extract DWT features from a 1D array\n",
    "def extract_dwt_features(signal, wavelet='db4', level=6):\n",
    "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "    feats = {}\n",
    "    # for each level (approximation + details) compute mean, std, energy\n",
    "    for idx, c in enumerate(coeffs):\n",
    "        feats[f'level_{idx}_mean']   = np.mean(c)\n",
    "        feats[f'level_{idx}_std']    = np.std(c)\n",
    "        feats[f'level_{idx}_energy'] = np.sum(c**2)\n",
    "    return feats\n",
    "\n",
    "# group by subject, fill gaps, extract features\n",
    "feature_rows = []\n",
    "for subj, grp in wLight.groupby('subject_id'):\n",
    "    values = grp['w_light'].fillna(method='ffill').fillna(0).values\n",
    "    row = extract_dwt_features(values)\n",
    "    row['subject_id'] = subj\n",
    "    feature_rows.append(row)\n",
    "\n",
    "# assemble into DataFrame\n",
    "features_df = pd.DataFrame(feature_rows)\n",
    "print(features_df.head())\n",
    "print(features_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdda3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id           timestamp  w_light\n",
      "0       id01 2024-06-26 12:17:00    633.0\n",
      "1       id01 2024-06-26 12:18:00    483.0\n",
      "2       id01 2024-06-26 12:19:00    541.0\n",
      "3       id01 2024-06-26 12:20:00    547.0\n",
      "4       id01 2024-06-26 12:21:00    547.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108030/380081244.py:28: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  .resample('1T').mean()\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 6 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   level_0_mean  level_0_std  level_0_energy  level_1_mean  level_1_std  \\\n",
      "0   2319.716888  1993.061284    1.590075e+08    220.529958   857.949949   \n",
      "1   1487.508117  3064.055081    3.364323e+08   -380.241503  2156.959740   \n",
      "2   2043.725484  4345.636720    6.687798e+08    496.916154  3143.140541   \n",
      "3    443.503950   762.056177    2.176791e+07    -15.674754   227.526787   \n",
      "4    643.657139  1392.467875    6.824458e+07     58.644312   399.099776   \n",
      "\n",
      "   level_1_energy  level_2_mean  level_2_std  level_2_energy  level_3_mean  \\\n",
      "0    1.334010e+07   -136.793320   868.805762    2.165900e+07   -105.754601   \n",
      "1    1.391147e+08    238.336705  1913.642816    1.896605e+08   -102.318272   \n",
      "2    2.936615e+08    626.101028  3516.648909    6.506999e+08   -266.706257   \n",
      "3    1.456396e+06     -5.185987    99.312189    4.944903e+05     -8.925586   \n",
      "4    4.718874e+06      8.353223   630.773780    2.029521e+07    -14.666764   \n",
      "\n",
      "   ...  level_4_std  level_4_energy  level_5_mean  level_5_std  \\\n",
      "0  ...   356.326837    1.194304e+07     52.702714   833.287429   \n",
      "1  ...   809.842478    1.236615e+08    -14.868727   522.140233   \n",
      "2  ...  3133.601351    1.799755e+09    134.645538  2668.841341   \n",
      "3  ...    58.401257    6.191852e+05     -1.593556    32.170706   \n",
      "4  ...   277.952579    1.447844e+07     -9.967894   292.133223   \n",
      "\n",
      "   level_5_energy  level_6_mean  level_6_std  level_6_energy  subject_id  \\\n",
      "0    1.261833e+08     58.040194   824.356485    2.424410e+08        id01   \n",
      "1    9.931795e+07    -18.353436   388.377004    1.091473e+08        id01   \n",
      "2    2.563563e+09   -109.360221  2543.240238    4.613782e+09        id01   \n",
      "3    3.672728e+05      0.006044    18.745470    2.466777e+05        id01   \n",
      "4    3.118603e+07     -9.289212   205.092875    3.047400e+07        id01   \n",
      "\n",
      "         date  \n",
      "0  2024-06-26  \n",
      "1  2024-06-27  \n",
      "2  2024-06-28  \n",
      "3  2024-06-29  \n",
      "4  2024-06-30  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "(664, 23)\n"
     ]
    }
   ],
   "source": [
    "import pywt\n",
    "\n",
    "# 1) timestamp를 datetime으로 변환하고 정렬\n",
    "wLight['timestamp'] = pd.to_datetime(wLight['timestamp'])\n",
    "wLight = wLight.sort_values(['subject_id', 'timestamp'])\n",
    "\n",
    "print(wLight.head())\n",
    "# 2) 날짜(date) 컬럼 추가\n",
    "wLight['date'] = wLight['timestamp'].dt.date\n",
    "\n",
    "# 3) DWT feature 추출 함수 (예시는 db4, 최대 6레벨)\n",
    "def extract_dwt_features(signal, wavelet='db4', level=6):\n",
    "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "    feats = {}\n",
    "    for idx, c in enumerate(coeffs):\n",
    "        feats[f'level_{idx}_mean']   = np.mean(c)\n",
    "        feats[f'level_{idx}_std']    = np.std(c)\n",
    "        feats[f'level_{idx}_energy'] = np.sum(c**2)\n",
    "    return feats\n",
    "\n",
    "# 4) subject_id × date별로 묶어서 feature 뽑기\n",
    "feature_rows = []\n",
    "for (subj, day), grp in wLight.groupby(['subject_id', 'date']):\n",
    "    # grp을 timestamp 기준으로 인덱싱하고 나서 w_light 컬럼을 꺼냅니다.\n",
    "    vals = (\n",
    "        grp\n",
    "        .set_index('timestamp')['w_light']\n",
    "        .resample('1T').mean()\n",
    "        .ffill().fillna(0)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    feats = extract_dwt_features(vals)\n",
    "    feats['subject_id'] = subj\n",
    "    feats['date']       = day\n",
    "    feature_rows.append(feats)\n",
    "\n",
    "# 5) DataFrame 조립\n",
    "daily_features = pd.DataFrame(feature_rows)\n",
    "print(daily_features.head())\n",
    "print(daily_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13950d0f",
   "metadata": {},
   "source": [
    "# DWT 적용!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1673de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1) mUsageStats\n",
    "prep_mUsageStats = mUsageStats[['subject_id', 'timestamp']].copy()\n",
    "prep_mUsageStats['total_usage_time'] = mUsageStats['m_usage_stats'].apply(\n",
    "    lambda lst: sum(d.get('total_time', 0) for d in lst)\n",
    ")\n",
    "\n",
    "# 2) wHr\n",
    "prep_wHr = wHr[['subject_id', 'timestamp']].copy()\n",
    "prep_wHr['avg_heart_rate'] = wHr['heart_rate'].apply(\n",
    "    lambda lst: np.mean(lst) if len(lst) > 0 else np.nan\n",
    ")\n",
    "\n",
    "# 3) mBle\n",
    "prep_mBle = mBle[['subject_id', 'timestamp']].copy()\n",
    "prep_mBle['wb_rssi'] = mBle['m_ble'].apply(\n",
    "    lambda lst: sum(np.exp(d.get('rssi', 0) / 10) for d in lst)\n",
    ")\n",
    "\n",
    "# 4) mWifi\n",
    "prep_mWifi = mWifi[['subject_id', 'timestamp']].copy()\n",
    "prep_mWifi['ww_rssi'] = mWifi['m_wifi'].apply(\n",
    "    lambda lst: sum(np.exp(d.get('rssi', 0) / 10) for d in lst)\n",
    ")\n",
    "\n",
    "# 5) wLight (just copy)\n",
    "prep_wLight = wLight.copy()\n",
    "\n",
    "# 6) mGps\n",
    "def avg_gps(arr):\n",
    "    if len(arr) == 0:\n",
    "        return pd.Series({'avg_alt': np.nan, 'avg_lat': np.nan, 'avg_long': np.nan, 'avg_speed': np.nan})\n",
    "    alts = [d.get('altitude', np.nan) for d in arr]\n",
    "    lats = [d.get('latitude', np.nan) for d in arr]\n",
    "    longs = [d.get('longitude', np.nan) for d in arr]\n",
    "    speeds = [d.get('speed', np.nan) for d in arr]\n",
    "    return pd.Series({\n",
    "        'avg_alt': np.nanmean(alts),\n",
    "        'avg_lat': np.nanmean(lats),\n",
    "        'avg_long': np.nanmean(longs),\n",
    "        'avg_speed': np.nanmean(speeds),\n",
    "    })\n",
    "\n",
    "prep_mGps = mGps[['subject_id', 'timestamp']].copy()\n",
    "gps_avgs = mGps['m_gps'].apply(avg_gps)\n",
    "prep_mGps = pd.concat([prep_mGps, gps_avgs], axis=1)\n",
    "\n",
    "# 7) wPedo\n",
    "prep_wPedo = wPedo[['subject_id', 'timestamp', 'distance', 'speed']].copy()\n",
    "\n",
    "# 2m 30s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d90f01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1108030/30519956.py:47: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/pywt/_multilevel.py:43: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id        date  total_usage_time_lev0_mean  \\\n",
      "0       id01  2024-06-26               142850.553034   \n",
      "1       id01  2024-06-27               111931.971924   \n",
      "2       id01  2024-06-28               109725.298833   \n",
      "3       id01  2024-06-29                96803.554352   \n",
      "4       id01  2024-06-30               152565.531960   \n",
      "\n",
      "   total_usage_time_lev0_std  total_usage_time_lev0_energy  \\\n",
      "0              320174.917383                  1.044805e+13   \n",
      "1              155658.006840                  6.542956e+12   \n",
      "2              141429.307699                  4.678116e+12   \n",
      "3              131253.540690                  3.351401e+12   \n",
      "4              204423.231830                  8.588593e+12   \n",
      "\n",
      "   total_usage_time_lev1_mean  total_usage_time_lev1_std  \\\n",
      "0                16868.787990              195746.453812   \n",
      "1               -10115.412519              225115.986414   \n",
      "2                -3088.248837              207698.429065   \n",
      "3                 7762.613261              182339.072653   \n",
      "4               -24755.793043              239793.620558   \n",
      "\n",
      "   total_usage_time_lev1_energy  total_usage_time_lev2_mean  \\\n",
      "0                  3.281105e+12                 8393.429136   \n",
      "1                  9.038756e+12                -4006.069624   \n",
      "2                  6.299634e+12                 -544.240020   \n",
      "3                  4.196782e+12                 2592.374962   \n",
      "4                  7.671026e+12                 6500.473761   \n",
      "\n",
      "   total_usage_time_lev2_std  total_usage_time_lev2_energy  \\\n",
      "0              180177.890292                  5.303127e+12   \n",
      "1              185136.200391                  1.200201e+13   \n",
      "2              181198.161862                  9.357425e+12   \n",
      "3              145464.131772                  5.185801e+12   \n",
      "4              212958.848610                  1.171158e+13   \n",
      "\n",
      "   total_usage_time_lev3_mean  total_usage_time_lev3_std  \\\n",
      "0                53342.197715              185288.665550   \n",
      "1                54911.301801              195693.536418   \n",
      "2                56390.708911              186457.968055   \n",
      "3                45834.472503              153057.705073   \n",
      "4                62640.764456              207012.781018   \n",
      "\n",
      "   total_usage_time_lev3_energy  \n",
      "0                  1.185955e+13  \n",
      "1                  2.866998e+13  \n",
      "2                  2.140182e+13  \n",
      "3                  1.235529e+13  \n",
      "4                  2.381008e+13  \n",
      "(690, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "# 7개의 prep_ 데이터프레임을 딕셔너리에 모아둡니다.\n",
    "prep_dfs = {\n",
    "    'mUsageStats': prep_mUsageStats,\n",
    "    'wHr':          prep_wHr,\n",
    "    'mBle':         prep_mBle,\n",
    "    'mWifi':        prep_mWifi,\n",
    "    'wLight':       prep_wLight,\n",
    "    'mGps':         prep_mGps,\n",
    "    'wPedo':        prep_wPedo\n",
    "}\n",
    "\n",
    "# 1) DWT feature extraction 함수\n",
    "def extract_dwt_features(signal, wavelet='db4', level=6):\n",
    "    coeffs = pywt.wavedec(signal, wavelet=wavelet, level=level)\n",
    "    feats = {}\n",
    "    for idx, c in enumerate(coeffs):\n",
    "        feats[f'lev{idx}_mean']   = np.mean(c)\n",
    "        feats[f'lev{idx}_std']    = np.std(c)\n",
    "        feats[f'lev{idx}_energy'] = np.sum(c**2)\n",
    "    return feats\n",
    "\n",
    "# 2) 각 데이터프레임별로 사용할 DWT 레벨 지정 (튜닝 가능)\n",
    "level_list = [3,3,3,3,3,3,3]\n",
    "\n",
    "daily_features = {}\n",
    "\n",
    "for (name, df), lvl in zip(prep_dfs.items(), level_list):\n",
    "    # 3) timestamp → datetime으로 변환 & date 컬럼 추가\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.sort_values(['subject_id', 'timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "\n",
    "    # 4) numeric 컬럼만 골라 subject_id 제외\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [c for c in numeric_cols if c != 'subject_id']\n",
    "\n",
    "    # 5) 그룹별로 DWT 피처 뽑아서 리스트에 저장\n",
    "    feature_rows = []\n",
    "    for (subj, day), grp in df.groupby(['subject_id', 'date']):\n",
    "        row = {'subject_id': subj, 'date': day}\n",
    "        for col in numeric_cols:\n",
    "            # 1분 단위 리샘플링 평균 → 누락치는 0으로 채움\n",
    "            ts = grp.set_index('timestamp')[col].resample('1T').mean().fillna(0).values\n",
    "            feats = extract_dwt_features(ts, level=lvl)\n",
    "            # 컬럼명에 접두어 붙이기\n",
    "            for k, v in feats.items():\n",
    "                row[f'{col}_{k}'] = v\n",
    "        feature_rows.append(row)\n",
    "\n",
    "    # 6) DataFrame으로 변환해서 저장\n",
    "    daily_features[name] = pd.DataFrame(feature_rows)\n",
    "\n",
    "# 7) 샘플: wLight 일별 DWT 피처 확인\n",
    "print(daily_features['mUsageStats'].head())\n",
    "print(daily_features['mUsageStats'].shape)\n",
    "\n",
    "# 11.4s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97d33519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id        date  transition_count  act_0_count  act_0_ratio  \\\n",
      "0       id01  2024-06-26                32           89     0.125176   \n",
      "1       id01  2024-06-27                39          211     0.146528   \n",
      "2       id01  2024-06-28                40          161     0.111806   \n",
      "3       id01  2024-06-29                27           95     0.065972   \n",
      "4       id01  2024-06-30                26          199     0.138194   \n",
      "\n",
      "   act_1_count  act_1_ratio  act_2_count  act_2_ratio  act_3_count  \\\n",
      "0            1     0.001406            0          0.0          478   \n",
      "1            0     0.000000            0          0.0          880   \n",
      "2            1     0.000694            0          0.0         1241   \n",
      "3            0     0.000000            0          0.0         1320   \n",
      "4            0     0.000000            0          0.0         1229   \n",
      "\n",
      "   act_3_ratio  act_4_count  act_4_ratio  act_5_count  act_5_ratio  \\\n",
      "0     0.672293          112     0.157525            0          0.0   \n",
      "1     0.611111          318     0.220833            0          0.0   \n",
      "2     0.861806            1     0.000694            0          0.0   \n",
      "3     0.916667            0     0.000000            0          0.0   \n",
      "4     0.853472            0     0.000000            0          0.0   \n",
      "\n",
      "   act_7_count  act_7_ratio  act_8_count  act_8_ratio  \n",
      "0           31     0.043601            0          0.0  \n",
      "1           31     0.021528            0          0.0  \n",
      "2           36     0.025000            0          0.0  \n",
      "3           25     0.017361            0          0.0  \n",
      "4           12     0.008333            0          0.0  \n",
      "(700, 19)\n",
      "  subject_id        date  A capella_prob_sum  \\\n",
      "0       id01  2024-06-26                 0.0   \n",
      "1       id01  2024-06-27                 0.0   \n",
      "2       id01  2024-06-28                 0.0   \n",
      "3       id01  2024-06-29                 0.0   \n",
      "4       id01  2024-06-30                 0.0   \n",
      "\n",
      "   Accelerating, revving, vroom_prob_sum  Accordion_prob_sum  \\\n",
      "0                               4.333376                 0.0   \n",
      "1                               0.000000                 0.0   \n",
      "2                               0.000000                 0.0   \n",
      "3                               0.000000                 0.0   \n",
      "4                               0.000000                 0.0   \n",
      "\n",
      "   Acoustic guitar_prob_sum  Afrobeat_prob_sum  Air brake_prob_sum  \\\n",
      "0                       0.0                0.0                 0.0   \n",
      "1                       0.0                0.0                 0.0   \n",
      "2                       0.0                0.0                 0.0   \n",
      "3                       0.0                0.0                 0.0   \n",
      "4                       0.0                0.0                 0.0   \n",
      "\n",
      "   Air conditioning_prob_sum  Air horn, truck horn_prob_sum  ...  \\\n",
      "0                    0.01064                       0.069598  ...   \n",
      "1                    0.00000                       0.000000  ...   \n",
      "2                    0.00000                       0.000000  ...   \n",
      "3                    0.00000                       0.000000  ...   \n",
      "4                    0.00000                       0.000000  ...   \n",
      "\n",
      "   Wind noise (microphone)_prob_sum  Wood_prob_sum  Wood block_prob_sum  \\\n",
      "0                          0.100864       0.372201                  0.0   \n",
      "1                          0.000000       0.000000                  0.0   \n",
      "2                          0.116728       0.000000                  0.0   \n",
      "3                          0.000000       0.000000                  0.0   \n",
      "4                          0.000000       0.000000                  0.0   \n",
      "\n",
      "   Writing_prob_sum  Yell_prob_sum  Yip_prob_sum  Yodeling_prob_sum  \\\n",
      "0          0.070800            0.0           0.0                0.0   \n",
      "1          0.000000            0.0           0.0                0.0   \n",
      "2          0.000000            0.0           0.0                0.0   \n",
      "3          0.035868            0.0           0.0                0.0   \n",
      "4          0.000000            0.0           0.0                0.0   \n",
      "\n",
      "   Zing_prob_sum  Zipper (clothing)_prob_sum  Zither_prob_sum  \n",
      "0            0.0                     0.01081              0.0  \n",
      "1            0.0                     0.00000              0.0  \n",
      "2            0.0                     0.00000              0.0  \n",
      "3            0.0                     0.00000              0.0  \n",
      "4            0.0                     0.00000              0.0  \n",
      "\n",
      "[5 rows x 519 columns]\n",
      "(700, 519)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- prep_mActivity 생성 ---\n",
    "# 1. timestamp → datetime, date 컬럼 추가\n",
    "mActivity['timestamp'] = pd.to_datetime(mActivity['timestamp'])\n",
    "mActivity = mActivity.sort_values(['subject_id', 'timestamp'])\n",
    "mActivity['date'] = mActivity['timestamp'].dt.date\n",
    "\n",
    "# 2. 사용할 activity 코드 목록 (0,1,2,3,4,5,7,8)\n",
    "activity_codes = [0,1,2,3,4,5,7,8]\n",
    "\n",
    "rows = []\n",
    "for (subj, day), grp in mActivity.groupby(['subject_id','date']):\n",
    "    total = len(grp)\n",
    "    counts = grp['m_activity'].value_counts().reindex(activity_codes, fill_value=0)\n",
    "    ratios = counts / total\n",
    "    # transition count: 연속된 activity 값이 바뀐 횟수\n",
    "    trans = (grp['m_activity'].shift() != grp['m_activity']).sum() - 1  # 첫 비교 제외\n",
    "    row = {\n",
    "        'subject_id': subj,\n",
    "        'date':       day,\n",
    "        'transition_count': int(trans)\n",
    "    }\n",
    "    # 각 코드별 count, ratio 추가\n",
    "    for code in activity_codes:\n",
    "        row[f'act_{code}_count'] = int(counts[code])\n",
    "        row[f'act_{code}_ratio'] = float(ratios[code])\n",
    "    rows.append(row)\n",
    "\n",
    "prep_mActivity = pd.DataFrame(rows)\n",
    "print(prep_mActivity.head())\n",
    "print(prep_mActivity.shape)\n",
    "\n",
    "\n",
    "# --- prep_mAmbience 생성 ---\n",
    "# 1. timestamp → datetime, date 컬럼 추가\n",
    "mAmbience['timestamp'] = pd.to_datetime(mAmbience['timestamp'])\n",
    "mAmbience = mAmbience.sort_values(['subject_id','timestamp'])\n",
    "mAmbience['date'] = mAmbience['timestamp'].dt.date\n",
    "\n",
    "# 2. 전체 고유 label pool 추출\n",
    "label_pool = set()\n",
    "for lst in mAmbience['m_ambience']:\n",
    "    for label, prob in lst:\n",
    "        label_pool.add(label)\n",
    "label_pool = sorted(label_pool)\n",
    "\n",
    "rows = []\n",
    "for (subj, day), grp in mAmbience.groupby(['subject_id','date']):\n",
    "    # initialize sum dict\n",
    "    sums = {lbl: 0.0 for lbl in label_pool}\n",
    "    for lst in grp['m_ambience']:\n",
    "        for label, prob in lst:\n",
    "            sums[label] += float(prob)\n",
    "    row = {'subject_id': subj, 'date': day}\n",
    "    # 각 label별 확률 합계 추가\n",
    "    for lbl in label_pool:\n",
    "        row[f'{lbl}_prob_sum'] = sums[lbl]\n",
    "    rows.append(row)\n",
    "\n",
    "prep_mAmbience = pd.DataFrame(rows)\n",
    "print(prep_mAmbience.head())\n",
    "print(prep_mAmbience.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ec1850f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id        date  total_usage_time_lev0_mean  \\\n",
      "0       id01  2024-06-26               142850.553034   \n",
      "1       id01  2024-06-27               111931.971924   \n",
      "2       id01  2024-06-28               109725.298833   \n",
      "3       id01  2024-06-29                96803.554352   \n",
      "4       id01  2024-06-30               152565.531960   \n",
      "\n",
      "   total_usage_time_lev0_std  total_usage_time_lev0_energy  \\\n",
      "0              320174.917383                  1.044805e+13   \n",
      "1              155658.006840                  6.542956e+12   \n",
      "2              141429.307699                  4.678116e+12   \n",
      "3              131253.540690                  3.351401e+12   \n",
      "4              204423.231830                  8.588593e+12   \n",
      "\n",
      "   total_usage_time_lev1_mean  total_usage_time_lev1_std  \\\n",
      "0                16868.787990              195746.453812   \n",
      "1               -10115.412519              225115.986414   \n",
      "2                -3088.248837              207698.429065   \n",
      "3                 7762.613261              182339.072653   \n",
      "4               -24755.793043              239793.620558   \n",
      "\n",
      "   total_usage_time_lev1_energy  total_usage_time_lev2_mean  \\\n",
      "0                  3.281105e+12                 8393.429136   \n",
      "1                  9.038756e+12                -4006.069624   \n",
      "2                  6.299634e+12                 -544.240020   \n",
      "3                  4.196782e+12                 2592.374962   \n",
      "4                  7.671026e+12                 6500.473761   \n",
      "\n",
      "   total_usage_time_lev2_std  ...  Wind noise (microphone)_prob_sum  \\\n",
      "0              180177.890292  ...                          0.100864   \n",
      "1              185136.200391  ...                          0.000000   \n",
      "2              181198.161862  ...                          0.116728   \n",
      "3              145464.131772  ...                          0.000000   \n",
      "4              212958.848610  ...                          0.000000   \n",
      "\n",
      "   Wood_prob_sum  Wood block_prob_sum  Writing_prob_sum  Yell_prob_sum  \\\n",
      "0       0.372201                  0.0          0.070800            0.0   \n",
      "1       0.000000                  0.0          0.000000            0.0   \n",
      "2       0.000000                  0.0          0.000000            0.0   \n",
      "3       0.000000                  0.0          0.035868            0.0   \n",
      "4       0.000000                  0.0          0.000000            0.0   \n",
      "\n",
      "   Yip_prob_sum  Yodeling_prob_sum  Zing_prob_sum  Zipper (clothing)_prob_sum  \\\n",
      "0           0.0                0.0            0.0                     0.01081   \n",
      "1           0.0                0.0            0.0                     0.00000   \n",
      "2           0.0                0.0            0.0                     0.00000   \n",
      "3           0.0                0.0            0.0                     0.00000   \n",
      "4           0.0                0.0            0.0                     0.00000   \n",
      "\n",
      "   Zither_prob_sum  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "\n",
      "[5 rows x 668 columns]\n",
      "(700, 668)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "\n",
    "# assume daily_features is a dict of DataFrames for the 7 DWT‐processed tables:\n",
    "# daily_features = {\n",
    "#     'mUsageStats': dwt_mUsageStats,\n",
    "#     'wHr':          dwt_wHr,\n",
    "#     'mBle':         dwt_mBle,\n",
    "#     'mWifi':        dwt_mWifi,\n",
    "#     'wLight':       dwt_wLight,\n",
    "#     'mGps':         dwt_mGps,\n",
    "#     'wPedo':        dwt_wPedo,\n",
    "# }\n",
    "\n",
    "# and the two preprocessed frames:\n",
    "# prep_mActivity, prep_mAmbience\n",
    "\n",
    "# 1. collect all DataFrames in a list\n",
    "dfs = list(daily_features.values()) + [prep_mActivity, prep_mAmbience]\n",
    "\n",
    "# 2. merge them all on ['subject_id', 'date'] via outer join\n",
    "merged_df = reduce(\n",
    "    lambda left, right: pd.merge(\n",
    "        left, right,\n",
    "        on=['subject_id', 'date'],\n",
    "        how='outer'\n",
    "    ),\n",
    "    dfs\n",
    ")\n",
    "\n",
    "# 3. inspect\n",
    "print(merged_df.head())\n",
    "print(merged_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d4fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('merged_df_lv3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52abb2f",
   "metadata": {},
   "source": [
    "# Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d99c726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "test_df = pd.read_csv('../data/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d866e0d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((450, 9), (250, 9))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape, test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f882d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>sleep_date</th>\n",
       "      <th>lifelog_date</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>2024-06-26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>2024-06-27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>2024-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>2024-06-29</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-07-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
       "0       id01  2024-06-27   2024-06-26   0   0   0   0   0   1\n",
       "1       id01  2024-06-28   2024-06-27   0   0   0   0   1   1\n",
       "2       id01  2024-06-29   2024-06-28   1   0   0   1   1   1\n",
       "3       id01  2024-06-30   2024-06-29   1   0   1   2   0   0\n",
       "4       id01  2024-07-01   2024-06-30   0   1   1   1   1   1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d640e1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ temp_train_df.shape: (700, 675)\n",
      "  subject_id       date lifelog_date   Q1   Q2   Q3   S1   S2   S3\n",
      "0       id01 2024-06-26   2024-06-26  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "1       id01 2024-06-27   2024-06-27  0.0  0.0  0.0  0.0  1.0  1.0\n",
      "2       id01 2024-06-28   2024-06-28  1.0  0.0  0.0  1.0  1.0  1.0\n",
      "3       id01 2024-06-29   2024-06-29  1.0  0.0  1.0  2.0  0.0  0.0\n",
      "4       id01 2024-06-30   2024-06-30  0.0  1.0  1.0  1.0  1.0  1.0\n"
     ]
    }
   ],
   "source": [
    "# 1) 양쪽 날짜 컬럼을 datetime 으로 통일\n",
    "merged_df['date']          = pd.to_datetime(merged_df['date'])\n",
    "train_df['lifelog_date']   = pd.to_datetime(train_df['lifelog_date'])\n",
    "\n",
    "# 2) merge\n",
    "temp_train_df = pd.merge(\n",
    "    merged_df,\n",
    "    train_df[['subject_id','lifelog_date','Q1','Q2','Q3','S1','S2','S3']],\n",
    "    left_on  = ['subject_id','date'],\n",
    "    right_on = ['subject_id','lifelog_date'],\n",
    "    how      = 'left'\n",
    ")\n",
    "\n",
    "print(\"▶ temp_train_df.shape:\", temp_train_df.shape)\n",
    "print(temp_train_df[['subject_id','date','lifelog_date','Q1','Q2','Q3','S1','S2','S3']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b896676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ dropped 250 rows where all Q/S are NaN\n",
      "▶ 남은 행 개수: 450\n"
     ]
    }
   ],
   "source": [
    "# 셀 2: Q1,Q2,Q3,S1,S2,S3 모두 결측치인 행 드롭\n",
    "before_cnt = temp_train_df.shape[0]\n",
    "\n",
    "# axis=1 로 Q1~S3 컬럼을 가져와 전부 NaN 인 행 True\n",
    "mask_all_na = temp_train_df[['Q1','Q2','Q3','S1','S2','S3']].isna().all(axis=1)\n",
    "\n",
    "# drop\n",
    "temp_train_df = temp_train_df.loc[~mask_all_na].reset_index(drop=True)\n",
    "\n",
    "after_cnt = temp_train_df.shape[0]\n",
    "print(f\"▶ dropped {before_cnt - after_cnt} rows where all Q/S are NaN\")\n",
    "print(\"▶ 남은 행 개수:\", after_cnt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb8ed874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ 전체 컬럼 중 하나라도 NaN인 행 개수: 88 / 전체 450 행\n"
     ]
    }
   ],
   "source": [
    "# 셀: 모든 컬럼 대상 결측치 개수 확인\n",
    "mask_any_na_all = temp_train_df.isna().any(axis=1)  # 하나라도 NaN 이면 True\n",
    "num_any_na_all = mask_any_na_all.sum()\n",
    "total_rows = temp_train_df.shape[0]\n",
    "\n",
    "print(f\"▶ 전체 컬럼 중 하나라도 NaN인 행 개수: {num_any_na_all} / 전체 {total_rows} 행\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f303ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ train_model_df.shape: (315, 675)\n",
      "    subject_id       date  total_usage_time_lev0_mean  \\\n",
      "409       id09 2024-08-27                61303.353314   \n",
      "108       id03 2024-08-13                67429.961673   \n",
      "229       id06 2024-06-09                         NaN   \n",
      "420       id10 2024-07-09                62980.359905   \n",
      "118       id03 2024-09-07                76791.552746   \n",
      "\n",
      "     total_usage_time_lev0_std  total_usage_time_lev0_energy  \\\n",
      "409              131908.902287                  3.110235e+12   \n",
      "108              114975.589831                  3.286744e+12   \n",
      "229                        NaN                           NaN   \n",
      "420              104034.585500                  1.937453e+12   \n",
      "118              110201.306013                  3.283511e+12   \n",
      "\n",
      "     total_usage_time_lev1_mean  total_usage_time_lev1_std  \\\n",
      "409                 6411.598572              177334.267646   \n",
      "108                -1992.012752              123111.969439   \n",
      "229                         NaN                        NaN   \n",
      "420               -12854.890963              152855.369861   \n",
      "118                 2546.629447              144031.825740   \n",
      "\n",
      "     total_usage_time_lev1_energy  total_usage_time_lev2_mean  \\\n",
      "409                  4.628817e+12                 3444.963343   \n",
      "108                  2.804697e+12                 9904.894144   \n",
      "229                           NaN                         NaN   \n",
      "420                  3.082432e+12                -8240.622834   \n",
      "118                  3.776801e+12                 5579.331892   \n",
      "\n",
      "     total_usage_time_lev2_std  ...  Zing_prob_sum  \\\n",
      "409              149797.454616  ...       0.000000   \n",
      "108              124636.711135  ...       0.000000   \n",
      "229                        NaN  ...       0.000000   \n",
      "420              108773.301873  ...       0.050192   \n",
      "118              128246.544873  ...       0.000000   \n",
      "\n",
      "     Zipper (clothing)_prob_sum  Zither_prob_sum  lifelog_date   Q1   Q2   Q3  \\\n",
      "409                    0.000000         0.000000    2024-08-27  1.0  1.0  1.0   \n",
      "108                    0.000000         0.000000    2024-08-13  1.0  0.0  0.0   \n",
      "229                    0.275106         0.000000    2024-06-09  0.0  0.0  0.0   \n",
      "420                    0.212570         0.143855    2024-07-09  0.0  1.0  1.0   \n",
      "118                    0.000000         0.000000    2024-09-07  1.0  1.0  1.0   \n",
      "\n",
      "      S1   S2   S3  \n",
      "409  1.0  1.0  0.0  \n",
      "108  1.0  1.0  1.0  \n",
      "229  1.0  1.0  1.0  \n",
      "420  1.0  0.0  1.0  \n",
      "118  2.0  0.0  0.0  \n",
      "\n",
      "[5 rows x 675 columns] \n",
      "\n",
      "▶ val_model_df.shape: (135, 675)\n"
     ]
    }
   ],
   "source": [
    "# 셀 3: temp_train_df를 70% / 30% 로 분할\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_model_df, val_model_df = train_test_split(\n",
    "    temp_train_df,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(\"▶ train_model_df.shape:\", train_model_df.shape)\n",
    "print(train_model_df.head(), \"\\n\")\n",
    "\n",
    "print(\"▶ val_model_df.shape:\", val_model_df.shape)\n",
    "# print(val_model_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4480ae",
   "metadata": {},
   "source": [
    "# 모델 적용 - 결측치 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e138dd",
   "metadata": {},
   "source": [
    "## 0. 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5abca2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ X_train.shape: (315, 666) y_train.shape: (315, 6)\n",
      "▶ X_val.shape:   (135, 666) y_val.shape:   (135, 6)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 이미 train_model_df, val_model_df 가 로드되어 있다고 가정합니다.\n",
    "# 이들에는 subject_id, date, lifelog_date, (feature들…), Q1,Q2,Q3,S1,S2,S3 컬럼이 있습니다.\n",
    "\n",
    "# 1) X, y 분할\n",
    "drop_cols = ['subject_id','date','lifelog_date']\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "\n",
    "X_val   = val_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df[target_cols]\n",
    "\n",
    "print(\"▶ X_train.shape:\", X_train.shape, \"y_train.shape:\", y_train.shape)\n",
    "print(\"▶ X_val.shape:  \", X_val.shape,   \"y_val.shape:  \", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff4070",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d139b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ train drop: 65, 남은 train: 250\n",
      "▶ val   drop: 23, 남은 val  : 112\n",
      "▶ LR per-target F1: {'Q1': 0.5351213282247764, 'Q2': 0.5221766079046929, 'Q3': 0.5470171890798787, 'S1': 0.36224475918478366, 'S2': 0.5143288084464556, 'S3': 0.5772806775292276}\n",
      "▶ LR mean F1     : 0.5096948950616359\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# 1) 타겟, 드롭 칼럼 정의\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']  # 실제 컬럼명에 맞춰 조정\n",
    "\n",
    "# 2) 피처·타겟 분리\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# (선택) 날짜형 칼럼이 남아있다면 수치형만 골라내기\n",
    "import numpy as np\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_val   = X_val  .select_dtypes(include=[np.number])\n",
    "\n",
    "# 3) NaN 행 제거\n",
    "mask_tr = X_train.notna().all(axis=1)\n",
    "mask_va = X_val.notna().all(axis=1)\n",
    "print(f\"▶ train drop: {mask_tr.size-mask_tr.sum()}, 남은 train: {mask_tr.sum()}\")\n",
    "print(f\"▶ val   drop: {mask_va.size-mask_va.sum()}, 남은 val  : {mask_va.sum()}\")\n",
    "\n",
    "X_train, y_train = X_train.loc[mask_tr], y_train.loc[mask_tr]\n",
    "X_val,   y_val   = X_val.loc[mask_va],   y_val  .loc[mask_va]\n",
    "\n",
    "# 4) 모델 학습\n",
    "lr = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(LogisticRegression(max_iter=1000, random_state=42))\n",
    ")\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# 5) 예측 및 F1\n",
    "y_pred = pd.DataFrame(\n",
    "    lr.predict(X_val), index=y_val.index, columns=target_cols\n",
    ")\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ LR per-target F1:\", f1s)\n",
    "print(\"▶ LR mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318f43f",
   "metadata": {},
   "source": [
    "## 2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85e65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ train 에서 drop: 65, 남은 train: 250\n",
      "▶ val   에서 drop: 23, 남은 val  : 112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:11:55] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:12:15] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:12:32] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:12:50] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:13:42] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:13:57] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ XGB per-target F1: {'Q1': 0.6066411238825031, 'Q2': 0.4725274725274725, 'Q3': 0.5292397660818713, 'S1': 0.4747222222222222, 'S2': 0.60625, 'S3': 0.567479674796748}\n",
      "▶ XGB mean F1     : 0.5428100432518028\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 필요한 라이브러리 임포트\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 타겟·드롭 컬럼 정의 (셀 1과 동일)\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']  # 실제 컬럼명 확인하고 필요시 수정\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) 피처·타겟 분리\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) 수치형 열만 골라내기 (datetime 같은 혼합형 완전 제거)\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_val   = X_val  .select_dtypes(include=[np.number])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) NaN 행 제거\n",
    "mask_tr = X_train.notna().all(axis=1)\n",
    "mask_va = X_val  .notna().all(axis=1)\n",
    "\n",
    "print(f\"▶ train 에서 drop: {mask_tr.size-mask_tr.sum()}, 남은 train: {mask_tr.sum()}\")\n",
    "print(f\"▶ val   에서 drop: {mask_va.size-mask_va.sum()}, 남은 val  : {mask_va.sum()}\")\n",
    "\n",
    "X_train, y_train = X_train.loc[mask_tr], y_train.loc[mask_tr]\n",
    "X_val,   y_val   = X_val  .loc[mask_va],   y_val  .loc[mask_va]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) XGBoost 파이프라인 생성 & 학습\n",
    "xgb_pipe = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            random_state=42\n",
    "        )\n",
    "    )\n",
    ")\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    xgb_pipe.predict(X_val), \n",
    "    index=y_val.index, \n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro') \n",
    "       for c in target_cols}\n",
    "print(\"▶ XGB per-target F1:\", f1s)\n",
    "print(\"▶ XGB mean F1     :\", np.mean(list(f1s.values())))\n",
    "\n",
    "\n",
    "# 2m 17.5s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa98ae3",
   "metadata": {},
   "source": [
    "## 3. Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3b955ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ train 에서 drop: 65, 남은 train: 250\n",
      "▶ val   에서 drop: 23, 남은 val  : 112\n",
      "▶ RF  per-target F1: {'Q1': 0.5153846153846154, 'Q2': 0.4873075322513525, 'Q3': 0.5472739820565907, 'S1': 0.2955437543292542, 'S2': 0.4740608228980322, 'S3': 0.5175963861750184}\n",
      "▶ RF  mean F1     : 0.4728611821824773\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 필요한 라이브러리 임포트\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 타겟·드롭 컬럼 정의 (셀 1,2와 동일)\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']  # 실제 컬럼명 확인 후 수정\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) 피처·타겟 분리\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) 수치형 열만 골라내기\n",
    "X_train = X_train.select_dtypes(include=[np.number])\n",
    "X_val   = X_val  .select_dtypes(include=[np.number])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) NaN 행 제거\n",
    "mask_tr = X_train.notna().all(axis=1)\n",
    "mask_va = X_val  .notna().all(axis=1)\n",
    "\n",
    "print(f\"▶ train 에서 drop: {mask_tr.size-mask_tr.sum()}, 남은 train: {mask_tr.sum()}\")\n",
    "print(f\"▶ val   에서 drop: {mask_va.size-mask_va.sum()}, 남은 val  : {mask_va.sum()}\")\n",
    "\n",
    "X_train, y_train = X_train.loc[mask_tr], y_train.loc[mask_tr]\n",
    "X_val,   y_val   = X_val  .loc[mask_va],   y_val  .loc[mask_va]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) Random Forest 파이프라인 생성 & 학습\n",
    "rf_pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    rf_pipe.predict(X_val),\n",
    "    index=y_val.index,\n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ RF  per-target F1:\", f1s)\n",
    "print(\"▶ RF  mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83109a5f",
   "metadata": {},
   "source": [
    "# 모델 적용 - 결측치 imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc9986",
   "metadata": {},
   "source": [
    "## 1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ebea7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ LR  per-target F1: {'Q1': 0.5258999122036874, 'Q2': 0.5358209796516543, 'Q3': 0.5151760889712698, 'S1': 0.3782287015147536, 'S2': 0.5106518282988871, 'S3': 0.5796574987026466}\n",
      "▶ LR  mean F1     : 0.5075725015571498\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 피처·타겟 분리\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']\n",
    "\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) 수치형 컬럼만 선택\n",
    "num_feats = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train = X_train[num_feats]\n",
    "X_val   = X_val  [num_feats]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) 파이프라인 정의 (imputer → scaler → classifier)\n",
    "lr_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(\n",
    "        LogisticRegression(max_iter=1000, random_state=42)\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) 학습\n",
    "lr_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    lr_pipe.predict(X_val),\n",
    "    index=y_val.index,\n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ LR  per-target F1:\", f1s)\n",
    "print(\"▶ LR  mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c022ab40",
   "metadata": {},
   "source": [
    "## 2. XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f6ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:14:19] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:14:38] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:14:54] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:15:13] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:16:08] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:16:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ XGB per-target F1: {'Q1': 0.6367181064308858, 'Q2': 0.4925666199158485, 'Q3': 0.5870588235294117, 'S1': 0.43591499072885465, 'S2': 0.571842250413679, 'S3': 0.6275486171859229}\n",
      "▶ XGB mean F1     : 0.558608234700767\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 피처·타겟 분리 (셀 1과 동일)\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']\n",
    "\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) 수치형 컬럼만 선택\n",
    "num_feats = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train = X_train[num_feats]\n",
    "X_val   = X_val  [num_feats]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) 파이프라인 정의 (imputer → scaler → classifier)\n",
    "xgb_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(\n",
    "        XGBClassifier(\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) 학습\n",
    "xgb_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    xgb_pipe.predict(X_val),\n",
    "    index=y_val.index,\n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ XGB per-target F1:\", f1s)\n",
    "print(\"▶ XGB mean F1     :\", np.mean(list(f1s.values())))\n",
    "\n",
    "# 2m 20s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1976d2e",
   "metadata": {},
   "source": [
    "## 2.1 XGBoost - 하이퍼파라미터 튜닝 (다시 해봐야됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bae3033",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-27 23:18:30,736] A new study created in memory with name: no-name-569abb44-30ba-438b-b40c-34f3abca8526\n",
      "/tmp/ipykernel_1108030/294053145.py:21: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
      "/tmp/ipykernel_1108030/294053145.py:23: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
      "/tmp/ipykernel_1108030/294053145.py:24: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
      "  \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
      "/tmp/ipykernel_1108030/294053145.py:25: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n",
      "/tmp/ipykernel_1108030/294053145.py:26: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 1.0),\n",
      "/tmp/ipykernel_1108030/294053145.py:27: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 1.0),\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:18:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:20:30] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:22:25] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [23:24:23] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[W 2025-05-27 23:27:19,763] Trial 0 failed with parameters: {'n_estimators': 315, 'learning_rate': 0.003863148976771556, 'max_depth': 10, 'subsample': 0.672790273406187, 'colsample_bytree': 0.5147263946206042, 'gamma': 2.1660782340515497e-05, 'reg_alpha': 0.00026678776078047816, 'reg_lambda': 0.000567997784757416} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_1108030/294053145.py\", line 42, in objective\n",
      "    pipe.fit(X_train, y_train)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/pipeline.py\", line 662, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/multioutput.py\", line 543, in fit\n",
      "    super().fit(X, Y, sample_weight=sample_weight, **fit_params)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/base.py\", line 1389, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/multioutput.py\", line 274, in fit\n",
      "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1985, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/joblib/parallel.py\", line 1913, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/wonjun/.local/lib/python3.10/site-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
      "    estimator.fit(X, y, **fit_params)\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/sklearn.py\", line 1682, in fit\n",
      "    self._Booster = train(\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py\", line 729, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py\", line 183, in train\n",
      "    bst.update(dtrain, iteration=i, fobj=obj)\n",
      "  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py\", line 2247, in update\n",
      "    _LIB.XGBoosterUpdateOneIter(\n",
      "KeyboardInterrupt\n",
      "[W 2025-05-27 23:27:19,771] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Optuna 스터디 실행\u001b[39;00m\n\u001b[1;32m     54\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▶ Best mean F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▶ Best params :\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[30], line 42\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m pipe \u001b[38;5;241m=\u001b[39m make_pipeline(\n\u001b[1;32m     36\u001b[0m     SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     37\u001b[0m     StandardScaler(),\n\u001b[1;32m     38\u001b[0m     MultiOutputClassifier(XGBClassifier(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams))\n\u001b[1;32m     39\u001b[0m )\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 학습\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mpipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# 예측\u001b[39;00m\n\u001b[1;32m     44\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m pipe\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/pipeline.py:662\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    657\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metadata_for_step(\n\u001b[1;32m    658\u001b[0m             step_idx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    659\u001b[0m             step_params\u001b[38;5;241m=\u001b[39mrouted_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m    660\u001b[0m             all_params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    661\u001b[0m         )\n\u001b[0;32m--> 662\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/multioutput.py:543\u001b[0m, in \u001b[0;36mMultiOutputClassifier.fit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, Y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params):\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model to data matrix X and targets Y.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m        Returns a fitted instance.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    544\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m [estimator\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;28;01mfor\u001b[39;00m estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_]\n\u001b[1;32m    545\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/multioutput.py:274\u001b[0m, in \u001b[0;36m_MultiOutputEstimator.fit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         routed_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sample_weight\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_features_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_features_in_\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1985\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1984\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1985\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1987\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1988\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1989\u001b[0m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1990\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1991\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1992\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/joblib/parallel.py:1913\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1913\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/multioutput.py:63\u001b[0m, in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     61\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m estimator\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/sklearn.py:1682\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[1;32m   1660\u001b[0m model, metric, params, feature_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(\n\u001b[1;32m   1661\u001b[0m     xgb_model, params, feature_weights\n\u001b[1;32m   1662\u001b[0m )\n\u001b[1;32m   1663\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1664\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[1;32m   1665\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1679\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[1;32m   1680\u001b[0m )\n\u001b[0;32m-> 1682\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1685\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1686\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1687\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1688\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1689\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[1;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[0;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/training.py:183\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/xgboost/core.py:2247\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[1;32m   2245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2246\u001b[0m     _check_call(\n\u001b[0;32m-> 2247\u001b[0m         \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2248\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\n\u001b[1;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2250\u001b[0m     )\n\u001b[1;32m   2251\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 1) Optuna로 XGBoost 하이퍼파라미터 튜닝\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# train/val 데이터는 미리 준비되어 있다고 가정합니다.\n",
    "# X_train, y_train, X_val, y_val 가 위 예시와 동일하게 정의돼 있어야 합니다.\n",
    "\n",
    "def objective(trial):\n",
    "    # 하이퍼파라미터 서치 스페이스\n",
    "    params = {\n",
    "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 500),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-3, 0.3),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.5, 1.0),\n",
    "        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n",
    "        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-8, 1.0),\n",
    "        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-8, 1.0),\n",
    "        \"random_state\": 42,\n",
    "        \"use_label_encoder\": False,\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "\n",
    "    # 파이프라인: 결측치→스케일링→MultiOutput XGB\n",
    "    pipe = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "        MultiOutputClassifier(XGBClassifier(**params))\n",
    "    )\n",
    "\n",
    "    # 학습\n",
    "    pipe.fit(X_train, y_train)\n",
    "    # 예측\n",
    "    y_pred = pipe.predict(X_val)\n",
    "\n",
    "    # Macro-F1 (각 타겟별 계산 후 평균)\n",
    "    f1s = [\n",
    "        f1_score(y_val[col], y_pred[:, i], average=\"macro\")\n",
    "        for i, col in enumerate(y_val.columns)\n",
    "    ]\n",
    "    return np.mean(f1s)\n",
    "\n",
    "# Optuna 스터디 실행\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=30, timeout=600)\n",
    "\n",
    "print(\"▶ Best mean F1:\", study.best_value)\n",
    "print(\"▶ Best params :\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98ee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# Cell 2) 최적 파라미터로 XGBoost 파이프라인 재학습 & 평가\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "best = study.best_params\n",
    "\n",
    "# 최적 파라미터를 반영한 XGB 분류기\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=best[\"n_estimators\"],\n",
    "    learning_rate=best[\"learning_rate\"],\n",
    "    max_depth=best[\"max_depth\"],\n",
    "    subsample=best[\"subsample\"],\n",
    "    colsample_bytree=best[\"colsample_bytree\"],\n",
    "    gamma=best[\"gamma\"],\n",
    "    reg_alpha=best[\"reg_alpha\"],\n",
    "    reg_lambda=best[\"reg_lambda\"],\n",
    "    use_label_encoder=False,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 파이프라인 정의\n",
    "final_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy=\"mean\"),\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(xgb_final)\n",
    ")\n",
    "\n",
    "# 전체 학습\n",
    "final_pipe.fit(X_train, y_train)\n",
    "# 검증 예측\n",
    "y_val_pred = final_pipe.predict(X_val)\n",
    "\n",
    "# per-target & mean Macro-F1 출력\n",
    "f1s_final = {\n",
    "    col: f1_score(y_val[col], y_val_pred[:, i], average=\"macro\")\n",
    "    for i, col in enumerate(y_val.columns)\n",
    "}\n",
    "\n",
    "print(\"▶ Final per-target F1:\", f1s_final)\n",
    "print(\"▶ Final mean F1     :\", np.mean(list(f1s_final.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d1f4f2",
   "metadata": {},
   "source": [
    "## 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2999eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ RF  per-target F1: {'Q1': 0.5480489545030459, 'Q2': 0.52668484612388, 'Q3': 0.505555900428332, 'S1': 0.3229457110054125, 'S2': 0.5238273921200751, 'S3': 0.5904126213592233}\n",
      "▶ RF  mean F1     : 0.5029125709233281\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1) 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2) 피처·타겟 분리 (셀 1과 동일)\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']\n",
    "\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3) 수치형 컬럼만 선택\n",
    "num_feats = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train = X_train[num_feats]\n",
    "X_val   = X_val  [num_feats]\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4) 파이프라인 정의 (imputer → scaler → classifier)\n",
    "rf_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(\n",
    "        RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5) 학습\n",
    "rf_pipe.fit(X_train, y_train)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    rf_pipe.predict(X_val),\n",
    "    index=y_val.index,\n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ RF  per-target F1:\", f1s)\n",
    "print(\"▶ RF  mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b33036",
   "metadata": {},
   "source": [
    "# 모델 적용 - 결측치 평균 채움 \n",
    "- LigthGBM\n",
    "- CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b84fc",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "330cca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 157, number of negative: 158\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.788067 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498413 -> initscore=-0.006349\n",
      "[LightGBM] [Info] Start training from score -0.006349\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 176, number of negative: 139\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.600632 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.558730 -> initscore=0.236010\n",
      "[LightGBM] [Info] Start training from score 0.236010\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 195, number of negative: 120\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.610206 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.619048 -> initscore=0.485508\n",
      "[LightGBM] [Info] Start training from score 0.485508\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.597985 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] Start training from score -1.177862\n",
      "[LightGBM] [Info] Start training from score -0.715620\n",
      "[LightGBM] [Info] Start training from score -1.593690\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 208, number of negative: 107\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.572619 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.660317 -> initscore=0.664709\n",
      "[LightGBM] [Info] Start training from score 0.664709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Info] Number of positive: 208, number of negative: 107\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.565087 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 26913\n",
      "[LightGBM] [Info] Number of data points in the train set: 315, number of used features: 585\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.660317 -> initscore=0.664709\n",
      "[LightGBM] [Info] Start training from score 0.664709\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "▶ LightGBM per-target F1: {'Q1': 0.5401098901098902, 'Q2': 0.5012315270935961, 'Q3': 0.5745798319327731, 'S1': 0.41711016979834187, 'S2': 0.5548751766368347, 'S3': 0.590457804743519}\n",
      "▶ LightGBM mean F1     : 0.5297274000524925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/home/wonjun/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 셀 1) LightGBM 적용\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1) 타겟·드롭 컬럼 정의\n",
    "target_cols = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "drop_cols   = ['subject_id','date','lifelog_date']\n",
    "\n",
    "# 2) 피처·타겟 분리\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# 3) 수치형 컬럼만 선택\n",
    "num_feats = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train = X_train[num_feats]\n",
    "X_val   = X_val  [num_feats]\n",
    "\n",
    "# 4) 파이프라인 정의: impute → scale → LGBM\n",
    "lgbm_pipe = make_pipeline(\n",
    "    SimpleImputer(strategy='mean'),\n",
    "    StandardScaler(),\n",
    "    MultiOutputClassifier(\n",
    "        LGBMClassifier(\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5) 학습\n",
    "lgbm_pipe.fit(X_train, y_train)\n",
    "\n",
    "# 6) 예측 & F1 계산\n",
    "y_pred = pd.DataFrame(\n",
    "    lgbm_pipe.predict(X_val),\n",
    "    index=y_val.index,\n",
    "    columns=target_cols\n",
    ")\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average='macro')\n",
    "       for c in target_cols}\n",
    "print(\"▶ LightGBM per-target F1:\", f1s)\n",
    "print(\"▶ LightGBM mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cfaed2",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e91f7d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ CatBoost per-target F1: {'Q1': 0.6517754239613632, 'Q2': 0.555052790346908, 'Q3': 0.5092664322745963, 'S1': 0.339274629777423, 'S2': 0.5619727449707982, 'S3': 0.5603287841191067}\n",
      "▶ CatBoost mean F1     : 0.529611800908366\n"
     ]
    }
   ],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "# CatBoost 를 MultiOutputClassifier 없이 타깃별로 직접 돌려주는 예제\n",
    "# ──────────────────────────────────────────────────────────────────────────────\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) 전처리기 세팅 (평균 impute + 스케일링)\n",
    "imputer = SimpleImputer(strategy=\"mean\")\n",
    "scaler  = StandardScaler()\n",
    "\n",
    "# 2) X, y, val 데이터 준비 (기존과 동일)\n",
    "X_train_num = imputer.fit_transform(X_train)\n",
    "X_train_num = scaler.fit_transform(X_train_num)\n",
    "X_val_num   = imputer.transform(X_val)\n",
    "X_val_num   = scaler.transform(X_val_num)\n",
    "\n",
    "# 3) 타깃별로 CatBoost 학습 & 예측\n",
    "y_pred = pd.DataFrame(index=y_val.index, columns=target_cols)\n",
    "\n",
    "for col in target_cols:\n",
    "    # 목적에 맞게 파라미터 수정 가능\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=200,\n",
    "        learning_rate=0.1,\n",
    "        depth=6,\n",
    "        random_state=42,\n",
    "        verbose=0,\n",
    "    )\n",
    "    model.fit(X_train_num, y_train[col])\n",
    "    y_pred[col] = model.predict(X_val_num)\n",
    "\n",
    "# 4) per-target & mean Macro-F1 계산\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average=\"macro\")\n",
    "       for c in target_cols}\n",
    "print(\"▶ CatBoost per-target F1:\", f1s)\n",
    "print(\"▶ CatBoost mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f7a48a",
   "metadata": {},
   "source": [
    "# Ensemble (아직 안함))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6324e3",
   "metadata": {},
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "174a61a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ X_train_num shape: (315, 666)\n",
      "▶ X_val_num   shape: (135, 666)\n",
      "▶ y_train     shape: (315, 6)\n",
      "▶ y_val       shape: (135, 6)\n"
     ]
    }
   ],
   "source": [
    "# 1) 라이브러리 임포트\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# (이전 셀에서 이미 train_model_df, val_model_df, target_cols, drop_cols 정의되어 있다고 가정)\n",
    "\n",
    "# 2) 피처·타깃 분리\n",
    "X_train = train_model_df.drop(columns=drop_cols + target_cols)\n",
    "y_train = train_model_df[target_cols]\n",
    "X_val   = val_model_df  .drop(columns=drop_cols + target_cols)\n",
    "y_val   = val_model_df  [target_cols]\n",
    "\n",
    "# 3) 숫자형 피처만 선택\n",
    "num_feats = X_train.select_dtypes(include=[np.number]).columns\n",
    "X_train = X_train[num_feats]\n",
    "X_val   = X_val  [num_feats]\n",
    "\n",
    "# 4) 결측치 평균 대체 → 표준화\n",
    "imp    = SimpleImputer(strategy=\"mean\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_num = imp.fit_transform(X_train)\n",
    "X_train_num = scaler.fit_transform(X_train_num)\n",
    "\n",
    "X_val_num   = imp.transform(X_val)\n",
    "X_val_num   = scaler.transform(X_val_num)\n",
    "\n",
    "print(\"▶ X_train_num shape:\", X_train_num.shape)\n",
    "print(\"▶ X_val_num   shape:\", X_val_num.shape)\n",
    "print(\"▶ y_train     shape:\", y_train.shape)\n",
    "print(\"▶ y_val       shape:\", y_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9dbe24f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-27 23:29:26.768333: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-27 23:29:26.780322: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748356166.791195 1108030 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748356166.794361 1108030 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1748356166.803811 1108030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748356166.803821 1108030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748356166.803822 1108030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1748356166.803823 1108030 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-27 23:29:26.808232: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "I0000 00:00:1748356168.032258 1108030 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46864 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:2a:00.0, compute capability: 8.6\n",
      "I0000 00:00:1748356168.032934 1108030 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 41929 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:3d:00.0, compute capability: 8.6\n",
      "I0000 00:00:1748356168.033425 1108030 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 46866 MB memory:  -> device: 2, name: NVIDIA RTX A6000, pci bus id: 0000:ab:00.0, compute capability: 8.6\n",
      "I0000 00:00:1748356168.034034 1108030 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 46866 MB memory:  -> device: 3, name: NVIDIA RTX A6000, pci bus id: 0000:bd:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">170,752</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">774</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m170,752\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m32,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │           \u001b[38;5;34m774\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">204,422</span> (798.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m204,422\u001b[0m (798.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">204,422</span> (798.52 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m204,422\u001b[0m (798.52 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1748356170.367996 1127465 service.cc:152] XLA service 0x8ece4009c10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1748356170.368046 1127465 service.cc:160]   StreamExecutor device (0): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "I0000 00:00:1748356170.368053 1127465 service.cc:160]   StreamExecutor device (1): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "I0000 00:00:1748356170.368057 1127465 service.cc:160]   StreamExecutor device (2): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "I0000 00:00:1748356170.368061 1127465 service.cc:160]   StreamExecutor device (3): NVIDIA RTX A6000, Compute Capability 8.6\n",
      "2025-05-27 23:29:30.427112: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "E0000 00:00:1748356170.628141 1127465 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1748356170.680014 1127465 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2025-05-27 23:29:30.690285: W tensorflow/core/framework/op_kernel.cc:1857] OP_REQUIRES failed at xla_ops.cc:591 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "2025-05-27 23:29:30.690361: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n",
      "\t [[{{node StatefulPartitionedCall}}]]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_1108030/4286890921.py\", line 30, in <module>\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_1464]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 30\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 2) 학습 (조기종료 콜백)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m es \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[1;32m     27\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mes\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\n\u001b[1;32m     37\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# 3) 예측 & F1 계산\u001b[39;00m\n\u001b[1;32m     40\u001b[0m y_pred_proba \u001b[38;5;241m=\u001b[39m mlp\u001b[38;5;241m.\u001b[39mpredict(X_val_num)\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/wonjun_base/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3077, in run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3132, in _run_cell\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3336, in run_cell_async\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3519, in run_ast_nodes\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n\n  File \"/tmp/ipykernel_1108030/4286890921.py\", line 30, in <module>\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/wonjun/.conda/envs/wonjun_base/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_multi_step_on_iterator_1464]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1) 모델 정의\n",
    "n_feats  = X_train_num.shape[1]\n",
    "n_targets = len(target_cols)\n",
    "\n",
    "mlp = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(n_feats,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_targets, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "mlp.compile(\n",
    "    optimizer=Adam(learning_rate=1e-3),\n",
    "    loss=\"binary_crossentropy\"\n",
    ")\n",
    "\n",
    "mlp.summary()\n",
    "\n",
    "# 2) 학습 (조기종료 콜백)\n",
    "es = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=5, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = mlp.fit(\n",
    "    X_train_num, y_train.values,\n",
    "    validation_data=(X_val_num, y_val.values),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# 3) 예측 & F1 계산\n",
    "y_pred_proba = mlp.predict(X_val_num)\n",
    "# threshold 0.5\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "y_pred = pd.DataFrame(y_pred, index=y_val.index, columns=target_cols)\n",
    "\n",
    "f1s = {c: f1_score(y_val[c], y_pred[c], average=\"macro\") \n",
    "       for c in target_cols}\n",
    "\n",
    "print(\"▶ MLP per-target F1:\", f1s)\n",
    "print(\"▶ MLP mean F1     :\", np.mean(list(f1s.values())))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ace606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wonjun_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
