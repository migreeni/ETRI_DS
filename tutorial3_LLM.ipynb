{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqHMFQVhzfAN"
   },
   "outputs": [],
   "source": [
    "!pip install together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nKhXPRr911mV"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from together import Together\n",
    "from google.colab import userdata # To load API key securely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LtUtCJ4n0W7m"
   },
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAxBV12R0DDG"
   },
   "outputs": [],
   "source": [
    "file_name = f\"/content/drive/MyDrive/KU/TA/Tutorial 3-2. Advanced Topic (LLM)/practice/data/diabetes.csv\" # Your Own Path\n",
    "df = pd.read_csv(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0A5G0Ds2SIl"
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnQ2ak9fcCEs"
   },
   "outputs": [],
   "source": [
    "# Target Attribute\n",
    "target_attribute = df.columns[-1]\n",
    "print(\"Target Attribute:\", target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKW8c_jPNEmp"
   },
   "outputs": [],
   "source": [
    "# Target Class\n",
    "X = df.convert_dtypes()\n",
    "y = df[target_attribute].to_numpy()\n",
    "\n",
    "label_list = np.unique(y).tolist()\n",
    "print(\"Target Class:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayJbt9Tw264S"
   },
   "outputs": [],
   "source": [
    "# Split Pool / Test\n",
    "X_pool, X_test, y_pool, y_test = train_test_split(\n",
    "        X.drop(target_attribute, axis=1),\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r06lZ9s5dndn"
   },
   "outputs": [],
   "source": [
    "shot =  # Number of Training Example\n",
    "\n",
    "X_train = X_pool.copy()\n",
    "X_train[target_attribute] = y_pool\n",
    "\n",
    "sampled_list = []\n",
    "remainder = shot % len(np.unique(y_pool))\n",
    "\n",
    "# Balance Class in Training Example\n",
    "for _, grouped in X_train.groupby(target_attribute):\n",
    "    sample_num = shot // len(np.unique(y_pool))\n",
    "    if remainder > 0:\n",
    "        sample_num += 1\n",
    "        remainder -= 1\n",
    "\n",
    "    sampled = grouped.sample(sample_num, random_state=42)\n",
    "    sampled_list.append(sampled)\n",
    "\n",
    "X_balanced = pd.concat(sampled_list)\n",
    "X_train = X_balanced.drop([target_attribute], axis=1)\n",
    "y_train = X_balanced[target_attribute].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXe3GaaA8ThO"
   },
   "outputs": [],
   "source": [
    "train = pd.DataFrame(X_train)\n",
    "train[target_attribute] = y_train\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfuvDf7JEL4T"
   },
   "outputs": [],
   "source": [
    "test = pd.DataFrame(X_test)\n",
    "test[target_attribute] = y_test\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPLk2RODf9Ac"
   },
   "outputs": [],
   "source": [
    "df[target_attribute].value_counts() # Original Data Class Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBxPwonTgDCN"
   },
   "outputs": [],
   "source": [
    "train[target_attribute].value_counts() # Train Data Class Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XcmYn3rMfvPN"
   },
   "outputs": [],
   "source": [
    "test[target_attribute].value_counts() # Test Data Class Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyoG10nduISS"
   },
   "source": [
    "## Generate Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8TQxL-uAJQm"
   },
   "outputs": [],
   "source": [
    "# Base Template for LLM Feature Generation\n",
    "\n",
    "template_str = \"\"\"\n",
    "You are an expert. Given the task description and the list of features and data examples, you are extracting conditions for each answer class to solve the task.\n",
    "\n",
    "Task:[TASK]\n",
    "\n",
    "Features:\n",
    "[FEATURES]\n",
    "\n",
    "Examples:\n",
    "[EXAMPLES]\n",
    "\n",
    "Let's first understand the problem and solve the problem step by step.\n",
    "\n",
    "Step 1. Analyze the causal relationship or tendency between each feature and task description based on general knowledge and common sense within a short sentence.\n",
    "\n",
    "Step 2. Based on the above examples and Step 1's results, infer [NUMBER] different conditions per answer, following the format below.\n",
    "The condition should make sense, well match examples, and must match the format for [condition] according to value type.\n",
    "\n",
    "Format for Response:\n",
    "[FEATURE FORMAT]\n",
    "\n",
    "Format for [Feature Condition]:\n",
    "For the categorical variable only,\n",
    "- [Feature_name] is in [list of Categorical_values]\n",
    "For the numerical variable only,\n",
    "- [Feature_name] (> or >= or < or <=) [Numerical_value]\n",
    "- [Feature_name] is within range of [Numerical_range_start, Numerical_range_end]\n",
    "\n",
    "Answer:\n",
    "Step 1. The relationship between each feature and the task description:\n",
    "\n",
    "Step 2.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDYJnO5vzLA"
   },
   "source": [
    "### 1. Task Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdHH_133xg7-"
   },
   "outputs": [],
   "source": [
    "task_desc = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0vcZC1ixAhJ"
   },
   "source": [
    "### 2. Feature Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt5YbneMw3Qk"
   },
   "outputs": [],
   "source": [
    "feature_desc = '''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4O3hnPXNjO4x"
   },
   "outputs": [],
   "source": [
    "feature_rule_number =  # Number of rules per Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHZqX8qnO_Gr"
   },
   "outputs": [],
   "source": [
    "format_list = [f'{feature_rule_number} different conditions for class \"{label}\":\\n- [Condition]\\n...' for label in label_list]\n",
    "format_desc = '\\n\\n'.join(format_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUxLOkUhxqga"
   },
   "source": [
    "### 3. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qlvqk32UFeAD"
   },
   "outputs": [],
   "source": [
    "# Train Example (Tabular) -> Text\n",
    "def serialize(row):\n",
    "    target_str = f\"\"\n",
    "    for attr_idx, attr_name in enumerate(list(row.index)):\n",
    "        if attr_idx < len(list(row.index)) - 1:\n",
    "            target_str += \" is \".join([attr_name, str(row[attr_name]).strip(\" .'\").strip('\"').strip()])\n",
    "            target_str += \". \"\n",
    "        else:\n",
    "            if len(attr_name.strip()) < 2:\n",
    "                continue\n",
    "            target_str += \" is \".join([attr_name, str(row[attr_name]).strip(\" .'\").strip('\"').strip()])\n",
    "            target_str += \".\"\n",
    "    return target_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mX2iq7kpGuw2"
   },
   "outputs": [],
   "source": [
    "def fill_in_templates(fill_in_dict, template_str):\n",
    "    for key, value in fill_in_dict.items():\n",
    "        if key in template_str:\n",
    "            template_str = template_str.replace(key, value)\n",
    "    return template_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jb2vtUGJSzp"
   },
   "outputs": [],
   "source": [
    "in_context_desc = \"\"\n",
    "df_current = train.copy()\n",
    "df_current = df_current.groupby(\n",
    "                target_attribute, group_keys=False\n",
    "            ).apply(lambda x: x.sample(frac=1))\n",
    "\n",
    "for icl_idx, icl_row in df_current.iterrows():\n",
    "  answer = icl_row[target_attribute]\n",
    "  icl_row = icl_row.drop(labels=target_attribute)\n",
    "  in_context_desc += serialize(icl_row)\n",
    "  in_context_desc += f\"\\nAnswer: {answer}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VV-2FfJrKG3f"
   },
   "outputs": [],
   "source": [
    "fill_in_dict = {\n",
    "                \"[TASK]\": task_desc,\n",
    "                \"[EXAMPLES]\": in_context_desc,\n",
    "                \"[FEATURES]\": feature_desc,\n",
    "                \"[FEATURE FORMAT]\": format_desc,\n",
    "                \"[NUMBER]\": str(feature_rule_number)\n",
    "            }\n",
    "prompt = fill_in_templates(fill_in_dict, template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUdbJgN7FwSn"
   },
   "outputs": [],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAD_4V6jQ1lq"
   },
   "source": [
    "## Prompt for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS3PmUu2O5oB"
   },
   "outputs": [],
   "source": [
    "# Set API Key for Inference\n",
    "client = Together(api_key=userdata.get(\"TOGETHER_API\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_b4__TzwHAS2"
   },
   "outputs": [],
   "source": [
    "max_try_num=5\n",
    "curr_try_num = 0\n",
    "while curr_try_num < max_try_num:\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\", # Free Model\n",
    "            messages=[{\"role\":\"user\", \"content\":_____}], # Fill in the blank!\n",
    "            max_tokens=1024)\n",
    "        result = response.choices[0].message.content\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        curr_try_num += 1\n",
    "        if curr_try_num >= max_try_num:\n",
    "            result = -1\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGmmA6o_P86X"
   },
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw0aTFXRRCVh"
   },
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv0BT5UFFwlS"
   },
   "outputs": [],
   "source": [
    "# Parse Rules\n",
    "splitter = \"onditions for class\"\n",
    "\n",
    "if splitter in result:\n",
    "    splitted = result.split(splitter)\n",
    "    if len(label_list) != 0 and len(splitted) == len(label_list) + 1:\n",
    "        rule_raws = splitted[1:]\n",
    "        rule_dict = {}\n",
    "        for rule_raw in rule_raws:\n",
    "            class_name = rule_raw.split(\":\")[0].strip(\" .'\").strip(' []\"')\n",
    "            rule_parsed = []\n",
    "            for txt in rule_raw.strip().split(\"\\n\")[1:]:\n",
    "                if len(txt) < 2:\n",
    "                    break\n",
    "                rule_parsed.append(\" \".join(txt.strip().split(\" \")[1:]))\n",
    "            rule_dict[class_name] = rule_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SIq4nDKjRTqp"
   },
   "outputs": [],
   "source": [
    "rule_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8_BEDrDSEPh"
   },
   "outputs": [],
   "source": [
    "# Base Template for LLM Code Generation\n",
    "prompt_code = '''\n",
    "Provide me a python code for function, given description below.\n",
    "\n",
    "Function name: [NAME]\n",
    "\n",
    "Input: Dataframe df_input\n",
    "\n",
    "Input Features:\n",
    "[FEATURES]\n",
    "\n",
    "Output: Dataframe df_output. Create a new dataframe df_output. Each column in df_output refers whether the selected column in df_input follows the condition (1) or not (0). Be sure that the function code well matches with its feature type (i.e., numerical, categorical).\n",
    "\n",
    "Conditions:\n",
    "[CONDITIONS]\n",
    "\n",
    "\n",
    "Wrap only the function part with <start> and <end>, and do not add any comments, descriptions, and package importing lines in the code.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7DjI_o5K3EF"
   },
   "outputs": [],
   "source": [
    "template_list = []\n",
    "for class_id, each_rule in rule_dict.items():\n",
    "    function_name = f'extracting_features_{class_id}'\n",
    "    rule_str = '\\n'.join([f'- {k}' for k in each_rule])\n",
    "\n",
    "    fill_in_dict = {\n",
    "        \"[NAME]\": function_name,\n",
    "        \"[CONDITIONS]\": rule_str,\n",
    "        \"[FEATURES]\": feature_desc\n",
    "    }\n",
    "    template = fill_in_templates(fill_in_dict, prompt_code)\n",
    "    template_list.append(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gvEOkJiuPA9M"
   },
   "outputs": [],
   "source": [
    "print(template_list[0]) # Prompt for Class \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j5V7f1OCPpwh"
   },
   "outputs": [],
   "source": [
    "print(template_list[1]) # Class \"yes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niK31s4dP_f8"
   },
   "outputs": [],
   "source": [
    "fct_results = []\n",
    "for prompt in tqdm(template_list):\n",
    "    curr_try_num = 0\n",
    "    while curr_try_num < 5:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                max_tokens=1024)\n",
    "            result = response.choices[0].message.content\n",
    "            fct_results.append(result)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            curr_try_num += 1\n",
    "            if curr_try_num >= max_try_num:\n",
    "                fct_results.append(-1)\n",
    "            time.sleep(10)\n",
    "\n",
    "# Extract function code from LLM response\n",
    "fct_strs = [fct_txt.split('<start>')[1].split('<end>')[0].strip() for fct_txt in fct_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkF6kITolp50"
   },
   "outputs": [],
   "source": [
    "# Parse function names\n",
    "fct_names = []\n",
    "fct_strs_final = []\n",
    "\n",
    "if 'def' in fct_strs[0]:\n",
    "    for fct_str in fct_strs:\n",
    "        fct_names.append(fct_str.split('def')[1].split('(')[0].strip())\n",
    "    fct_strs_final = fct_strs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8VMftMzdQgmF"
   },
   "outputs": [],
   "source": [
    "fct_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPLJcKSbmJuG"
   },
   "outputs": [],
   "source": [
    "print(fct_strs_final[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7i1__uCpmSBe"
   },
   "outputs": [],
   "source": [
    "print(fct_strs_final[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of1goSJtmu0X"
   },
   "outputs": [],
   "source": [
    "def convert_to_binary_vectors(fct_strs, fct_names, label_list, X_train, X_test):\n",
    "    X_train_dict, X_test_dict = {}, {}\n",
    "\n",
    "    # Match function names to class labels\n",
    "    fct_idx_dict = {}\n",
    "    for idx, name in enumerate(fct_names):\n",
    "        for label in label_list:\n",
    "            label_name = '_'.join(label.split(' '))\n",
    "            if label_name.lower() in name.lower():\n",
    "                fct_idx_dict[label] = idx\n",
    "\n",
    "    # Check if all class labels are matched\n",
    "    if len(fct_idx_dict) != len(label_list):\n",
    "        raise ValueError(\"Mismatch between rules and label classes\")\n",
    "\n",
    "    for label in label_list:\n",
    "        fct_idx = fct_idx_dict[label]\n",
    "        exec(fct_strs[fct_idx].strip('` \"'))\n",
    "        func = locals()[fct_names[fct_idx]]\n",
    "        X_train_each = func(X_train).astype('int').to_numpy()\n",
    "        X_test_each = func(X_test).astype('int').to_numpy()\n",
    "        assert X_train_each.shape[1] == X_test_each.shape[1]\n",
    "        X_train_dict[label] = torch.tensor(X_train_each).float()\n",
    "        X_test_dict[label] = torch.tensor(X_test_each).float()\n",
    "\n",
    "    return X_train_dict, X_test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVvhtCHBm1wq"
   },
   "outputs": [],
   "source": [
    "X_train_dict, X_test_dict = convert_to_binary_vectors(fct_strs_final, fct_names, label_list, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QH3PbIoem4ZG"
   },
   "outputs": [],
   "source": [
    "X_train_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPwmDBuym5WI"
   },
   "outputs": [],
   "source": [
    "X_test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhTHs7m9Ww9m"
   },
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pSfnUTzWzSy"
   },
   "outputs": [],
   "source": [
    "class simple_model(nn.Module):\n",
    "    def __init__(self, X):\n",
    "        super(simple_model, self).__init__()\n",
    "        self.weights = nn.ParameterList([nn.Parameter(torch.ones(x_each.shape[1] , 1) / x_each.shape[1]) for x_each in X])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_total_score = []\n",
    "        for idx, x_each in enumerate(x):\n",
    "            x_score = x_each @ torch.clamp(self.weights[idx], min=0)\n",
    "            x_total_score.append(x_score)\n",
    "        x_total_score = torch.cat(x_total_score, dim=-1)\n",
    "        return x_total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjAK0DcHXFyF"
   },
   "outputs": [],
   "source": [
    "def train(X_train_now, label_list, shot):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if shot // len(label_list) == 1:\n",
    "        model = simple_model(X_train_now)\n",
    "        opt = Adam(model.parameters(), lr=1e-2)\n",
    "        for _ in range(200):\n",
    "            opt.zero_grad()\n",
    "            outputs = model(X_train_now)\n",
    "            preds = outputs.argmax(dim=1).numpy()\n",
    "            acc = (np.array(y_train_num) == preds).sum() / len(preds)\n",
    "            if acc == 1:\n",
    "                break\n",
    "            loss = criterion(outputs, torch.tensor(y_train_num))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "    else:\n",
    "        if shot // len(label_list) <= 2:\n",
    "            n_splits = 2\n",
    "        else:\n",
    "            n_splits = 4\n",
    "\n",
    "        kfold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n",
    "        model_list = []\n",
    "        for fold, (train_ids, valid_ids) in enumerate(kfold.split(X_train_now[0], y_train_num)):\n",
    "            model = simple_model(X_train_now)\n",
    "            opt = Adam(model.parameters(), lr=1e-2)\n",
    "            X_train_now_fold = [x_train_now[train_ids] for x_train_now in X_train_now]\n",
    "            X_valid_now_fold = [x_train_now[valid_ids] for x_train_now in X_train_now]\n",
    "            y_train_fold = y_train_num[train_ids]\n",
    "            y_valid_fold = y_train_num[valid_ids]\n",
    "\n",
    "            max_acc = -1\n",
    "            for _ in range(200):\n",
    "                opt.zero_grad()\n",
    "                outputs = model(X_train_now_fold)\n",
    "                loss = criterion(outputs, torch.tensor(y_train_fold))\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                valid_outputs = model(X_valid_now_fold)\n",
    "                preds = valid_outputs.argmax(dim=1).numpy()\n",
    "                acc = (np.array(y_valid_fold) == preds).sum() / len(preds)\n",
    "                if max_acc < acc:\n",
    "                    max_acc = acc\n",
    "                    final_model = copy.deepcopy(model)\n",
    "                    if max_acc >= 1:\n",
    "                        break\n",
    "            model_list.append(final_model)\n",
    "\n",
    "        sdict = model_list[0].state_dict()\n",
    "        for key in sdict:\n",
    "            sdict[key] = torch.stack([model.state_dict()[key] for model in model_list], dim=0).mean(dim=0)\n",
    "\n",
    "        model = simple_model(X_train_now)\n",
    "        model.load_state_dict(sdict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abCvYmOtYlut"
   },
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChA204-NYTGA"
   },
   "outputs": [],
   "source": [
    "def evaluate(pred_probs, answers, multiclass=False):\n",
    "    if multiclass == False:\n",
    "        result_auc = roc_auc_score(answers, pred_probs[:, 1])\n",
    "    else:\n",
    "        result_auc = roc_auc_score(answers, pred_probs, multi_class='ovr', average='macro')\n",
    "    return result_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5xJhgRInCph"
   },
   "outputs": [],
   "source": [
    "X_train_now = list(X_train_dict.values())\n",
    "X_test_now = list(X_test_dict.values())\n",
    "\n",
    "# Convert labels to numeric\n",
    "y_train_num = np.array([label_list.index(k) for k in y_train])\n",
    "y_test_num = np.array([label_list.index(k) for k in y_test])\n",
    "multiclass = len(label_list) > 2\n",
    "\n",
    "# Train\n",
    "trained_model = train(X_train_now, label_list, shot)\n",
    "\n",
    "# Predict\n",
    "test_outputs = trained_model(X_test_now).detach().cpu()\n",
    "test_outputs = F.softmax(test_outputs, dim=1).detach()\n",
    "\n",
    "# Evaluate\n",
    "result_auc = evaluate(test_outputs.numpy(), y_test_num, multiclass=multiclass)\n",
    "print(\"AUC:\", result_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3k7y57enu77"
   },
   "source": [
    "## ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lkgukRqOnfpK"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test):\n",
    "    results = []\n",
    "    # Logistic Regression\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "    logreg.fit(X_train, y_train)\n",
    "    y_prob_logreg = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # RandomForest\n",
    "    rf = RandomForestClassifier(class_weight='balanced')\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_prob_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # XGBoost\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "    X_train.columns = X_train.columns.astype(str)\n",
    "    X_test.columns = X_test.columns.astype(str)\n",
    "\n",
    "    xgb.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_prob_xgb = xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # AUC\n",
    "    results.append((\"LogReg\", roc_auc_score(y_test, y_prob_logreg)))\n",
    "    results.append((\"RandomForest\", roc_auc_score(y_test, y_prob_rf)))\n",
    "    results.append((\"XGBoost\", roc_auc_score(y_test, y_prob_xgb)))\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "JTSTeWgOnyX-"
   },
   "outputs": [],
   "source": [
    "y_train = np.where(y_train == label_list[1], 1, 0)\n",
    "y_test = np.where(y_test == label_list[1], 1, 0)\n",
    "experiment_results = train_and_evaluate_models(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jt5aqUHzxO_"
   },
   "outputs": [],
   "source": [
    "experiment_results.append((\"Ours\", result_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYeE-P0ZpUPE"
   },
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(experiment_results, columns=[\"Model\", \"AUC\"])\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOqs/N3wt12UDkb8F4w1R+v",
   "mount_file_id": "15bXCO1PG8eBYjP86aIpniJ9M2kwZRWRE",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
