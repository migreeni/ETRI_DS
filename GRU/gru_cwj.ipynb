{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d155b048",
   "metadata": {},
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44915715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ merged_df_cwj.csv Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV ÌååÏùº Î∂àÎü¨Ïò§Í∏∞\n",
    "df = pd.read_csv('../data/merged_df_original.csv')\n",
    "\n",
    "# 'Ï∫êÏãúÏõåÌÅ¨'Î∂ÄÌÑ∞ 'Ïö∞Ï≤¥Íµ≠Î≥¥Ìóò'ÍπåÏßÄÏùò Ïó¥ Ïù¥Î¶Ñ Î™©Î°ù Ï∂îÏ∂ú\n",
    "start_col = 'Ï∫êÏãúÏõåÌÅ¨'\n",
    "end_col = 'Ïö∞Ï≤¥Íµ≠Î≥¥Ìóò'\n",
    "cols_to_sum = df.loc[:, start_col:end_col].columns\n",
    "\n",
    "df['m_usagestats'] = df[cols_to_sum].sum(axis=1)\n",
    "\n",
    "df.drop(columns=cols_to_sum, inplace=True)\n",
    "\n",
    "merged_df_cwj = df.copy()\n",
    "\n",
    "merged_df_cwj.rename(columns={'m_wtb_rssi_x': 'm_wtb_rssi', 'm_wtb_rssi_y': 'm_wtw_rssi'}, inplace=True)\n",
    "\n",
    "# CSVÎ°ú ÎÇ¥Î≥¥ÎÇ¥Í∏∞\n",
    "merged_df_cwj.to_csv('../data/merged_df_cwj.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"‚úÖ merged_df_cwj.csv Ï†ÄÏû• ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad28eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Í≤∞Ï∏°Ïπò Í∞úÏàò:\n",
      "met_activity     2690\n",
      "m_wtb_rssi      77337\n",
      "m_wtw_rssi      22842\n",
      "heart_rate      55846\n",
      "distance        22852\n",
      "latitude        16044\n",
      "longitude       16044\n",
      "altitude        16044\n",
      "speed           16044\n",
      "m_usagestats        0\n",
      "w_light         19040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/merged_df_cwj.csv')\n",
    "\n",
    "# Í≤ÄÏÇ¨Ìï† Ïó¥ Î™©Î°ù\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# Í∞Å Ïó¥Ïùò Í≤∞Ï∏°Ïπò Í∞úÏàò Ï∂úÎ†•\n",
    "missing_counts = df[cols_to_check].isnull().sum()\n",
    "print(\"üìä Í≤∞Ï∏°Ïπò Í∞úÏàò:\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157bbe6",
   "metadata": {},
   "source": [
    "### Í≤∞Ï∏°ÏπòÎ•º Í∞ÄÏû• Í∞ÄÍπåÏö¥ Ïù¥ÏõÉÏùò Í∞í(ÏïûÎí§ Í∏∞Ï§Ä)ÏúºÎ°ú Ï±ÑÏõÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce709a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Í≤∞Ï∏°Ïπò Î≥¥Í∞Ñ ÏôÑÎ£å. ÎÇ®ÏùÄ Í≤∞Ï∏°Ïπò Ïàò:\n",
      "met_activity    0\n",
      "m_wtb_rssi      0\n",
      "m_wtw_rssi      0\n",
      "heart_rate      0\n",
      "distance        0\n",
      "latitude        0\n",
      "longitude       0\n",
      "altitude        0\n",
      "speed           0\n",
      "m_usagestats    0\n",
      "w_light         0\n",
      "dtype: int64\n",
      "üìÅ '../data/merged_df_cwj_filled.csv' Ï†ÄÏû• ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "#ÎåÄÏÉÅ Ïó¥ Î™©Î°ù\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Î≥¥Í∞Ñ Ìï®Ïàò (ÏïûÎí§ ÌèâÍ∑†)\n",
    "def fill_nearest_avg(series):\n",
    "    forward = series.ffill()  # ÏïûÏ™Ω Í∞íÏúºÎ°ú Ï±ÑÏö∞Í∏∞\n",
    "    backward = series.bfill()  # Îí§Ï™Ω Í∞íÏúºÎ°ú Ï±ÑÏö∞Í∏∞\n",
    "    filled = series.copy()\n",
    "    \n",
    "    # ÏïûÎí§ Í∞íÏù¥ Î™®Îëê ÏûàÎäî Í≤ΩÏö∞ ÌèâÍ∑†ÏúºÎ°ú\n",
    "    for i in series[series.isnull()].index:\n",
    "        f, b = forward[i], backward[i]\n",
    "        if pd.notnull(f) and pd.notnull(b):\n",
    "            filled[i] = (f + b) / 2\n",
    "        elif pd.notnull(f):\n",
    "            filled[i] = f\n",
    "        elif pd.notnull(b):\n",
    "            filled[i] = b\n",
    "    return filled\n",
    "\n",
    "# 'burned_calories' Ïó¥Ïùò Í≤∞Ï∏°ÏπòÎ•º 0ÏúºÎ°ú Ï±ÑÏö∞Í∏∞\n",
    "df['burned_calories'] = df['burned_calories'].fillna(0)\n",
    "\n",
    "# Í∞Å Ïó¥Ïóê ÎåÄÌï¥ Í≤∞Ï∏°Ïπò Ï≤òÎ¶¨\n",
    "for col in cols_to_check:\n",
    "    df[col] = fill_nearest_avg(df[col])\n",
    "\n",
    "# ÌôïÏù∏Ïö© Ï∂úÎ†• (ÏÑ†ÌÉùÏÇ¨Ìï≠)\n",
    "print(\"‚úÖ Í≤∞Ï∏°Ïπò Î≥¥Í∞Ñ ÏôÑÎ£å. ÎÇ®ÏùÄ Í≤∞Ï∏°Ïπò Ïàò:\")\n",
    "print(df[cols_to_check].isnull().sum())\n",
    "\n",
    "# Ï†ÄÏû•\n",
    "df.to_csv('../data/merged_df_cwj_filled.csv', index=False)\n",
    "print(\"üìÅ '../data/merged_df_cwj_filled.csv' Ï†ÄÏû• ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda8fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä [cols_to_check Ïô∏] Í≤∞Ï∏°ÏπòÍ∞Ä ÏûàÎäî Ïó¥Îì§:\n",
      "Music                        3717\n",
      "Vehicle                      3717\n",
      "Motor vehicle (road)         3717\n",
      "Outside, urban or manmade    3717\n",
      "Outside, rural or natural    3717\n",
      "                             ... \n",
      "Heavy metal                  3717\n",
      "Double bass                  3717\n",
      "Drum and bass                3717\n",
      "String section               3717\n",
      "Punk rock                    3717\n",
      "Length: 517, dtype: int64\n",
      "‚úÖ Í≤∞Ï∏°ÏπòÎ•º 0ÏúºÎ°ú Ï±ÑÏö¥ ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: merged_df_cwj_tozero.csv\n",
      "‚úÖ Í≤∞Ï∏°Ïπò Ìñâ ÏÇ≠Ï†ú ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: merged_df_cwj_delete.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ÎÇòÎ®∏ÏßÄ Ïó¥Îì§ ÏÑ†ÌÉù\n",
    "remaining_cols = [col for col in df.columns if col not in cols_to_check]\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Í∞úÏàò ÌôïÏù∏\n",
    "missing_counts = df[remaining_cols].isnull().sum()\n",
    "\n",
    "# Í≤∞Ï∏°ÏπòÍ∞Ä Ï°¥Ïû¨ÌïòÎäî Ïó¥Îßå Ï∂úÎ†•\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(\"üìä [cols_to_check Ïô∏] Í≤∞Ï∏°ÏπòÍ∞Ä ÏûàÎäî Ïó¥Îì§:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# Í≤∞Ï∏°ÏπòÎ•º 0ÏúºÎ°ú Ï±ÑÏõå Ï†ÄÏû•\n",
    "df_zero_filled = df.copy()\n",
    "df_zero_filled[remaining_cols] = df_zero_filled[remaining_cols].fillna(0)\n",
    "df_zero_filled.to_csv('../data/merged_df_cwj_tozero.csv', index=False)\n",
    "print(\"‚úÖ Í≤∞Ï∏°ÏπòÎ•º 0ÏúºÎ°ú Ï±ÑÏö¥ ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: merged_df_cwj_tozero.csv\")\n",
    "\n",
    "# Í≤∞Ï∏°ÏπòÍ∞Ä ÏûàÎäî ÌñâÏùÑ ÏÇ≠Ï†úÌïòÏó¨ Ï†ÄÏû•\n",
    "df_dropna = df.copy()\n",
    "df_dropna = df_dropna.dropna(subset=remaining_cols)\n",
    "df_dropna.to_csv('../data/merged_df_cwj_delete.csv', index=False)\n",
    "print(\"‚úÖ Í≤∞Ï∏°Ïπò Ìñâ ÏÇ≠Ï†ú ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å: merged_df_cwj_delete.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ea2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_zero_filled Í≤∞Ï∏°Ïπò Ï¥ù Í∞úÏàò: 0\n",
      "df_dropna Í≤∞Ï∏°Ïπò Ï¥ù Í∞úÏàò: 0\n",
      "(99190, 541) (95473, 541)\n"
     ]
    }
   ],
   "source": [
    "# Í≤∞Ï∏°Ïπò Í∞úÏàò ÏÑ∏Í∏∞\n",
    "missing_zero_filled = df_zero_filled.isnull().sum().sum()\n",
    "missing_dropna = df_dropna.isnull().sum().sum()\n",
    "\n",
    "print(f\"df_zero_filled Í≤∞Ï∏°Ïπò Ï¥ù Í∞úÏàò: {missing_zero_filled}\")\n",
    "print(f\"df_dropna Í≤∞Ï∏°Ïπò Ï¥ù Í∞úÏàò: {missing_dropna}\")\n",
    "\n",
    "print(df_zero_filled.shape, df_dropna.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfc549",
   "metadata": {},
   "source": [
    "# train, test ÏÖã Ï§ÄÎπÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14215e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df_zero_filled shape: (99190, 541)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "print(\"‚úÖ df_zero_filled shape:\", df_zero_filled.shape)\n",
    "#df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "785bf30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "24        id01  2024-07-24   2024-07-23   0   1   1   0   0   1\n",
      "17        id01  2024-07-15   2024-07-14   0   1   0   0   1   1\n",
      "66        id02  2024-08-19   2024-08-18   0   1   1   1   0   1\n",
      "148       id04  2024-09-02   2024-09-01   0   0   1   1   1   0\n",
      "249       id06  2024-07-03   2024-07-02   0   1   1   1   1   1\n",
      "31        id01  2024-08-21   2024-08-20   0   0   1   0   0   1\n",
      "84        id02  2024-09-24   2024-09-23   0   1   1   1   1   1\n",
      "307       id07  2024-08-02   2024-08-01   0   0   0   0   0   0\n",
      "406       id09  2024-08-24   2024-08-23   0   0   0   0   1   0\n",
      "389       id09  2024-07-18   2024-07-17   1   0   0   1   1   1\n",
      "\n",
      "df_val.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "407       id09  2024-08-25   2024-08-24   1   0   0   1   1   1\n",
      "444       id10  2024-09-03   2024-09-02   0   0   0   0   0   0\n",
      "117       id03  2024-09-07   2024-09-06   1   1   0   1   0   0\n",
      "30        id01  2024-08-20   2024-08-19   0   1   1   0   1   1\n",
      "415       id09  2024-09-03   2024-09-02   1   1   1   1   0   0\n",
      "157       id04  2024-09-17   2024-09-16   1   0   0   1   1   1\n",
      "325       id08  2024-07-01   2024-06-30   0   1   0   0   1   1\n",
      "447       id10  2024-09-09   2024-09-08   1   1   1   0   1   1\n",
      "268       id06  2024-08-11   2024-08-10   0   1   1   1   1   1\n",
      "297       id07  2024-07-07   2024-07-06   1   0   1   0   0   0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 9), (360, 9))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "\n",
    "df_train, df_val = train_test_split(train_val_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"df_train.head():\")\n",
    "print(df_train.head(10))\n",
    "print(\"\\ndf_val.head():\")\n",
    "print(df_val.head(10))\n",
    "\n",
    "\n",
    "df_val.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e461298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   0\n",
      "\n",
      "df_val_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define metric columns\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Split df_train and df_val for each metric\n",
    "for metric in metrics:\n",
    "    globals()[f'df_train_{metric.lower()}'] = df_train[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "    globals()[f'df_val_{metric.lower()}'] = df_val[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "\n",
    "# Display head of all 12 dataframes\n",
    "for metric in metrics:\n",
    "    print(f\"df_train_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_train_{metric.lower()}'].head())\n",
    "    print(f\"\\ndf_val_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_val_{metric.lower()}'].head())\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526449",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b98986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total sequences built : 700\n",
      "Train tensor shape : (360, 144, 529) (360, 1)\n",
      "Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.5876\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.5847\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5553\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.5217\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.5207\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5174\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5375\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5662\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5574\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5331\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5869\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.5207\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.5694\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df_merged = df_zero_filled.copy()\n",
    "# timestamp ‚Üí datetime Î≥ÄÌôò\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "# lifelog_date = ÎÇ†Ïßú(Î¨∏ÏûêÏó¥) ; 2024-06-26 Í∞ôÏùÄ ÌòïÏãù\n",
    "df_merged[\"lifelog_date\"] = df_merged[\"timestamp\"].dt.date.astype(str)\n",
    "\n",
    "# -------- 2. feature / sensor column Ï†ïÏùò -----------\n",
    "drop_cols = [\"timestamp\",                # ÏãúÍ∞ÑÏùÄ ÏòàÏ∏°Ïóê Î∂àÌïÑÏöî\n",
    "             \"subject_id\",               # Îß§ÏπòÏö©\n",
    "             \"lifelog_date\"] + [c for c in df_merged.columns if c.startswith(\"id\")]  # one-hot id\n",
    "\n",
    "sensor_cols = [c for c in df_merged.columns if c not in drop_cols]\n",
    "# ÌïòÎ£® Îãπ ÏµúÎåÄ 144 ÌÉÄÏûÑÏä§ÌÖùÏúºÎ°ú Ìå®Îî©\n",
    "MAX_SEQ_LEN = 144\n",
    "\n",
    "\n",
    "# -------- 3. ÏãúÌÄÄÏä§ Î¨∂Îäî Ìï®Ïàò -----------\n",
    "def build_sequences(df):\n",
    "    \"\"\"(subject_id, lifelog_date) ‚Üí ndarray(seq_len, n_feat)\"\"\"\n",
    "    seq_dict = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        # 10-Î∂Ñ Í∞ÑÍ≤© Î≥¥Ïû• ÏïàÎê† ÏàòÎèÑ ÏûàÏúºÎãà timestamp Í∏∞Ï§Ä Ï†ïÎ†¨\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[sensor_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # Í∏∏Ïù¥ Ï°∞Ï†ï\n",
    "        if len(x) > MAX_SEQ_LEN:          # ÏûòÎùºÎÇ¥Í∏∞\n",
    "            x = x[:MAX_SEQ_LEN]\n",
    "        if len(x) < MAX_SEQ_LEN:          # 0-Ìå®Îî©\n",
    "            pad = np.zeros((MAX_SEQ_LEN - len(x), x.shape[1]), np.float32)\n",
    "            x = np.vstack([x, pad])\n",
    "\n",
    "        seq_dict[(sid, day)] = x          # shape = (144, n_feat)\n",
    "    return seq_dict\n",
    "\n",
    "sequence_dict = build_sequences(df_merged)\n",
    "print(\"# total sequences built :\", len(sequence_dict))\n",
    "\n",
    "# ---------- 3. Train / Val Tensor Ï§ÄÎπÑ ----------\n",
    "def rows_to_tensors(df_label):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # ÎàÑÎùΩÎêú ÎÇ†Ïßú skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row['Q1']])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q1)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q1)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "\n",
    "# ---------- 4. Feature Ï†ïÍ∑úÌôî ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU Î™®Îç∏ ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. ÌïôÏäµ Î£®ÌîÑ ----------\n",
    "EPOCHS = 13\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # --- val ---\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = model(xb).cpu().numpy()\n",
    "            preds.append(prob)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    y_hat = (preds >= 0.5).astype(int)\n",
    "    f1 = f1_score(trues, y_hat, average='macro')\n",
    "    print(f\"Epoch {epoch:2d} | Val macro‚ÄëF1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b6dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q1] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.5264\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.5342\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5179\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.5104\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.5031\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5031\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5187\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5654\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5654\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5036\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5676\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.5759\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.5264\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.5636\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.5342\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.5569\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.5537\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.5569\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5486\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.5750\n",
      "[Q2] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q2] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.5328\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.5403\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5403\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.5759\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.5503\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5588\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5593\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5767\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.6063\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5537\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5909\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.5673\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.6112\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.6021\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.5759\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.5817\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.5944\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.5930\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5507\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.5593\n",
      "[Q3] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q3] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.5878\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.5680\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.5588\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.5588\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5853\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5661\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5786\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5503\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.6121\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.6277\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.5971\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.6156\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.6021\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.5680\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.5840\n",
      "[S1] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S1] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.5930\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.6027\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.6021\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.5726\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5817\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5971\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.6063\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.6184\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.5909\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.6021\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.6092\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.6404\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.6250\n",
      "[S2] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S2] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.6000\n",
      "[S3] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S3] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  2 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  3 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch  4 | Val macro‚ÄëF1: 0.6000\n",
      "Epoch  5 | Val macro‚ÄëF1: 0.6156\n",
      "Epoch  6 | Val macro‚ÄëF1: 0.6063\n",
      "Epoch  7 | Val macro‚ÄëF1: 0.6063\n",
      "Epoch  8 | Val macro‚ÄëF1: 0.5909\n",
      "Epoch  9 | Val macro‚ÄëF1: 0.5909\n",
      "Epoch 10 | Val macro‚ÄëF1: 0.5930\n",
      "Epoch 11 | Val macro‚ÄëF1: 0.5846\n",
      "Epoch 12 | Val macro‚ÄëF1: 0.5746\n",
      "Epoch 13 | Val macro‚ÄëF1: 0.5673\n",
      "Epoch 14 | Val macro‚ÄëF1: 0.5944\n",
      "Epoch 15 | Val macro‚ÄëF1: 0.6250\n",
      "Epoch 16 | Val macro‚ÄëF1: 0.5909\n",
      "Epoch 17 | Val macro‚ÄëF1: 0.5588\n",
      "Epoch 18 | Val macro‚ÄëF1: 0.5750\n",
      "Epoch 19 | Val macro‚ÄëF1: 0.5840\n",
      "Epoch 20 | Val macro‚ÄëF1: 0.5750\n"
     ]
    }
   ],
   "source": [
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # ÎàÑÎùΩÎêú ÎÇ†Ïßú skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])             # ÏõêÌïòÎäî metric Ïª¨Îüº ÏÇ¨Ïö©\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "'''\n",
    "# metricÎ≥ÑÎ°ú train/val tensor ÏÉùÏÑ± Î∞è shape Ï∂úÎ†•\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# ---------- 3. Train / Val Tensor Ï§ÄÎπÑ ----------\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # ÎàÑÎùΩÎêú ÎÇ†Ïßú skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q2, metric)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q2, metric)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "# ---------- 4. Feature Ï†ïÍ∑úÌôî ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU Î™®Îç∏ ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. ÌïôÏäµ Î£®ÌîÑ ----------\n",
    "EPOCHS = 20\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                prob = model(xb).cpu().numpy()\n",
    "                preds.append(prob)\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.vstack(preds)\n",
    "        trues = np.vstack(trues)\n",
    "        y_hat = (preds >= 0.5).astype(int)\n",
    "        f1 = f1_score(trues, y_hat, average='macro')\n",
    "        print(f\"Epoch {epoch:2d} | Val macro‚ÄëF1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ac04e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Í∞ÄÏû• ÎÜíÏùÄ Í∞íÏùò Ïù∏Îç±Ïä§: 13\n",
      "Í∞ÄÏû• ÎÜíÏùÄ Í∞í: 0.5983666666666667\n",
      "[np.float64(0.5733333333333334), np.float64(0.5742), np.float64(0.5672166666666667), np.float64(0.5696166666666667), np.float64(0.5671333333333334), np.float64(0.56815), np.float64(0.5714333333333333), np.float64(0.5800333333333333), np.float64(0.5833666666666667), np.float64(0.5692666666666667), np.float64(0.5754), np.float64(0.5856500000000001), np.float64(0.5814833333333334), np.float64(0.5983666666666667), np.float64(0.5871833333333333), np.float64(0.5885333333333334), np.float64(0.5837), np.float64(0.5862166666666666), np.float64(0.57255), np.float64(0.5863833333333334)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q1_list = [0.5264, 0.5342, 0.5179, 0.5104, 0.5031, 0.5031, 0.5187, 0.5654, 0.5654, 0.5036, 0.5676, 0.5759, 0.5264, 0.5636, 0.5342, 0.5569, 0.5537, 0.5569, 0.5486, 0.5750]\n",
    "Q2_list = [0.5328, 0.5403, 0.5403, 0.5759, 0.5503, 0.5588, 0.5593, 0.5767, 0.6063, 0.5537, 0.5909, 0.5673, 0.6112, 0.6021, 0.5759, 0.5817, 0.5944, 0.5930, 0.5507, 0.5593]\n",
    "Q3_list = [0.5878, 0.5680, 0.5750, 0.5588, 0.5588, 0.5750, 0.5853, 0.5661, 0.5786, 0.5750, 0.5503, 0.6121, 0.6000, 0.6277, 0.5971, 0.6156, 0.6021, 0.5680, 0.5840, 0.5840]\n",
    "S1_list = [0.5930, 0.6027, 0.6021, 0.5726, 0.5750, 0.5817, 0.5750, 0.5971, 0.5750, 0.6063, 0.5750, 0.6000, 0.6000, 0.6184, 0.5909, 0.6021, 0.6092, 0.6404, 0.5840, 0.6250]\n",
    "S2_list = [0.6000, 0.6000, 0.5840, 0.6000, 0.6000, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.6000, 0.5840, 0.5840, 0.5840, 0.5840, 0.6000]\n",
    "S3_list = [0.6000, 0.6000, 0.5840, 0.6000, 0.6156, 0.6063, 0.6063, 0.5909, 0.5909, 0.5930, 0.5846, 0.5746, 0.5673, 0.5944, 0.6250, 0.5909, 0.5588, 0.5750, 0.5840, 0.5750]\n",
    "# Í∞Å Ïù∏Îç±Ïä§Î≥ÑÎ°ú 6Í∞ú Î¶¨Ïä§Ìä∏Ïùò Í∞íÏùÑ ÌèâÍ∑†ÎÇ¥Ïñ¥ f1_listÏóê Ï†ÄÏû•\n",
    "f1_list = [np.mean([lst[i] for lst in all_lists]) for i in range(len(Q1_list))]\n",
    "all_lists = [Q1_list, Q2_list, Q3_list, S1_list, S2_list, S3_list]\n",
    "max_idx = np.argmax(f1_list)\n",
    "print(\"Í∞ÄÏû• ÎÜíÏùÄ Í∞íÏùò Ïù∏Îç±Ïä§:\", max_idx)\n",
    "print(\"Í∞ÄÏû• ÎÜíÏùÄ Í∞í:\", f1_list[max_idx])\n",
    "print(f1_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0744d8b",
   "metadata": {},
   "source": [
    "# Ïã§Ï†ú Ï†ÅÏö©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a66f1459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ df_zero_filled shape: (99190, 541)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "print(\"‚úÖ df_zero_filled shape:\", df_zero_filled.shape)\n",
    "#df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d3a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "\n",
    "testset = pd.read_csv('../data/ch2025_submission_sample.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea96bd7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>sleep_date</th>\n",
       "      <th>lifelog_date</th>\n",
       "      <th>Q1</th>\n",
       "      <th>Q2</th>\n",
       "      <th>Q3</th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>S3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>2024-07-30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>2024-08-01</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-03</td>\n",
       "      <td>2024-08-02</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-04</td>\n",
       "      <td>2024-08-03</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-06</td>\n",
       "      <td>2024-08-05</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-07</td>\n",
       "      <td>2024-08-06</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>2024-08-08</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-10</td>\n",
       "      <td>2024-08-09</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>id01</td>\n",
       "      <td>2024-08-12</td>\n",
       "      <td>2024-08-11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
       "0       id01  2024-07-31   2024-07-30   0   0   0   0   0   0\n",
       "1       id01  2024-08-01   2024-07-31   0   0   0   0   0   0\n",
       "2       id01  2024-08-02   2024-08-01   0   0   0   0   0   0\n",
       "3       id01  2024-08-03   2024-08-02   0   0   0   0   0   0\n",
       "4       id01  2024-08-04   2024-08-03   0   0   0   0   0   0\n",
       "5       id01  2024-08-06   2024-08-05   0   0   0   0   0   0\n",
       "6       id01  2024-08-07   2024-08-06   0   0   0   0   0   0\n",
       "7       id01  2024-08-09   2024-08-08   0   0   0   0   0   0\n",
       "8       id01  2024-08-10   2024-08-09   0   0   0   0   0   0\n",
       "9       id01  2024-08-12   2024-08-11   0   0   0   0   0   0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96831416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset_q1 head:\n",
      "  subject_id  sleep_date lifelog_date  Q1\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   1\n",
      "4       id01  2024-07-01   2024-06-30   0\n",
      "trainset_q2 head:\n",
      "  subject_id  sleep_date lifelog_date  Q2\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   0\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_q3 head:\n",
      "  subject_id  sleep_date lifelog_date  Q3\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   0\n",
      "3       id01  2024-06-30   2024-06-29   1\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s1 head:\n",
      "  subject_id  sleep_date lifelog_date  S1\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   2\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s2 head:\n",
      "  subject_id  sleep_date lifelog_date  S2\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   1\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s3 head:\n",
      "  subject_id  sleep_date lifelog_date  S3\n",
      "0       id01  2024-06-27   2024-06-26   1\n",
      "1       id01  2024-06-28   2024-06-27   1\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n"
     ]
    }
   ],
   "source": [
    "# Define metric columns\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Split df_train and df_val for each metric\n",
    "for metric in metrics:\n",
    "    globals()[f'trainset_{metric.lower()}'] = trainset[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "\n",
    "# Display head of all 12 dataframes\n",
    "for metric in metrics:\n",
    "    print(f\"trainset_{metric.lower()} head:\")\n",
    "    print(globals()[f'trainset_{metric.lower()}'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ea68fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 80\u001b[0m\n\u001b[1;32m     77\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(test_ds, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# 5) Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∞è ÌïôÏäµ (trainÎßå)\u001b[39;00m\n\u001b[0;32m---> 80\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mGRUModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msensor_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m criterion \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     82\u001b[0m optim \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:223\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 223\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "df_merged = df_zero_filled.copy()\n",
    "# timestamp ‚Üí datetime Î≥ÄÌôò\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "# lifelog_date = ÎÇ†Ïßú(Î¨∏ÏûêÏó¥) ; 2024-06-26 Í∞ôÏùÄ ÌòïÏãù\n",
    "df_merged[\"lifelog_date\"] = df_merged[\"timestamp\"].dt.date.astype(str)\n",
    "\n",
    "# -------- 2. feature / sensor column Ï†ïÏùò -----------\n",
    "drop_cols = [\"timestamp\",                # ÏãúÍ∞ÑÏùÄ ÏòàÏ∏°Ïóê Î∂àÌïÑÏöî\n",
    "             \"subject_id\",               # Îß§ÏπòÏö©\n",
    "             \"lifelog_date\"] + [c for c in df_merged.columns if c.startswith(\"id\")]  # one-hot id\n",
    "\n",
    "sensor_cols = [c for c in df_merged.columns if c not in drop_cols]\n",
    "# ÌïòÎ£® Îãπ ÏµúÎåÄ 144 ÌÉÄÏûÑÏä§ÌÖùÏúºÎ°ú Ìå®Îî©\n",
    "MAX_SEQ_LEN = 144\n",
    "\n",
    "# testset Î∂àÎü¨Ïò§Í∏∞\n",
    "testset = pd.read_csv(\"../data/ch2025_submission_sample.csv\")\n",
    "\n",
    "# ---- 1. testsetÏóê ÎåÄÌï¥ ÏãúÍ≥ÑÏó¥ ÏãúÌÄÄÏä§ Íµ¨ÏÑ± ----\n",
    "def build_test_sequences(testset, sequence_dict):\n",
    "    X_test = []\n",
    "    key_list = []\n",
    "    for idx, row in testset.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        key_list.append(key)\n",
    "        if key in sequence_dict:\n",
    "            X_test.append(sequence_dict[key])\n",
    "        else:\n",
    "            # Ìå®Îî© ÏãúÌÄÄÏä§ (Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÎäî Í≤ΩÏö∞)\n",
    "            x_pad = np.zeros((MAX_SEQ_LEN, len(sensor_cols)), dtype=np.float32)\n",
    "            X_test.append(x_pad)\n",
    "    return np.stack(X_test), key_list\n",
    "\n",
    "\n",
    "\n",
    "# ---- 2. metricÎ≥ÑÎ°ú ÌïôÏäµ/ÏòàÏ∏°/Ï±ÑÏö∞Í∏∞ ----\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "for metric in metrics:\n",
    "    # 1) train/val set ÎßåÎì§Í∏∞\n",
    "    train_df = trainset[['subject_id', 'lifelog_date', metric]].copy()\n",
    "    # (validation split ÌïÑÏöî ÏóÜÏúºÎ©¥ Ï†ÑÏ≤¥Î•º trainÏúºÎ°ú ÏÇ¨Ïö©)\n",
    "    X_train, y_train = [], []\n",
    "    for _, row in train_df.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:\n",
    "            continue\n",
    "        X_train.append(sequence_dict[key])\n",
    "        y_train.append([row[metric]])\n",
    "    X_train, y_train = np.stack(X_train), np.array(y_train, dtype=np.float32)\n",
    "\n",
    "    # 2) Ï†ïÍ∑úÌôî (train Í∏∞Ï§Ä)\n",
    "    scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "    X_train = scaler.transform(X_train.reshape(-1, len(sensor_cols))).reshape(X_train.shape)\n",
    "    \n",
    "    # 3) test ÏãúÌÄÄÏä§ ÏÉùÏÑ± & Ï†ïÍ∑úÌôî\n",
    "    X_test, key_list = build_test_sequences(testset, sequence_dict)\n",
    "    X_test = scaler.transform(X_test.reshape(-1, len(sensor_cols))).reshape(X_test.shape)\n",
    "\n",
    "    # 4) Dataset/dataloader ÏÉùÏÑ±\n",
    "    test_ds = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32))\n",
    "    test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "\n",
    "    # 5) Î™®Îç∏ Ï¥àÍ∏∞Ìôî Î∞è ÌïôÏäµ (trainÎßå)\n",
    "    model = GRUModel(len(sensor_cols)).to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    EPOCHS = 13  # (Îçî ÎäòÎ†§ÎèÑ Îê®)\n",
    "    train_tensor = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.float32))\n",
    "    train_loader = torch.utils.data.DataLoader(train_tensor, batch_size=64, shuffle=True)\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "    # 6) testset ÏòàÏ∏°\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for (xb,) in test_loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = model(xb).cpu().numpy()\n",
    "            preds.append(prob)\n",
    "    preds = np.vstack(preds)\n",
    "    if metric == \"S1\":  # S1Ïù¥ 3-classÎùºÎ©¥ softmax/argmax Îì± Îî∞Î°ú Ï≤òÎ¶¨\n",
    "        testset[metric] = preds.argmax(axis=1)  # (ÌïÑÏöîÏãú ÏàòÏ†ï)\n",
    "    else:\n",
    "        testset[metric] = (preds >= 0.5).astype(int)\n",
    "\n",
    "# ---- 3. Í≤∞Í≥º Ï†ÄÏû• ----\n",
    "testset[['subject_id', 'sleep_date', 'lifelog_date'] + metrics].to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fa060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïã§Ï†ú submissionÏùÑ ÏúÑÌïú ÏµúÏ¢Ö ÏΩîÎìú\n",
    "\n",
    "df_merged = df_zero_filled.copy()\n",
    "# timestamp ‚Üí datetime Î≥ÄÌôò\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "# lifelog_date = ÎÇ†Ïßú(Î¨∏ÏûêÏó¥) ; 2024-06-26 Í∞ôÏùÄ ÌòïÏãù\n",
    "df_merged[\"lifelog_date\"] = df_merged[\"timestamp\"].dt.date.astype(str)\n",
    "\n",
    "# -------- 2. feature / sensor column Ï†ïÏùò -----------\n",
    "drop_cols = [\"timestamp\",                # ÏãúÍ∞ÑÏùÄ ÏòàÏ∏°Ïóê Î∂àÌïÑÏöî\n",
    "             \"subject_id\",               # Îß§ÏπòÏö©\n",
    "             \"lifelog_date\"] + [c for c in df_merged.columns if c.startswith(\"id\")]  # one-hot id\n",
    "\n",
    "sensor_cols = [c for c in df_merged.columns if c not in drop_cols]\n",
    "# ÌïòÎ£® Îãπ ÏµúÎåÄ 144 ÌÉÄÏûÑÏä§ÌÖùÏúºÎ°ú Ìå®Îî©\n",
    "MAX_SEQ_LEN = 144\n",
    "\n",
    "\n",
    "# -------- 3. ÏãúÌÄÄÏä§ Î¨∂Îäî Ìï®Ïàò -----------\n",
    "def build_sequences(df):\n",
    "    \"\"\"(subject_id, lifelog_date) ‚Üí ndarray(seq_len, n_feat)\"\"\"\n",
    "    seq_dict = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        # 10-Î∂Ñ Í∞ÑÍ≤© Î≥¥Ïû• ÏïàÎê† ÏàòÎèÑ ÏûàÏúºÎãà timestamp Í∏∞Ï§Ä Ï†ïÎ†¨\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[sensor_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # Í∏∏Ïù¥ Ï°∞Ï†ï\n",
    "        if len(x) > MAX_SEQ_LEN:          # ÏûòÎùºÎÇ¥Í∏∞\n",
    "            x = x[:MAX_SEQ_LEN]\n",
    "        if len(x) < MAX_SEQ_LEN:          # 0-Ìå®Îî©\n",
    "            pad = np.zeros((MAX_SEQ_LEN - len(x), x.shape[1]), np.float32)\n",
    "            x = np.vstack([x, pad])\n",
    "\n",
    "        seq_dict[(sid, day)] = x          # shape = (144, n_feat)\n",
    "    return seq_dict\n",
    "\n",
    "sequence_dict = build_sequences(df_merged)\n",
    "print(\"# total sequences built :\", len(sequence_dict))\n",
    "\n",
    "# ---------- 3. Train / Val Tensor Ï§ÄÎπÑ ----------\n",
    "def rows_to_tensors(df_label):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # ÎàÑÎùΩÎêú ÎÇ†Ïßú skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row['Q1']])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q1)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q1)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "\n",
    "# ---------- 4. Feature Ï†ïÍ∑úÌôî ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU Î™®Îç∏ ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. ÌïôÏäµ Î£®ÌîÑ ----------\n",
    "EPOCHS = 13\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # --- val ---\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = model(xb).cpu().numpy()\n",
    "            preds.append(prob)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    y_hat = (preds >= 0.5).astype(int)\n",
    "    f1 = f1_score(trues, y_hat, average='macro')\n",
    "    print(f\"Epoch {epoch:2d} | Val macro‚ÄëF1: {f1:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wonjun_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
