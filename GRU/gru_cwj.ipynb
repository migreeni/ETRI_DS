{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d155b048",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44915715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged_df_cwj.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('../data/merged_df_original.csv')\n",
    "\n",
    "# '캐시워크'부터 '우체국보험'까지의 열 이름 목록 추출\n",
    "start_col = '캐시워크'\n",
    "end_col = '우체국보험'\n",
    "cols_to_sum = df.loc[:, start_col:end_col].columns\n",
    "\n",
    "df['m_usagestats'] = df[cols_to_sum].sum(axis=1)\n",
    "\n",
    "df.drop(columns=cols_to_sum, inplace=True)\n",
    "\n",
    "merged_df_cwj = df.copy()\n",
    "\n",
    "merged_df_cwj.rename(columns={'m_wtb_rssi_x': 'm_wtb_rssi', 'm_wtb_rssi_y': 'm_wtw_rssi'}, inplace=True)\n",
    "\n",
    "# CSV로 내보내기\n",
    "merged_df_cwj.to_csv('../data/merged_df_cwj.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"✅ merged_df_cwj.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad28eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 결측치 개수:\n",
      "met_activity     2690\n",
      "m_wtb_rssi      77337\n",
      "m_wtw_rssi      22842\n",
      "heart_rate      55846\n",
      "distance        22852\n",
      "latitude        16044\n",
      "longitude       16044\n",
      "altitude        16044\n",
      "speed           16044\n",
      "m_usagestats        0\n",
      "w_light         19040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/merged_df_cwj.csv')\n",
    "\n",
    "# 검사할 열 목록\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# 각 열의 결측치 개수 출력\n",
    "missing_counts = df[cols_to_check].isnull().sum()\n",
    "print(\"📊 결측치 개수:\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157bbe6",
   "metadata": {},
   "source": [
    "### 결측치를 가장 가까운 이웃의 값(앞뒤 기준)으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce709a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 결측치 보간 완료. 남은 결측치 수:\n",
      "met_activity    0\n",
      "m_wtb_rssi      0\n",
      "m_wtw_rssi      0\n",
      "heart_rate      0\n",
      "distance        0\n",
      "latitude        0\n",
      "longitude       0\n",
      "altitude        0\n",
      "speed           0\n",
      "m_usagestats    0\n",
      "w_light         0\n",
      "dtype: int64\n",
      "📁 '../data/merged_df_cwj_filled.csv' 저장 완료\n"
     ]
    }
   ],
   "source": [
    "#대상 열 목록\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# 결측치 보간 함수 (앞뒤 평균)\n",
    "def fill_nearest_avg(series):\n",
    "    forward = series.ffill()  # 앞쪽 값으로 채우기\n",
    "    backward = series.bfill()  # 뒤쪽 값으로 채우기\n",
    "    filled = series.copy()\n",
    "    \n",
    "    # 앞뒤 값이 모두 있는 경우 평균으로\n",
    "    for i in series[series.isnull()].index:\n",
    "        f, b = forward[i], backward[i]\n",
    "        if pd.notnull(f) and pd.notnull(b):\n",
    "            filled[i] = (f + b) / 2\n",
    "        elif pd.notnull(f):\n",
    "            filled[i] = f\n",
    "        elif pd.notnull(b):\n",
    "            filled[i] = b\n",
    "    return filled\n",
    "\n",
    "# 'burned_calories' 열의 결측치를 0으로 채우기\n",
    "df['burned_calories'] = df['burned_calories'].fillna(0)\n",
    "\n",
    "# 각 열에 대해 결측치 처리\n",
    "for col in cols_to_check:\n",
    "    df[col] = fill_nearest_avg(df[col])\n",
    "\n",
    "# 확인용 출력 (선택사항)\n",
    "print(\"✅ 결측치 보간 완료. 남은 결측치 수:\")\n",
    "print(df[cols_to_check].isnull().sum())\n",
    "\n",
    "# 저장\n",
    "df.to_csv('../data/merged_df_cwj_filled.csv', index=False)\n",
    "print(\"📁 '../data/merged_df_cwj_filled.csv' 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda8fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 [cols_to_check 외] 결측치가 있는 열들:\n",
      "Music                        3717\n",
      "Vehicle                      3717\n",
      "Motor vehicle (road)         3717\n",
      "Outside, urban or manmade    3717\n",
      "Outside, rural or natural    3717\n",
      "                             ... \n",
      "Heavy metal                  3717\n",
      "Double bass                  3717\n",
      "Drum and bass                3717\n",
      "String section               3717\n",
      "Punk rock                    3717\n",
      "Length: 517, dtype: int64\n",
      "✅ 결측치를 0으로 채운 파일 저장 완료: merged_df_cwj_tozero.csv\n",
      "✅ 결측치 행 삭제 파일 저장 완료: merged_df_cwj_delete.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 나머지 열들 선택\n",
    "remaining_cols = [col for col in df.columns if col not in cols_to_check]\n",
    "\n",
    "# 결측치 개수 확인\n",
    "missing_counts = df[remaining_cols].isnull().sum()\n",
    "\n",
    "# 결측치가 존재하는 열만 출력\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(\"📊 [cols_to_check 외] 결측치가 있는 열들:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# 결측치를 0으로 채워 저장\n",
    "df_zero_filled = df.copy()\n",
    "df_zero_filled[remaining_cols] = df_zero_filled[remaining_cols].fillna(0)\n",
    "df_zero_filled.to_csv('../data/merged_df_cwj_tozero.csv', index=False)\n",
    "print(\"✅ 결측치를 0으로 채운 파일 저장 완료: merged_df_cwj_tozero.csv\")\n",
    "\n",
    "# 결측치가 있는 행을 삭제하여 저장\n",
    "df_dropna = df.copy()\n",
    "df_dropna = df_dropna.dropna(subset=remaining_cols)\n",
    "df_dropna.to_csv('../data/merged_df_cwj_delete.csv', index=False)\n",
    "print(\"✅ 결측치 행 삭제 파일 저장 완료: merged_df_cwj_delete.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ea2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_zero_filled 결측치 총 개수: 0\n",
      "df_dropna 결측치 총 개수: 0\n",
      "(99190, 541) (95473, 541)\n"
     ]
    }
   ],
   "source": [
    "# 결측치 개수 세기\n",
    "missing_zero_filled = df_zero_filled.isnull().sum().sum()\n",
    "missing_dropna = df_dropna.isnull().sum().sum()\n",
    "\n",
    "print(f\"df_zero_filled 결측치 총 개수: {missing_zero_filled}\")\n",
    "print(f\"df_dropna 결측치 총 개수: {missing_dropna}\")\n",
    "\n",
    "print(df_zero_filled.shape, df_dropna.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfc549",
   "metadata": {},
   "source": [
    "# train, test 셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14215e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_zero_filled shape: (99190, 541)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "print(\"✅ df_zero_filled shape:\", df_zero_filled.shape)\n",
    "#df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "785bf30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "24        id01  2024-07-24   2024-07-23   0   1   1   0   0   1\n",
      "17        id01  2024-07-15   2024-07-14   0   1   0   0   1   1\n",
      "66        id02  2024-08-19   2024-08-18   0   1   1   1   0   1\n",
      "148       id04  2024-09-02   2024-09-01   0   0   1   1   1   0\n",
      "249       id06  2024-07-03   2024-07-02   0   1   1   1   1   1\n",
      "31        id01  2024-08-21   2024-08-20   0   0   1   0   0   1\n",
      "84        id02  2024-09-24   2024-09-23   0   1   1   1   1   1\n",
      "307       id07  2024-08-02   2024-08-01   0   0   0   0   0   0\n",
      "406       id09  2024-08-24   2024-08-23   0   0   0   0   1   0\n",
      "389       id09  2024-07-18   2024-07-17   1   0   0   1   1   1\n",
      "\n",
      "df_val.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "407       id09  2024-08-25   2024-08-24   1   0   0   1   1   1\n",
      "444       id10  2024-09-03   2024-09-02   0   0   0   0   0   0\n",
      "117       id03  2024-09-07   2024-09-06   1   1   0   1   0   0\n",
      "30        id01  2024-08-20   2024-08-19   0   1   1   0   1   1\n",
      "415       id09  2024-09-03   2024-09-02   1   1   1   1   0   0\n",
      "157       id04  2024-09-17   2024-09-16   1   0   0   1   1   1\n",
      "325       id08  2024-07-01   2024-06-30   0   1   0   0   1   1\n",
      "447       id10  2024-09-09   2024-09-08   1   1   1   0   1   1\n",
      "268       id06  2024-08-11   2024-08-10   0   1   1   1   1   1\n",
      "297       id07  2024-07-07   2024-07-06   1   0   1   0   0   0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 9), (360, 9))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "\n",
    "df_train, df_val = train_test_split(train_val_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"df_train.head():\")\n",
    "print(df_train.head(10))\n",
    "print(\"\\ndf_val.head():\")\n",
    "print(df_val.head(10))\n",
    "\n",
    "\n",
    "df_val.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e461298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   0\n",
      "\n",
      "df_val_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define metric columns\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Split df_train and df_val for each metric\n",
    "for metric in metrics:\n",
    "    globals()[f'df_train_{metric.lower()}'] = df_train[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "    globals()[f'df_val_{metric.lower()}'] = df_val[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "\n",
    "# Display head of all 12 dataframes\n",
    "for metric in metrics:\n",
    "    print(f\"df_train_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_train_{metric.lower()}'].head())\n",
    "    print(f\"\\ndf_val_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_val_{metric.lower()}'].head())\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526449",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b98986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total sequences built : 700\n",
      "Train tensor shape : (360, 144, 529) (360, 1)\n",
      "Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.5553\n",
      "Epoch  2 | Val macro‑F1: 0.4623\n",
      "Epoch  3 | Val macro‑F1: 0.5950\n",
      "Epoch  4 | Val macro‑F1: 0.5801\n",
      "Epoch  5 | Val macro‑F1: 0.6192\n",
      "Epoch  6 | Val macro‑F1: 0.6111\n",
      "Epoch  7 | Val macro‑F1: 0.5218\n",
      "Epoch  8 | Val macro‑F1: 0.5744\n",
      "Epoch  9 | Val macro‑F1: 0.5982\n",
      "Epoch 10 | Val macro‑F1: 0.6099\n",
      "Epoch 11 | Val macro‑F1: 0.5347\n",
      "Epoch 12 | Val macro‑F1: 0.5312\n",
      "Epoch 13 | Val macro‑F1: 0.4985\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df_merged = df_zero_filled.copy()\n",
    "# timestamp → datetime 변환\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "# lifelog_date = 날짜(문자열) ; 2024-06-26 같은 형식\n",
    "df_merged[\"lifelog_date\"] = df_merged[\"timestamp\"].dt.date.astype(str)\n",
    "\n",
    "# -------- 2. feature / sensor column 정의 -----------\n",
    "drop_cols = [\"timestamp\",                # 시간은 예측에 불필요\n",
    "             \"subject_id\",               # 매치용\n",
    "             \"lifelog_date\"] + [c for c in df_merged.columns if c.startswith(\"id\")]  # one-hot id\n",
    "\n",
    "sensor_cols = [c for c in df_merged.columns if c not in drop_cols]\n",
    "# 하루 당 최대 144 타임스텝으로 패딩\n",
    "MAX_SEQ_LEN = 144\n",
    "\n",
    "\n",
    "# -------- 3. 시퀀스 묶는 함수 -----------\n",
    "def build_sequences(df):\n",
    "    \"\"\"(subject_id, lifelog_date) → ndarray(seq_len, n_feat)\"\"\"\n",
    "    seq_dict = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        # 10-분 간격 보장 안될 수도 있으니 timestamp 기준 정렬\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[sensor_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # 길이 조정\n",
    "        if len(x) > MAX_SEQ_LEN:          # 잘라내기\n",
    "            x = x[:MAX_SEQ_LEN]\n",
    "        if len(x) < MAX_SEQ_LEN:          # 0-패딩\n",
    "            pad = np.zeros((MAX_SEQ_LEN - len(x), x.shape[1]), np.float32)\n",
    "            x = np.vstack([x, pad])\n",
    "\n",
    "        seq_dict[(sid, day)] = x          # shape = (144, n_feat)\n",
    "    return seq_dict\n",
    "\n",
    "sequence_dict = build_sequences(df_merged)\n",
    "print(\"# total sequences built :\", len(sequence_dict))\n",
    "\n",
    "# ---------- 3. Train / Val Tensor 준비 ----------\n",
    "def rows_to_tensors(df_label):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row['Q1']])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q1)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q1)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "\n",
    "# ---------- 4. Feature 정규화 ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU 모델 ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. 학습 루프 ----------\n",
    "EPOCHS = 13\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # --- val ---\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = model(xb).cpu().numpy()\n",
    "            preds.append(prob)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    y_hat = (preds >= 0.5).astype(int)\n",
    "    f1 = f1_score(trues, y_hat, average='macro')\n",
    "    print(f\"Epoch {epoch:2d} | Val macro‑F1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4b6dc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q1] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.5264\n",
      "Epoch  2 | Val macro‑F1: 0.5342\n",
      "Epoch  3 | Val macro‑F1: 0.5179\n",
      "Epoch  4 | Val macro‑F1: 0.5104\n",
      "Epoch  5 | Val macro‑F1: 0.5031\n",
      "Epoch  6 | Val macro‑F1: 0.5031\n",
      "Epoch  7 | Val macro‑F1: 0.5187\n",
      "Epoch  8 | Val macro‑F1: 0.5654\n",
      "Epoch  9 | Val macro‑F1: 0.5654\n",
      "Epoch 10 | Val macro‑F1: 0.5036\n",
      "Epoch 11 | Val macro‑F1: 0.5676\n",
      "Epoch 12 | Val macro‑F1: 0.5759\n",
      "Epoch 13 | Val macro‑F1: 0.5264\n",
      "Epoch 14 | Val macro‑F1: 0.5636\n",
      "Epoch 15 | Val macro‑F1: 0.5342\n",
      "Epoch 16 | Val macro‑F1: 0.5569\n",
      "Epoch 17 | Val macro‑F1: 0.5537\n",
      "Epoch 18 | Val macro‑F1: 0.5569\n",
      "Epoch 19 | Val macro‑F1: 0.5486\n",
      "Epoch 20 | Val macro‑F1: 0.5750\n",
      "[Q2] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q2] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.5328\n",
      "Epoch  2 | Val macro‑F1: 0.5403\n",
      "Epoch  3 | Val macro‑F1: 0.5403\n",
      "Epoch  4 | Val macro‑F1: 0.5759\n",
      "Epoch  5 | Val macro‑F1: 0.5503\n",
      "Epoch  6 | Val macro‑F1: 0.5588\n",
      "Epoch  7 | Val macro‑F1: 0.5593\n",
      "Epoch  8 | Val macro‑F1: 0.5767\n",
      "Epoch  9 | Val macro‑F1: 0.6063\n",
      "Epoch 10 | Val macro‑F1: 0.5537\n",
      "Epoch 11 | Val macro‑F1: 0.5909\n",
      "Epoch 12 | Val macro‑F1: 0.5673\n",
      "Epoch 13 | Val macro‑F1: 0.6112\n",
      "Epoch 14 | Val macro‑F1: 0.6021\n",
      "Epoch 15 | Val macro‑F1: 0.5759\n",
      "Epoch 16 | Val macro‑F1: 0.5817\n",
      "Epoch 17 | Val macro‑F1: 0.5944\n",
      "Epoch 18 | Val macro‑F1: 0.5930\n",
      "Epoch 19 | Val macro‑F1: 0.5507\n",
      "Epoch 20 | Val macro‑F1: 0.5593\n",
      "[Q3] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[Q3] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.5878\n",
      "Epoch  2 | Val macro‑F1: 0.5680\n",
      "Epoch  3 | Val macro‑F1: 0.5750\n",
      "Epoch  4 | Val macro‑F1: 0.5588\n",
      "Epoch  5 | Val macro‑F1: 0.5588\n",
      "Epoch  6 | Val macro‑F1: 0.5750\n",
      "Epoch  7 | Val macro‑F1: 0.5853\n",
      "Epoch  8 | Val macro‑F1: 0.5661\n",
      "Epoch  9 | Val macro‑F1: 0.5786\n",
      "Epoch 10 | Val macro‑F1: 0.5750\n",
      "Epoch 11 | Val macro‑F1: 0.5503\n",
      "Epoch 12 | Val macro‑F1: 0.6121\n",
      "Epoch 13 | Val macro‑F1: 0.6000\n",
      "Epoch 14 | Val macro‑F1: 0.6277\n",
      "Epoch 15 | Val macro‑F1: 0.5971\n",
      "Epoch 16 | Val macro‑F1: 0.6156\n",
      "Epoch 17 | Val macro‑F1: 0.6021\n",
      "Epoch 18 | Val macro‑F1: 0.5680\n",
      "Epoch 19 | Val macro‑F1: 0.5840\n",
      "Epoch 20 | Val macro‑F1: 0.5840\n",
      "[S1] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S1] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.5930\n",
      "Epoch  2 | Val macro‑F1: 0.6027\n",
      "Epoch  3 | Val macro‑F1: 0.6021\n",
      "Epoch  4 | Val macro‑F1: 0.5726\n",
      "Epoch  5 | Val macro‑F1: 0.5750\n",
      "Epoch  6 | Val macro‑F1: 0.5817\n",
      "Epoch  7 | Val macro‑F1: 0.5750\n",
      "Epoch  8 | Val macro‑F1: 0.5971\n",
      "Epoch  9 | Val macro‑F1: 0.5750\n",
      "Epoch 10 | Val macro‑F1: 0.6063\n",
      "Epoch 11 | Val macro‑F1: 0.5750\n",
      "Epoch 12 | Val macro‑F1: 0.6000\n",
      "Epoch 13 | Val macro‑F1: 0.6000\n",
      "Epoch 14 | Val macro‑F1: 0.6184\n",
      "Epoch 15 | Val macro‑F1: 0.5909\n",
      "Epoch 16 | Val macro‑F1: 0.6021\n",
      "Epoch 17 | Val macro‑F1: 0.6092\n",
      "Epoch 18 | Val macro‑F1: 0.6404\n",
      "Epoch 19 | Val macro‑F1: 0.5840\n",
      "Epoch 20 | Val macro‑F1: 0.6250\n",
      "[S2] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S2] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.6000\n",
      "Epoch  2 | Val macro‑F1: 0.6000\n",
      "Epoch  3 | Val macro‑F1: 0.5840\n",
      "Epoch  4 | Val macro‑F1: 0.6000\n",
      "Epoch  5 | Val macro‑F1: 0.6000\n",
      "Epoch  6 | Val macro‑F1: 0.5840\n",
      "Epoch  7 | Val macro‑F1: 0.5840\n",
      "Epoch  8 | Val macro‑F1: 0.5840\n",
      "Epoch  9 | Val macro‑F1: 0.5840\n",
      "Epoch 10 | Val macro‑F1: 0.5840\n",
      "Epoch 11 | Val macro‑F1: 0.5840\n",
      "Epoch 12 | Val macro‑F1: 0.5840\n",
      "Epoch 13 | Val macro‑F1: 0.5840\n",
      "Epoch 14 | Val macro‑F1: 0.5840\n",
      "Epoch 15 | Val macro‑F1: 0.6000\n",
      "Epoch 16 | Val macro‑F1: 0.5840\n",
      "Epoch 17 | Val macro‑F1: 0.5840\n",
      "Epoch 18 | Val macro‑F1: 0.5840\n",
      "Epoch 19 | Val macro‑F1: 0.5840\n",
      "Epoch 20 | Val macro‑F1: 0.6000\n",
      "[S3] Train tensor shape : (360, 144, 529) (360, 1)\n",
      "[S3] Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.6000\n",
      "Epoch  2 | Val macro‑F1: 0.6000\n",
      "Epoch  3 | Val macro‑F1: 0.5840\n",
      "Epoch  4 | Val macro‑F1: 0.6000\n",
      "Epoch  5 | Val macro‑F1: 0.6156\n",
      "Epoch  6 | Val macro‑F1: 0.6063\n",
      "Epoch  7 | Val macro‑F1: 0.6063\n",
      "Epoch  8 | Val macro‑F1: 0.5909\n",
      "Epoch  9 | Val macro‑F1: 0.5909\n",
      "Epoch 10 | Val macro‑F1: 0.5930\n",
      "Epoch 11 | Val macro‑F1: 0.5846\n",
      "Epoch 12 | Val macro‑F1: 0.5746\n",
      "Epoch 13 | Val macro‑F1: 0.5673\n",
      "Epoch 14 | Val macro‑F1: 0.5944\n",
      "Epoch 15 | Val macro‑F1: 0.6250\n",
      "Epoch 16 | Val macro‑F1: 0.5909\n",
      "Epoch 17 | Val macro‑F1: 0.5588\n",
      "Epoch 18 | Val macro‑F1: 0.5750\n",
      "Epoch 19 | Val macro‑F1: 0.5840\n",
      "Epoch 20 | Val macro‑F1: 0.5750\n"
     ]
    }
   ],
   "source": [
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])             # 원하는 metric 컬럼 사용\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "'''\n",
    "# metric별로 train/val tensor 생성 및 shape 출력\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# ---------- 3. Train / Val Tensor 준비 ----------\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q2, metric)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q2, metric)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "# ---------- 4. Feature 정규화 ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU 모델 ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. 학습 루프 ----------\n",
    "EPOCHS = 20\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                prob = model(xb).cpu().numpy()\n",
    "                preds.append(prob)\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.vstack(preds)\n",
    "        trues = np.vstack(trues)\n",
    "        y_hat = (preds >= 0.5).astype(int)\n",
    "        f1 = f1_score(trues, y_hat, average='macro')\n",
    "        print(f\"Epoch {epoch:2d} | Val macro‑F1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac04e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[np.float64(0.5733333333333334), np.float64(0.5742), np.float64(0.5672166666666667), np.float64(0.5696166666666667), np.float64(0.5671333333333334), np.float64(0.56815), np.float64(0.5714333333333333), np.float64(0.5800333333333333), np.float64(0.5833666666666667), np.float64(0.5692666666666667), np.float64(0.5754), np.float64(0.5856500000000001), np.float64(0.5814833333333334), np.float64(0.5983666666666667), np.float64(0.5871833333333333), np.float64(0.5885333333333334), np.float64(0.5837), np.float64(0.5862166666666666), np.float64(0.57255), np.float64(0.5863833333333334)]\n",
      "Max value: 0.5983666666666667\n",
      "Index: 13\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Q1_list = [0.5264, 0.5342, 0.5179, 0.5104, 0.5031, 0.5031, 0.5187, 0.5654, 0.5654, 0.5036, 0.5676, 0.5759, 0.5264, 0.5636, 0.5342, 0.5569, 0.5537, 0.5569, 0.5486, 0.5750]\n",
    "Q2_list = [0.5328, 0.5403, 0.5403, 0.5759, 0.5503, 0.5588, 0.5593, 0.5767, 0.6063, 0.5537, 0.5909, 0.5673, 0.6112, 0.6021, 0.5759, 0.5817, 0.5944, 0.5930, 0.5507, 0.5593]\n",
    "Q3_list = [0.5878, 0.5680, 0.5750, 0.5588, 0.5588, 0.5750, 0.5853, 0.5661, 0.5786, 0.5750, 0.5503, 0.6121, 0.6000, 0.6277, 0.5971, 0.6156, 0.6021, 0.5680, 0.5840, 0.5840]\n",
    "S1_list = [0.5930, 0.6027, 0.6021, 0.5726, 0.5750, 0.5817, 0.5750, 0.5971, 0.5750, 0.6063, 0.5750, 0.6000, 0.6000, 0.6184, 0.5909, 0.6021, 0.6092, 0.6404, 0.5840, 0.6250]\n",
    "S2_list = [0.6000, 0.6000, 0.5840, 0.6000, 0.6000, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.5840, 0.6000, 0.5840, 0.5840, 0.5840, 0.5840, 0.6000]\n",
    "S3_list = [0.6000, 0.6000, 0.5840, 0.6000, 0.6156, 0.6063, 0.6063, 0.5909, 0.5909, 0.5930, 0.5846, 0.5746, 0.5673, 0.5944, 0.6250, 0.5909, 0.5588, 0.5750, 0.5840, 0.5750]\n",
    "# 각 인덱스별로 6개 리스트의 값을 평균내어 f1_list에 저장\n",
    "f1_list = []\n",
    "all_lists = [Q1_list, Q2_list, Q3_list, S1_list, S2_list, S3_list]\n",
    "\n",
    "for i in range(len(Q1_list)):\n",
    "    avg_value = np.mean([lst[i] for lst in all_lists])\n",
    "    f1_list.append(avg_value)\n",
    "\n",
    "print(f1_list)\n",
    "max_value = max(f1_list)\n",
    "max_index = f1_list.index(max_value)\n",
    "print(\"Max value:\", max_value)\n",
    "print(\"Index:\", max_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0744d8b",
   "metadata": {},
   "source": [
    "# 실제 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a66f1459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_zero_filled shape: (99190, 541)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "print(\"✅ df_zero_filled shape:\", df_zero_filled.shape)\n",
    "#df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d3a1d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainset = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "\n",
    "testset = pd.read_csv('../data/ch2025_submission_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea96bd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "0       id01  2024-07-31   2024-07-30   0   0   0   0   0   0\n",
      "1       id01  2024-08-01   2024-07-31   0   0   0   0   0   0\n",
      "2       id01  2024-08-02   2024-08-01   0   0   0   0   0   0\n",
      "3       id01  2024-08-03   2024-08-02   0   0   0   0   0   0\n",
      "4       id01  2024-08-04   2024-08-03   0   0   0   0   0   0\n",
      "5       id01  2024-08-06   2024-08-05   0   0   0   0   0   0\n",
      "6       id01  2024-08-07   2024-08-06   0   0   0   0   0   0\n",
      "7       id01  2024-08-09   2024-08-08   0   0   0   0   0   0\n",
      "8       id01  2024-08-10   2024-08-09   0   0   0   0   0   0\n",
      "9       id01  2024-08-12   2024-08-11   0   0   0   0   0   0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(250, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(testset.head(10))\n",
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58cf2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "0       id01  2024-06-27   2024-06-26   0   0   0   0   0   1\n",
      "1       id01  2024-06-28   2024-06-27   0   0   0   0   1   1\n",
      "2       id01  2024-06-29   2024-06-28   1   0   0   1   1   1\n",
      "3       id01  2024-06-30   2024-06-29   1   0   1   2   0   0\n",
      "4       id01  2024-07-01   2024-06-30   0   1   1   1   1   1\n",
      "5       id01  2024-07-02   2024-07-01   0   1   1   0   1   1\n",
      "6       id01  2024-07-03   2024-07-02   0   0   1   1   0   1\n",
      "7       id01  2024-07-04   2024-07-03   0   0   0   1   0   1\n",
      "8       id01  2024-07-05   2024-07-04   0   0   1   0   0   1\n",
      "9       id01  2024-07-06   2024-07-05   1   1   1   2   1   1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(450, 9)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(trainset.head(10))\n",
    "trainset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96831416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainset_q1 head:\n",
      "  subject_id  sleep_date lifelog_date  Q1\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   1\n",
      "4       id01  2024-07-01   2024-06-30   0\n",
      "trainset_q2 head:\n",
      "  subject_id  sleep_date lifelog_date  Q2\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   0\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_q3 head:\n",
      "  subject_id  sleep_date lifelog_date  Q3\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   0\n",
      "3       id01  2024-06-30   2024-06-29   1\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s1 head:\n",
      "  subject_id  sleep_date lifelog_date  S1\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   0\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   2\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s2 head:\n",
      "  subject_id  sleep_date lifelog_date  S2\n",
      "0       id01  2024-06-27   2024-06-26   0\n",
      "1       id01  2024-06-28   2024-06-27   1\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n",
      "trainset_s3 head:\n",
      "  subject_id  sleep_date lifelog_date  S3\n",
      "0       id01  2024-06-27   2024-06-26   1\n",
      "1       id01  2024-06-28   2024-06-27   1\n",
      "2       id01  2024-06-29   2024-06-28   1\n",
      "3       id01  2024-06-30   2024-06-29   0\n",
      "4       id01  2024-07-01   2024-06-30   1\n"
     ]
    }
   ],
   "source": [
    "# Define metric columns\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Split df_train and df_val for each metric\n",
    "for metric in metrics:\n",
    "    globals()[f'trainset_{metric.lower()}'] = trainset[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "\n",
    "# Display head of all 12 dataframes\n",
    "for metric in metrics:\n",
    "    print(f\"trainset_{metric.lower()} head:\")\n",
    "    print(globals()[f'trainset_{metric.lower()}'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9856391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      " -> best F1=0.4911 @ epoch 42\n",
      "\n",
      "=== Fold 2/5 ===\n",
      " -> best F1=0.5031 @ epoch 49\n",
      "\n",
      "=== Fold 3/5 ===\n",
      " -> best F1=0.4711 @ epoch 3\n",
      "\n",
      "=== Fold 4/5 ===\n",
      " -> best F1=0.4079 @ epoch 49\n",
      "\n",
      "=== Fold 5/5 ===\n",
      " -> best F1=0.4749 @ epoch 46\n",
      "\n",
      "Selected BEST_EPOCH = 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final-train: 100%|██████████| 38/38 [01:09<00:00,  1.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ submission saved -> ../data/submission.csv\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 0. 라이브러리\n",
    "# =========================================================\n",
    "import os, copy, json, random, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import trange\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# =========================================================\n",
    "# 1. 데이터 로드\n",
    "# =========================================================\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "trainset = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "testset  = pd.read_csv('../data/ch2025_submission_sample.csv')   # id/date 컬럼만 사용\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1-1. 시계열 전처리\n",
    "# ---------------------------------------------------------\n",
    "df_zero_filled['timestamp']   = pd.to_datetime(df_zero_filled['timestamp'])\n",
    "df_zero_filled['lifelog_date'] = df_zero_filled['timestamp'].dt.date.astype(str)\n",
    "\n",
    "DROP_COLS = ['timestamp', 'subject_id', 'lifelog_date']\n",
    "SENSOR_COLS = [c for c in df_zero_filled.columns if c not in DROP_COLS]\n",
    "\n",
    "MAX_SEQ = 144    # 10-min × 24 h 기준\n",
    "\n",
    "def build_sequences(df):\n",
    "    \"\"\"Return dict key->np.ndarray(seq_len, n_feat)\"\"\"\n",
    "    seqs = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[SENSOR_COLS].to_numpy(np.float32)\n",
    "        if len(x) > MAX_SEQ: x = x[:MAX_SEQ]\n",
    "        if len(x) < MAX_SEQ:\n",
    "            x = np.concatenate([x, np.zeros((MAX_SEQ-len(x), x.shape[1]), np.float32)])\n",
    "        seqs[(sid, day)] = x\n",
    "    return seqs\n",
    "\n",
    "SEQ_DICT = build_sequences(df_zero_filled)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1-2. 학습/추론용 텐서 변환\n",
    "# ---------------------------------------------------------\n",
    "TARGETS = ['Q1','Q2','Q3','S1','S2','S3']   # S1 = 3-class, others binary\n",
    "\n",
    "def rows_to_xy(df):\n",
    "    xs, ys, groups = [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        key = (r.subject_id, r.lifelog_date)\n",
    "        if key not in SEQ_DICT:        # missing sequence\n",
    "            continue\n",
    "        xs.append( SEQ_DICT[key] )\n",
    "        ys.append( r[TARGETS].to_list() )\n",
    "        groups.append( r.subject_id )  # fold split anchor\n",
    "    return np.stack(xs), np.array(ys, np.int64), np.array(groups)\n",
    "\n",
    "X_all, y_all, group_all = rows_to_xy(trainset)\n",
    "X_test, _, _ = rows_to_xy(testset)     # y_dummy ignored\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1-3. 스케일링 (training set 기준)\n",
    "# ---------------------------------------------------------\n",
    "scaler = StandardScaler().fit(X_all.reshape(-1, X_all.shape[-1]))\n",
    "def scale(x):\n",
    "    shp = x.shape\n",
    "    return scaler.transform(x.reshape(-1, shp[-1])).reshape(shp)\n",
    "\n",
    "X_all  = scale(X_all)\n",
    "X_test = scale(X_test)\n",
    "\n",
    "# =========================================================\n",
    "# 2. Dataset / DataLoader\n",
    "# =========================================================\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is None: return self.X[i]\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "# =========================================================\n",
    "# 3. GRU 모델 (멀티-헤드)\n",
    "# =========================================================\n",
    "class GRUNet(nn.Module):\n",
    "    def __init__(self, n_feat, hid=128, n_layers=2, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(n_feat, hid, n_layers, batch_first=True, dropout=drop)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Linear(hid, 2),   # Q1\n",
    "            nn.Linear(hid, 2),   # Q2\n",
    "            nn.Linear(hid, 2),   # Q3\n",
    "            nn.Linear(hid, 3),   # S1 (3-class)\n",
    "            nn.Linear(hid, 2),   # S2\n",
    "            nn.Linear(hid, 2)    # S3\n",
    "        ])\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        h = h[-1]                 # last layer output (B,H)\n",
    "        return [head(h) for head in self.heads]   # list of logits\n",
    "\n",
    "# loss per task\n",
    "LOSS_FNS = [\n",
    "    nn.CrossEntropyLoss(), nn.CrossEntropyLoss(), nn.CrossEntropyLoss(),\n",
    "    nn.CrossEntropyLoss(), nn.CrossEntropyLoss(), nn.CrossEntropyLoss()\n",
    "]\n",
    "\n",
    "# =========================================================\n",
    "# 4. k-fold validation (subject 그룹 유지)\n",
    "# =========================================================\n",
    "N_FOLD     = 5\n",
    "EPOCH_MAX  = 50\n",
    "BATCH_SIZE = 64\n",
    "DEVICE     = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "best_epochs = []          # fold별 best epoch 저장\n",
    "\n",
    "gkf = GroupKFold(n_splits=N_FOLD)\n",
    "for fold, (tr_idx, val_idx) in enumerate(gkf.split(X_all, y_all, group_all), 1):\n",
    "    print(f'\\n=== Fold {fold}/{N_FOLD} ===')\n",
    "    tr_loader = DataLoader(SleepDS(X_all[tr_idx], y_all[tr_idx]),\n",
    "                           batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(SleepDS(X_all[val_idx], y_all[val_idx]),\n",
    "                            batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = GRUNet(n_feat=X_all.shape[-1]).to(DEVICE)\n",
    "    opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "    best_f1, best_ep, best_state = -1, 0, None\n",
    "    for epoch in range(1, EPOCH_MAX+1):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        for xb, yb in tr_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = sum( LOSS_FNS[k](logits[k], yb[:,k]) for k in range(6) )\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "        # --- validation ---\n",
    "        model.eval()\n",
    "        y_true, y_pred = [[] for _ in range(6)], [[] for _ in range(6)]\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                logits = model(xb.to(DEVICE))\n",
    "                for k in range(6):\n",
    "                    preds = logits[k].argmax(1).cpu().numpy()\n",
    "                    y_pred[k].extend(preds)\n",
    "                    y_true[k].extend(yb[:,k].numpy())\n",
    "        f1s = [f1_score(y_true[k], y_pred[k], average='macro') for k in range(6)]\n",
    "        mac_f1 = np.mean(f1s)\n",
    "        print(f'E{epoch:02d}  F1={mac_f1:.4f}', end='\\r')\n",
    "\n",
    "        if mac_f1 > best_f1:\n",
    "            best_f1, best_ep = mac_f1, epoch\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "    print(f' -> best F1={best_f1:.4f} @ epoch {best_ep}')\n",
    "    best_epochs.append(best_ep)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4-1. 최적 epoch 결정 (평균 반올림)\n",
    "# ---------------------------------------------------------\n",
    "BEST_EPOCH = int(round(np.mean(best_epochs)))\n",
    "print('\\nSelected BEST_EPOCH =', BEST_EPOCH)\n",
    "\n",
    "# =========================================================\n",
    "# 5. 전체 trainset으로 재학습 (BEST_EPOCH), test 예측\n",
    "# =========================================================\n",
    "full_loader = DataLoader(SleepDS(X_all, y_all),\n",
    "                         batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(SleepDS(X_test),\n",
    "                         batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model_final = GRUNet(n_feat=X_all.shape[-1]).to(DEVICE)\n",
    "opt = torch.optim.AdamW(model_final.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "for epoch in trange(1, BEST_EPOCH+1, desc='Final-train'):\n",
    "    model_final.train()\n",
    "    for xb, yb in full_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        logits = model_final(xb)\n",
    "        loss = sum( LOSS_FNS[k](logits[k], yb[:,k]) for k in range(6) )\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "# --- inference ---\n",
    "model_final.eval()\n",
    "preds_all = [[] for _ in range(6)]\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        logits = model_final(xb.to(DEVICE))\n",
    "        for k in range(6):\n",
    "            preds_all[k].extend( logits[k].argmax(1).cpu().numpy() )\n",
    "\n",
    "# =========================================================\n",
    "# 6. submission 생성\n",
    "# =========================================================\n",
    "sub = testset[['subject_id','sleep_date','lifelog_date']].copy()\n",
    "for k,t in enumerate(TARGETS):\n",
    "    sub[t] = preds_all[k]\n",
    "\n",
    "SAVE_PATH = '../data/submission.csv'\n",
    "sub.to_csv(SAVE_PATH, index=False)\n",
    "print('✅ submission saved ->', SAVE_PATH)\n",
    "\n",
    "# 10m 45.7s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0373216d",
   "metadata": {},
   "source": [
    "# GRU for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eb30ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q1 training ===\n",
      "Fold 1 - Best F1: 0.5968 @ Epoch 40\n",
      "Fold 2 - Best F1: 0.5137 @ Epoch 15\n",
      "Fold 3 - Best F1: 0.6220 @ Epoch 49\n",
      "Fold 4 - Best F1: 0.5160 @ Epoch 48\n",
      "Fold 5 - Best F1: 0.6735 @ Epoch 38\n",
      "\n",
      "=== Q2 training ===\n",
      "Fold 1 - Best F1: 0.6627 @ Epoch 31\n",
      "Fold 2 - Best F1: 0.6104 @ Epoch 1\n",
      "Fold 3 - Best F1: 0.5930 @ Epoch 4\n",
      "Fold 4 - Best F1: 0.5135 @ Epoch 18\n",
      "Fold 5 - Best F1: 0.5704 @ Epoch 33\n",
      "\n",
      "=== Q3 training ===\n",
      "Fold 1 - Best F1: 0.5507 @ Epoch 1\n",
      "Fold 2 - Best F1: 0.4218 @ Epoch 44\n",
      "Fold 3 - Best F1: 0.5312 @ Epoch 1\n",
      "Fold 4 - Best F1: 0.5356 @ Epoch 50\n",
      "Fold 5 - Best F1: 0.5885 @ Epoch 40\n",
      "\n",
      "=== S1 training ===\n",
      "Fold 1 - Best F1: 0.3190 @ Epoch 22\n",
      "Fold 2 - Best F1: 0.3862 @ Epoch 50\n",
      "Fold 3 - Best F1: 0.3525 @ Epoch 29\n",
      "Fold 4 - Best F1: 0.4463 @ Epoch 25\n",
      "Fold 5 - Best F1: 0.3423 @ Epoch 31\n",
      "\n",
      "=== S2 training ===\n",
      "Fold 1 - Best F1: 0.6495 @ Epoch 3\n",
      "Fold 2 - Best F1: 0.5665 @ Epoch 24\n",
      "Fold 3 - Best F1: 0.5767 @ Epoch 28\n",
      "Fold 4 - Best F1: 0.5967 @ Epoch 28\n",
      "Fold 5 - Best F1: 0.5178 @ Epoch 31\n",
      "\n",
      "=== S3 training ===\n",
      "Fold 1 - Best F1: 0.5520 @ Epoch 20\n",
      "Fold 2 - Best F1: 0.5436 @ Epoch 9\n",
      "Fold 3 - Best F1: 0.5420 @ Epoch 4\n",
      "Fold 4 - Best F1: 0.5052 @ Epoch 34\n",
      "Fold 5 - Best F1: 0.4771 @ Epoch 2\n",
      "✅ submission saved -> ../data/submission2.csv\n"
     ]
    }
   ],
   "source": [
    "# 🔁 GRU 모델을 지표별로 따로 학습/추론하는 구조로 수정한 코드\n",
    "# 각 지표별로 best epoch 및 best model 추적 → 개별 모델로 testset 예측\n",
    "\n",
    "import os, copy, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "# ------------------------\n",
    "# Load Data\n",
    "# ------------------------\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "trainset = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "testset  = pd.read_csv('../data/ch2025_submission_sample.csv')\n",
    "\n",
    "# Preprocess\n",
    "TARGETS = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "df_zero_filled['timestamp'] = pd.to_datetime(df_zero_filled['timestamp'])\n",
    "df_zero_filled['lifelog_date'] = df_zero_filled['timestamp'].dt.date.astype(str)\n",
    "\n",
    "DROP_COLS = ['timestamp', 'subject_id', 'lifelog_date']\n",
    "SENSOR_COLS = [c for c in df_zero_filled.columns if c not in DROP_COLS]\n",
    "MAX_SEQ = 144\n",
    "\n",
    "def build_sequences(df):\n",
    "    seqs = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[SENSOR_COLS].to_numpy(np.float32)\n",
    "        if len(x) > MAX_SEQ: x = x[:MAX_SEQ]\n",
    "        if len(x) < MAX_SEQ:\n",
    "            x = np.concatenate([x, np.zeros((MAX_SEQ-len(x), x.shape[1]), np.float32)])\n",
    "        seqs[(sid, day)] = x\n",
    "    return seqs\n",
    "\n",
    "SEQ_DICT = build_sequences(df_zero_filled)\n",
    "\n",
    "def rows_to_xy(df):\n",
    "    xs, ys, groups = [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        key = (r.subject_id, r.lifelog_date)\n",
    "        if key not in SEQ_DICT:\n",
    "            continue\n",
    "        xs.append(SEQ_DICT[key])\n",
    "        ys.append(r[TARGETS].to_list())\n",
    "        groups.append(r.subject_id)\n",
    "    return np.stack(xs), np.array(ys, np.int64), np.array(groups)\n",
    "\n",
    "X_all, y_all, group_all = rows_to_xy(trainset)\n",
    "X_test, _, _ = rows_to_xy(testset)\n",
    "\n",
    "# Scale\n",
    "scaler = StandardScaler().fit(X_all.reshape(-1, X_all.shape[-1]))\n",
    "def scale(x):\n",
    "    shp = x.shape\n",
    "    return scaler.transform(x.reshape(-1, shp[-1])).reshape(shp)\n",
    "\n",
    "X_all = scale(X_all)\n",
    "X_test = scale(X_test)\n",
    "\n",
    "# ------------------------\n",
    "# Dataset / Model\n",
    "# ------------------------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i] if self.y is None else (self.X[i], self.y[i])\n",
    "\n",
    "class SingleHeadGRU(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim, hidden=128, layers=2, drop=0.3):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_dim, hidden, layers, batch_first=True, dropout=drop)\n",
    "        self.fc = nn.Linear(hidden, out_dim)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "# ------------------------\n",
    "# Train per target\n",
    "# ------------------------\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "N_FOLD = 5\n",
    "EPOCH_MAX = 50\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "preds_dict = {}\n",
    "\n",
    "for k, target in enumerate(TARGETS):\n",
    "    print(f\"\\n=== {target} training ===\")\n",
    "    y_target = y_all[:, k]\n",
    "    out_dim = 3 if target == 'S1' else 2\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_epochs = []\n",
    "\n",
    "    gkf = GroupKFold(n_splits=N_FOLD)\n",
    "    for fold, (tr_idx, val_idx) in enumerate(gkf.split(X_all, y_target, group_all)):\n",
    "        model = SingleHeadGRU(X_all.shape[-1], out_dim).to(DEVICE)\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "\n",
    "        best_f1, best_ep, best_state = -1, 0, None\n",
    "        for epoch in range(1, EPOCH_MAX+1):\n",
    "            model.train()\n",
    "            for xb, yb in DataLoader(SleepDS(X_all[tr_idx], y_target[tr_idx]), batch_size=BATCH_SIZE, shuffle=True):\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                pred = model(xb)\n",
    "                loss = criterion(pred, yb)\n",
    "                opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            # validation\n",
    "            model.eval()\n",
    "            all_pred, all_true = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in DataLoader(SleepDS(X_all[val_idx], y_target[val_idx]), batch_size=BATCH_SIZE):\n",
    "                    pred = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                    all_pred.extend(pred)\n",
    "                    all_true.extend(yb.numpy())\n",
    "            f1 = f1_score(all_true, all_pred, average='macro')\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_ep = epoch\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        best_epochs.append(best_ep)\n",
    "        print(f\"Fold {fold+1} - Best F1: {best_f1:.4f} @ Epoch {best_ep}\")\n",
    "\n",
    "    # Final train on all data\n",
    "    final_model = SingleHeadGRU(X_all.shape[-1], out_dim).to(DEVICE)\n",
    "    opt = torch.optim.AdamW(final_model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
    "    loader = DataLoader(SleepDS(X_all, y_target), batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for epoch in range(1, int(round(np.mean(best_epochs)))+1):\n",
    "        final_model.train()\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            pred = final_model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "    # Predict\n",
    "    final_model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb in DataLoader(SleepDS(X_test), batch_size=BATCH_SIZE):\n",
    "            out = final_model(xb.to(DEVICE))\n",
    "            preds.extend(out.argmax(1).cpu().numpy())\n",
    "    preds_dict[target] = preds\n",
    "\n",
    "# ------------------------\n",
    "# Submission\n",
    "# ------------------------\n",
    "sub = testset[['subject_id','sleep_date','lifelog_date']].copy()\n",
    "for t in TARGETS:\n",
    "    sub[t] = preds_dict[t]\n",
    "\n",
    "SAVE_PATH = '../data/submission2.csv'\n",
    "sub.to_csv(SAVE_PATH, index=False)\n",
    "print('✅ submission saved ->', SAVE_PATH)\n",
    "\n",
    "# 85m 54.7s for 5 folds, 50 epochs each"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d68a23",
   "metadata": {},
   "source": [
    "# GRU + OPTUNA\n",
    "- n_trials = 20, FOLD = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e7e0a159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 14:58:00,343] A new study created in memory with name: no-name-20fccd60-f1f4-4777-9800-584b7dc6b52c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing Q1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 15:03:36,217] Trial 0 finished with value: 0.4808357797731812 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.0048856791494689585, 'wd': 0.0002509962492049059, 'epochs': 30}. Best is trial 0 with value: 0.4808357797731812.\n",
      "[I 2025-06-05 15:08:40,293] Trial 1 finished with value: 0.5023608089565808 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.1, 'lr': 0.0008476317631758312, 'wd': 0.0006430327830613585, 'epochs': 30}. Best is trial 1 with value: 0.5023608089565808.\n",
      "[I 2025-06-05 15:10:16,883] Trial 2 finished with value: 0.5307364925233908 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.009010815268596269, 'wd': 1.0287136692431994e-05, 'epochs': 10}. Best is trial 2 with value: 0.5307364925233908.\n",
      "[I 2025-06-05 15:13:37,058] Trial 3 finished with value: 0.49174575264657194 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.1, 'lr': 0.0022538037118753278, 'wd': 0.00022761365993591062, 'epochs': 20}. Best is trial 2 with value: 0.5307364925233908.\n",
      "[I 2025-06-05 15:16:32,232] Trial 4 finished with value: 0.5341075305051438 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.0068312213206005905, 'wd': 0.00028454038274411793, 'epochs': 20}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:21:34,707] Trial 5 finished with value: 0.5211160525694486 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.1, 'lr': 0.0005998518306570217, 'wd': 0.0047529405655691945, 'epochs': 40}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:26:30,211] Trial 6 finished with value: 0.5254333187187612 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.4, 'lr': 0.005115026512591019, 'wd': 0.009875331905049698, 'epochs': 40}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:30:18,181] Trial 7 finished with value: 0.49751101076766313 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.00307443524144169, 'wd': 0.001978859616131926, 'epochs': 30}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:32:32,362] Trial 8 finished with value: 0.5013194043931077 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.2, 'lr': 0.0002868345007743179, 'wd': 0.00012703785731662748, 'epochs': 30}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:33:57,744] Trial 9 finished with value: 0.42754655670711134 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.5, 'lr': 0.00012555224461962772, 'wd': 2.8244981373552595e-05, 'epochs': 20}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:34:47,461] Trial 10 finished with value: 0.519440878208299 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.2, 'lr': 0.002094251549844852, 'wd': 5.717118321031822e-05, 'epochs': 10}. Best is trial 4 with value: 0.5341075305051438.\n",
      "[I 2025-06-05 15:35:37,414] Trial 11 finished with value: 0.566690939321625 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009939767427754764, 'wd': 1.0342806430569786e-05, 'epochs': 10}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:36:21,585] Trial 12 finished with value: 0.48286876492141867 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009008203406228519, 'wd': 0.0009537410903739049, 'epochs': 10}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:38:19,724] Trial 13 finished with value: 0.5255616544610315 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.2, 'lr': 0.004686679274759764, 'wd': 1.0032644547099744e-05, 'epochs': 20}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:39:45,499] Trial 14 finished with value: 0.4909413048208872 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.4, 'lr': 0.0012522852649265543, 'wd': 5.046751541215302e-05, 'epochs': 10}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:42:17,019] Trial 15 finished with value: 0.515789450389206 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009529648446940136, 'wd': 0.0001048774352711117, 'epochs': 20}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:43:34,880] Trial 16 finished with value: 0.5178651516084368 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.4, 'lr': 0.003676926137834878, 'wd': 2.291739222074526e-05, 'epochs': 10}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:45:57,645] Trial 17 finished with value: 0.5252268948883971 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.1, 'lr': 0.0015582915771632652, 'wd': 0.0007680020559037569, 'epochs': 20}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:47:05,900] Trial 18 finished with value: 0.4880241665805124 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.2, 'lr': 0.00034946945942566335, 'wd': 0.0017079488094460821, 'epochs': 10}. Best is trial 11 with value: 0.566690939321625.\n",
      "[I 2025-06-05 15:49:23,155] Trial 19 finished with value: 0.5343237713002907 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.006548933188894804, 'wd': 0.00048408870556890986, 'epochs': 20}. Best is trial 11 with value: 0.566690939321625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009939767427754764, 'wd': 1.0342806430569786e-05, 'epochs': 10} Best F1 -> 0.566690939321625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 15:49:38,943] A new study created in memory with name: no-name-65a592bf-16e3-48e8-8755-905ec321c4d9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing Q2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 15:51:05,760] Trial 0 finished with value: 0.4930097328179969 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.5, 'lr': 0.0004606657125303683, 'wd': 0.00020232758057082577, 'epochs': 10}. Best is trial 0 with value: 0.4930097328179969.\n",
      "[I 2025-06-05 15:55:32,285] Trial 1 finished with value: 0.4958816499489801 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.4, 'lr': 0.000863341771774417, 'wd': 1.3192290066940933e-05, 'epochs': 30}. Best is trial 1 with value: 0.4958816499489801.\n",
      "[I 2025-06-05 15:58:31,840] Trial 2 finished with value: 0.5062630339388019 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.2, 'lr': 0.00013624267312673752, 'wd': 0.0001845821644537221, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:05:07,421] Trial 3 finished with value: 0.44245446159071256 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.0009804415974283619, 'wd': 0.00020808405673901893, 'epochs': 40}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:11:27,628] Trial 4 finished with value: 0.46934306466607223 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.2, 'lr': 0.00022287826204727947, 'wd': 0.0025899233822752905, 'epochs': 40}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:14:17,780] Trial 5 finished with value: 0.5022363877910204 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.1, 'lr': 0.000750424345687127, 'wd': 1.9685689698577057e-05, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:15:47,893] Trial 6 finished with value: 0.5043700286186199 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.2, 'lr': 0.00013929780624843033, 'wd': 0.0019229194255912705, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:18:33,491] Trial 7 finished with value: 0.44356213119224863 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.2, 'lr': 0.00020017785101565853, 'wd': 0.0001328081681308049, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:20:11,222] Trial 8 finished with value: 0.4976988466310173 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.4, 'lr': 0.00019209260042307564, 'wd': 5.9438486473458693e-05, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:26:13,728] Trial 9 finished with value: 0.45038673247766736 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.0007208495121026538, 'wd': 4.629527642371525e-05, 'epochs': 40}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:30:59,749] Trial 10 finished with value: 0.5035586988970182 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.1, 'lr': 0.004154714318331942, 'wd': 0.0008261555476202101, 'epochs': 30}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:32:35,525] Trial 11 finished with value: 0.44932021801545946 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.00011343965456565525, 'wd': 0.008126505423044487, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:35:03,725] Trial 12 finished with value: 0.4600178609859816 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.2, 'lr': 0.003118094546229714, 'wd': 0.000762592518451039, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:36:13,175] Trial 13 finished with value: 0.4718432751345302 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.1, 'lr': 0.00010393396011615559, 'wd': 0.0006851247373323263, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:38:26,584] Trial 14 finished with value: 0.4819840787292943 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.009426021997806771, 'wd': 0.00412256684403468, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:41:46,790] Trial 15 finished with value: 0.42687619486019396 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.00036836387520766564, 'wd': 0.0016121910118054648, 'epochs': 30}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:43:07,737] Trial 16 finished with value: 0.5020628814503996 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.4, 'lr': 0.001752699950519281, 'wd': 8.108442188247164e-05, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:46:01,616] Trial 17 finished with value: 0.48337080024317414 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.1, 'lr': 0.00036476387310484785, 'wd': 0.0004349406904609417, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:47:28,200] Trial 18 finished with value: 0.470572654568515 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.2, 'lr': 0.00015183059837115242, 'wd': 0.009996196285416079, 'epochs': 10}. Best is trial 2 with value: 0.5062630339388019.\n",
      "[I 2025-06-05 16:49:22,239] Trial 19 finished with value: 0.5007826080447446 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.0003140251144522845, 'wd': 0.0015887488753956148, 'epochs': 20}. Best is trial 2 with value: 0.5062630339388019.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 128, 'layers': 3, 'drop': 0.2, 'lr': 0.00013624267312673752, 'wd': 0.0001845821644537221, 'epochs': 20} Best F1 -> 0.5062630339388019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 16:49:49,849] A new study created in memory with name: no-name-77720886-db5e-4068-9e69-155e9a6a04e1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing Q3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 16:52:26,585] Trial 0 finished with value: 0.46799626004728223 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.4, 'lr': 0.009080997950698, 'wd': 8.537882259216108e-05, 'epochs': 20}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 16:55:02,219] Trial 1 finished with value: 0.40356325599378584 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.2, 'lr': 0.0001994055087091531, 'wd': 0.002684431121309825, 'epochs': 20}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 16:59:02,143] Trial 2 finished with value: 0.4505269941725816 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.2, 'lr': 0.0001740202704426324, 'wd': 0.006713460894859095, 'epochs': 40}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:00:32,817] Trial 3 finished with value: 0.4090435392927427 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.4, 'lr': 0.0011070428509169143, 'wd': 0.00010452197025502075, 'epochs': 10}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:08:14,615] Trial 4 finished with value: 0.43372673393082284 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.2, 'lr': 0.0003113958044443368, 'wd': 7.13021628740137e-05, 'epochs': 40}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:11:58,652] Trial 5 finished with value: 0.4360727390391751 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.00021808067700499297, 'wd': 0.000678643647510105, 'epochs': 30}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:16:18,865] Trial 6 finished with value: 0.46171518027877506 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.1, 'lr': 0.0008770844958873253, 'wd': 1.9521962912220023e-05, 'epochs': 30}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:18:49,078] Trial 7 finished with value: 0.46097728429706136 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.1, 'lr': 0.001010739715015104, 'wd': 0.0018621707460517748, 'epochs': 10}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:21:24,357] Trial 8 finished with value: 0.45527415659139425 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.003613747155688358, 'wd': 0.00022038687616054612, 'epochs': 10}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:25:59,897] Trial 9 finished with value: 0.4489193812128067 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.4, 'lr': 0.00014366282528875383, 'wd': 1.0254171272559129e-05, 'epochs': 20}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:28:59,985] Trial 10 finished with value: 0.4196815258204601 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.5, 'lr': 0.009690422328874286, 'wd': 5.520047793754556e-05, 'epochs': 20}. Best is trial 0 with value: 0.46799626004728223.\n",
      "[I 2025-06-05 17:33:40,512] Trial 11 finished with value: 0.5131683451272998 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.002536242954103861, 'wd': 1.250607484415822e-05, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 17:39:40,163] Trial 12 finished with value: 0.48690402476780187 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.5, 'lr': 0.009051138178547592, 'wd': 2.7928130516484082e-05, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 17:49:19,121] Trial 13 finished with value: 0.4624635446994521 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.5, 'lr': 0.003653909556991212, 'wd': 2.4151521344449717e-05, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 17:55:51,710] Trial 14 finished with value: 0.46633468428314284 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.1, 'lr': 0.0038987122409574835, 'wd': 2.7661676579560814e-05, 'epochs': 40}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 17:59:06,927] Trial 15 finished with value: 0.45681564986885004 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.5, 'lr': 0.0022189013723928766, 'wd': 1.266844561463749e-05, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 18:01:57,494] Trial 16 finished with value: 0.481132453009155 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.005147344041436291, 'wd': 0.0002912985118719145, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 18:06:27,949] Trial 17 finished with value: 0.4720459559664715 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.2, 'lr': 0.0019225570136569027, 'wd': 3.61452605142999e-05, 'epochs': 40}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 18:11:04,680] Trial 18 finished with value: 0.4538544011691924 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.4, 'lr': 0.0005104392657462023, 'wd': 0.0001690877958083296, 'epochs': 30}. Best is trial 11 with value: 0.5131683451272998.\n",
      "[I 2025-06-05 18:14:44,031] Trial 19 finished with value: 0.47311429262355864 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.006500232351498446, 'wd': 0.0008845732712426141, 'epochs': 20}. Best is trial 11 with value: 0.5131683451272998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.002536242954103861, 'wd': 1.250607484415822e-05, 'epochs': 30} Best F1 -> 0.5131683451272998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 18:16:07,445] A new study created in memory with name: no-name-954392a2-c89f-4248-9491-525ee539c9dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing S1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 18:19:52,753] Trial 0 finished with value: 0.34024242872965627 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.4, 'lr': 0.0008745733512930146, 'wd': 0.00843908855446404, 'epochs': 20}. Best is trial 0 with value: 0.34024242872965627.\n",
      "[I 2025-06-05 18:25:46,911] Trial 1 finished with value: 0.3402171021387113 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.5, 'lr': 0.005374640860659699, 'wd': 0.0002544092942471286, 'epochs': 40}. Best is trial 0 with value: 0.34024242872965627.\n",
      "[I 2025-06-05 18:31:19,633] Trial 2 finished with value: 0.2940094296146226 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.4, 'lr': 0.0002377692583660246, 'wd': 2.226396220035422e-05, 'epochs': 40}. Best is trial 0 with value: 0.34024242872965627.\n",
      "[I 2025-06-05 18:33:56,965] Trial 3 finished with value: 0.3262898069501778 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.5, 'lr': 0.002535431772853788, 'wd': 0.000156957184280679, 'epochs': 20}. Best is trial 0 with value: 0.34024242872965627.\n",
      "[I 2025-06-05 18:36:45,714] Trial 4 finished with value: 0.32339650282496546 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.5, 'lr': 0.0001883967443909165, 'wd': 8.210248873392553e-05, 'epochs': 20}. Best is trial 0 with value: 0.34024242872965627.\n",
      "[I 2025-06-05 18:41:01,303] Trial 5 finished with value: 0.3723131089178561 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.5, 'lr': 0.0009475669863072139, 'wd': 0.00022215642109565018, 'epochs': 30}. Best is trial 5 with value: 0.3723131089178561.\n",
      "[I 2025-06-05 18:44:55,463] Trial 6 finished with value: 0.35425071221159343 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.006496758471900205, 'wd': 0.0006176774517422034, 'epochs': 30}. Best is trial 5 with value: 0.3723131089178561.\n",
      "[I 2025-06-05 18:45:46,999] Trial 7 finished with value: 0.3801351987973203 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.0021564326902317305, 'wd': 2.087515700664565e-05, 'epochs': 10}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:49:39,892] Trial 8 finished with value: 0.33201277089595965 and parameters: {'hidden': 64, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.0004567547119003751, 'wd': 4.538151180092656e-05, 'epochs': 30}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:50:45,290] Trial 9 finished with value: 0.3248528345225949 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009649274981263087, 'wd': 0.00031723709300158224, 'epochs': 10}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:52:01,040] Trial 10 finished with value: 0.3432635871318773 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.0024741042163439796, 'wd': 1.0500020600494422e-05, 'epochs': 10}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:55:28,231] Trial 11 finished with value: 0.31601734855050573 and parameters: {'hidden': 192, 'layers': 1, 'drop': 0.2, 'lr': 0.0013506709434941949, 'wd': 0.0020141209466610524, 'epochs': 30}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:56:19,035] Trial 12 finished with value: 0.35327102310670877 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.0007189120782136887, 'wd': 0.0011971949438678504, 'epochs': 10}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 18:59:49,156] Trial 13 finished with value: 0.3257122064375111 and parameters: {'hidden': 192, 'layers': 1, 'drop': 0.2, 'lr': 0.0019759459341047647, 'wd': 4.295297851972597e-05, 'epochs': 30}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:02:26,110] Trial 14 finished with value: 0.2800459647399851 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.4, 'lr': 0.0004344685059538779, 'wd': 1.4721828467119595e-05, 'epochs': 20}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:09:11,363] Trial 15 finished with value: 0.3563710439422818 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.003847545070984336, 'wd': 8.728266172074644e-05, 'epochs': 40}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:10:54,747] Trial 16 finished with value: 0.32881262905677644 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.00010169427997469858, 'wd': 0.0036967399633450965, 'epochs': 10}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:14:12,691] Trial 17 finished with value: 0.320209975410157 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.4, 'lr': 0.0011329800774629122, 'wd': 0.0005913840694602707, 'epochs': 20}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:18:31,997] Trial 18 finished with value: 0.3190872415554261 and parameters: {'hidden': 128, 'layers': 1, 'drop': 0.2, 'lr': 0.0005650219913508794, 'wd': 2.7922275557528396e-05, 'epochs': 30}. Best is trial 7 with value: 0.3801351987973203.\n",
      "[I 2025-06-05 19:25:55,331] Trial 19 finished with value: 0.34515489218005974 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.0014705615186922124, 'wd': 0.00010202065178118587, 'epochs': 40}. Best is trial 7 with value: 0.3801351987973203.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 192, 'layers': 2, 'drop': 0.1, 'lr': 0.0021564326902317305, 'wd': 2.087515700664565e-05, 'epochs': 10} Best F1 -> 0.3801351987973203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 19:26:17,052] A new study created in memory with name: no-name-c15df6c9-0a1f-4d28-977c-fbfba3705a1b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing S2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 19:30:50,000] Trial 0 finished with value: 0.5320869171639158 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.5, 'lr': 0.0027480829123465327, 'wd': 0.0003239780262680119, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 19:41:29,220] Trial 1 finished with value: 0.5241168663273127 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.5, 'lr': 0.00031164807491248435, 'wd': 6.395895715946433e-05, 'epochs': 40}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 19:44:16,869] Trial 2 finished with value: 0.5039539355190009 and parameters: {'hidden': 64, 'layers': 1, 'drop': 0.30000000000000004, 'lr': 0.00048628492929025496, 'wd': 1.1949690969130588e-05, 'epochs': 20}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 19:53:14,674] Trial 3 finished with value: 0.506178446745453 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.2, 'lr': 0.003866686069963752, 'wd': 0.00048645062418645624, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 19:59:39,308] Trial 4 finished with value: 0.48848531580837895 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.2, 'lr': 0.0017613201944319781, 'wd': 6.858972124931173e-05, 'epochs': 20}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:05:43,098] Trial 5 finished with value: 0.4873391457097099 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.1, 'lr': 0.00025678585096562635, 'wd': 0.006943959511719913, 'epochs': 20}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:16:39,332] Trial 6 finished with value: 0.47555921470307594 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.1, 'lr': 0.0011537958587718997, 'wd': 0.008792997961568304, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:24:53,092] Trial 7 finished with value: 0.4878042088877391 and parameters: {'hidden': 192, 'layers': 1, 'drop': 0.5, 'lr': 0.0003355147531704863, 'wd': 0.00025139965791548703, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:33:33,080] Trial 8 finished with value: 0.4701832576703124 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.1, 'lr': 0.00132211817485878, 'wd': 0.0002609195983420934, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:39:18,940] Trial 9 finished with value: 0.5299497858663077 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.5, 'lr': 0.0037628563054313887, 'wd': 3.107103509790935e-05, 'epochs': 30}. Best is trial 0 with value: 0.5320869171639158.\n",
      "[I 2025-06-05 20:40:55,904] Trial 10 finished with value: 0.5382710499714908 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.4, 'lr': 0.009496203492354464, 'wd': 0.0014535062380554168, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:42:23,818] Trial 11 finished with value: 0.5138618785648282 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.4, 'lr': 0.008057634537052228, 'wd': 0.0014611219731339028, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:43:38,756] Trial 12 finished with value: 0.49372189137751743 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.4, 'lr': 0.009320677709866065, 'wd': 0.0012706821976177147, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:51:12,193] Trial 13 finished with value: 0.4568210895612116 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.4, 'lr': 0.00010054688561495923, 'wd': 0.0018677814906783222, 'epochs': 40}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:52:59,764] Trial 14 finished with value: 0.5353043532489268 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.4, 'lr': 0.0035953530259885606, 'wd': 0.0006964937177259822, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:53:37,546] Trial 15 finished with value: 0.5244732662238044 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.005993213970322092, 'wd': 0.0033535539355548623, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:53:39,509] Trial 16 finished with value: 0.5200486943337727 and parameters: {'hidden': 128, 'layers': 2, 'drop': 0.4, 'lr': 0.004992845739579307, 'wd': 0.0006754745579618523, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:53:43,191] Trial 17 finished with value: 0.47984951107223905 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.0022477155905620597, 'wd': 0.00011273302135389284, 'epochs': 20}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:53:45,714] Trial 18 finished with value: 0.5271923274513386 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.4, 'lr': 0.0007055166697143163, 'wd': 0.003468719155152067, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n",
      "[I 2025-06-05 20:53:53,803] Trial 19 finished with value: 0.48924144505105244 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.009562013900894582, 'wd': 0.0008088043057758354, 'epochs': 10}. Best is trial 10 with value: 0.5382710499714908.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 64, 'layers': 2, 'drop': 0.4, 'lr': 0.009496203492354464, 'wd': 0.0014535062380554168, 'epochs': 10} Best F1 -> 0.5382710499714908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 20:53:54,707] A new study created in memory with name: no-name-688ddabb-dcbb-4d0c-a398-1d9737eb2741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Optimizing S3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-05 20:54:22,306] Trial 0 finished with value: 0.44799479877004983 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.5, 'lr': 0.0004727264082242207, 'wd': 0.0011531906565245898, 'epochs': 40}. Best is trial 0 with value: 0.44799479877004983.\n",
      "[I 2025-06-05 20:54:42,299] Trial 1 finished with value: 0.4261338030618921 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.5, 'lr': 0.0013206701628361622, 'wd': 0.0003957140996103862, 'epochs': 40}. Best is trial 0 with value: 0.44799479877004983.\n",
      "[I 2025-06-05 20:54:53,687] Trial 2 finished with value: 0.4282468332344698 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.2, 'lr': 0.0003014516478996842, 'wd': 0.005816372451915116, 'epochs': 10}. Best is trial 0 with value: 0.44799479877004983.\n",
      "[I 2025-06-05 20:55:43,911] Trial 3 finished with value: 0.5125244467688084 and parameters: {'hidden': 256, 'layers': 3, 'drop': 0.1, 'lr': 0.00015771374640461902, 'wd': 0.004076354233663116, 'epochs': 40}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:56:14,412] Trial 4 finished with value: 0.45255839063573927 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.2, 'lr': 0.00053431554584508, 'wd': 0.001268790592294575, 'epochs': 40}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:56:41,100] Trial 5 finished with value: 0.4277852180350738 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.4, 'lr': 0.0006124423328338264, 'wd': 1.2153474144484214e-05, 'epochs': 30}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:56:56,348] Trial 6 finished with value: 0.450743254635068 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.1, 'lr': 0.00011967358964162862, 'wd': 0.00450686746112744, 'epochs': 30}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:57:18,036] Trial 7 finished with value: 0.4362523059184669 and parameters: {'hidden': 192, 'layers': 1, 'drop': 0.5, 'lr': 0.0016658226603844081, 'wd': 0.0001242606039011124, 'epochs': 30}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:57:26,919] Trial 8 finished with value: 0.42814481121139786 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.00030758751439985106, 'wd': 0.0009030642759958137, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:57:46,705] Trial 9 finished with value: 0.40113439984467314 and parameters: {'hidden': 128, 'layers': 3, 'drop': 0.30000000000000004, 'lr': 0.00021526511133470854, 'wd': 4.544475719955397e-05, 'epochs': 40}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:58:07,652] Trial 10 finished with value: 0.5112053819979343 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.006511797339874017, 'wd': 0.009777411914283152, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:58:26,409] Trial 11 finished with value: 0.45227982961777435 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.007747228244184128, 'wd': 0.009633752630336516, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:58:38,306] Trial 12 finished with value: 0.3521922935600212 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.00807463558167706, 'wd': 0.003434693398579877, 'epochs': 10}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:58:58,780] Trial 13 finished with value: 0.49303506098566874 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.00337428259054021, 'wd': 0.002152497076784396, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:59:17,908] Trial 14 finished with value: 0.4327885463946647 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.1, 'lr': 0.003313854826166836, 'wd': 0.0003747143474934126, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:59:27,806] Trial 15 finished with value: 0.4342947136768647 and parameters: {'hidden': 64, 'layers': 2, 'drop': 0.2, 'lr': 0.00011195630474758887, 'wd': 0.009926975647839599, 'epochs': 30}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 20:59:38,698] Trial 16 finished with value: 0.39262592903843646 and parameters: {'hidden': 256, 'layers': 2, 'drop': 0.1, 'lr': 0.0025526802552400295, 'wd': 0.002159066482024714, 'epochs': 10}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 21:00:06,434] Trial 17 finished with value: 0.4472137762386062 and parameters: {'hidden': 192, 'layers': 3, 'drop': 0.4, 'lr': 0.0009167228385626254, 'wd': 9.351087388691435e-05, 'epochs': 30}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 21:00:19,752] Trial 18 finished with value: 0.5024746891149433 and parameters: {'hidden': 256, 'layers': 1, 'drop': 0.2, 'lr': 0.004986761759662596, 'wd': 0.0007127587690328697, 'epochs': 20}. Best is trial 3 with value: 0.5125244467688084.\n",
      "[I 2025-06-05 21:00:27,234] Trial 19 finished with value: 0.3870909050705931 and parameters: {'hidden': 192, 'layers': 2, 'drop': 0.30000000000000004, 'lr': 0.00018147496140618404, 'wd': 0.0026637904275460204, 'epochs': 10}. Best is trial 3 with value: 0.5125244467688084.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params -> {'hidden': 256, 'layers': 3, 'drop': 0.1, 'lr': 0.00015771374640461902, 'wd': 0.004076354233663116, 'epochs': 40} Best F1 -> 0.5125244467688084\n",
      "✅ submission saved -> ../data/submission_optuna.csv\n"
     ]
    }
   ],
   "source": [
    "# 🚀 GRU with Optuna Hyperparameter Tuning (per‑target)\n",
    "# - Tune hidden size, layers, dropout, learning rate, weight decay, epochs\n",
    "# - Use GroupKFold inside Optuna objective (validation F1 ↑)\n",
    "# - Train final model with best params, predict test, save submission.csv\n",
    "# Requirements: pip install optuna\n",
    "\n",
    "import os, copy, random, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import optuna                          # ✨ NEW\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "trainset = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "testset  = pd.read_csv('../data/ch2025_submission_sample.csv')\n",
    "\n",
    "TARGETS = ['Q1','Q2','Q3','S1','S2','S3']\n",
    "\n",
    "df_zero_filled['timestamp']   = pd.to_datetime(df_zero_filled['timestamp'])\n",
    "df_zero_filled['lifelog_date'] = df_zero_filled['timestamp'].dt.date.astype(str)\n",
    "\n",
    "DROP_COLS   = ['timestamp','subject_id','lifelog_date']\n",
    "SENSOR_COLS = [c for c in df_zero_filled.columns if c not in DROP_COLS]\n",
    "MAX_SEQ     = 144   # 10‑min resolution\n",
    "\n",
    "# ---------- utils ----------\n",
    "\n",
    "def build_sequences(df):\n",
    "    seqs = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id','lifelog_date']):\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[SENSOR_COLS].astype('float32').to_numpy()\n",
    "        if len(x) > MAX_SEQ: x = x[:MAX_SEQ]\n",
    "        if len(x) < MAX_SEQ:\n",
    "            x = np.concatenate([x, np.zeros((MAX_SEQ-len(x), x.shape[1]), np.float32)])\n",
    "        seqs[(sid, day)] = x\n",
    "    return seqs\n",
    "\n",
    "SEQ_DICT = build_sequences(df_zero_filled)\n",
    "\n",
    "\n",
    "def rows_to_xy(df):\n",
    "    xs, ys, groups = [], [], []\n",
    "    for _, r in df.iterrows():\n",
    "        k = (r.subject_id, r.lifelog_date)\n",
    "        if k not in SEQ_DICT:\n",
    "            continue\n",
    "        xs.append(SEQ_DICT[k])\n",
    "        ys.append(r[TARGETS].to_list())\n",
    "        groups.append(r.subject_id)\n",
    "    return np.stack(xs), np.array(ys, np.int64), np.array(groups)\n",
    "\n",
    "X_all, y_all, group_all = rows_to_xy(trainset)\n",
    "X_test, _, _            = rows_to_xy(testset)\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler().fit(X_all.reshape(-1, X_all.shape[-1]))\n",
    "X_all  = scaler.transform(X_all.reshape(-1, X_all.shape[-1])).reshape(X_all.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "\n",
    "# ---------- dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.long)\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx] if self.y is None else (self.X[idx], self.y[idx])\n",
    "\n",
    "# ---------- model ----------\n",
    "class SingleHeadGRU(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim, hidden, layers, drop):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(inp_dim, hidden, layers, batch_first=True, dropout=drop)\n",
    "        self.fc  = nn.Linear(hidden, out_dim)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)\n",
    "        return self.fc(h[-1])\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "BATCH_SIZE_DEFAULT = 64\n",
    "N_FOLD = 5\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Optuna tuning per target\n",
    "# --------------------------------------------------\n",
    "preds_dict = {}\n",
    "\n",
    "for idx_target, target in enumerate(TARGETS):\n",
    "    print(f\"\\n🎯 Optimizing {target}\")\n",
    "    y_target = y_all[:, idx_target]\n",
    "    out_dim  = 3 if target == 'S1' else 2\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def objective(trial):\n",
    "        # hyperparameters to tune\n",
    "        hidden = trial.suggest_int('hidden', 64, 256, step=64)\n",
    "        layers = trial.suggest_int('layers', 1, 3)\n",
    "        drop   = trial.suggest_float('drop', 0.1, 0.5, step=0.1)\n",
    "        lr     = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
    "        wd     = trial.suggest_loguniform('wd', 1e-5, 1e-2)\n",
    "        epochs = trial.suggest_int('epochs', 10, 40, step=10)\n",
    "\n",
    "        gkf = GroupKFold(n_splits=N_FOLD)\n",
    "        f1_scores = []\n",
    "\n",
    "        for tr_idx, val_idx in gkf.split(X_all, y_target, group_all):\n",
    "            model = SingleHeadGRU(X_all.shape[-1], out_dim, hidden, layers, drop).to(DEVICE)\n",
    "            opt   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "            tr_loader  = DataLoader(SleepDS(X_all[tr_idx], y_target[tr_idx]), batch_size=BATCH_SIZE_DEFAULT, shuffle=True)\n",
    "            val_loader = DataLoader(SleepDS(X_all[val_idx], y_target[val_idx]), batch_size=BATCH_SIZE_DEFAULT)\n",
    "\n",
    "            # training loop\n",
    "            for epoch in range(epochs):\n",
    "                model.train()\n",
    "                for xb, yb in tr_loader:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "                    loss = criterion(model(xb), yb)\n",
    "                    opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "            # validation\n",
    "            model.eval(); y_true, y_pred = [], []\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_loader:\n",
    "                    preds = model(xb.to(DEVICE)).argmax(1).cpu().numpy()\n",
    "                    y_pred.extend(preds); y_true.extend(yb.numpy())\n",
    "            f1_scores.append(f1_score(y_true, y_pred, average='macro'))\n",
    "\n",
    "        return float(np.mean(f1_scores))\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=20, timeout=None)  # 🕑 adjust n_trials\n",
    "    best_params = study.best_params\n",
    "    print('Best params ->', best_params, 'Best F1 ->', study.best_value)\n",
    "\n",
    "    # ---------- train final model with best params ----------\n",
    "    hidden = best_params['hidden']\n",
    "    layers = best_params['layers']\n",
    "    drop   = best_params['drop']\n",
    "    lr     = best_params['lr']\n",
    "    wd     = best_params['wd']\n",
    "    epochs = best_params['epochs']\n",
    "\n",
    "    model_final = SingleHeadGRU(X_all.shape[-1], out_dim, hidden, layers, drop).to(DEVICE)\n",
    "    opt_final   = torch.optim.AdamW(model_final.parameters(), lr=lr, weight_decay=wd)\n",
    "    full_loader = DataLoader(SleepDS(X_all, y_target), batch_size=BATCH_SIZE_DEFAULT, shuffle=True)\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model_final.train()\n",
    "        for xb, yb in full_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            loss = criterion(model_final(xb), yb)\n",
    "            opt_final.zero_grad(); loss.backward(); opt_final.step()\n",
    "\n",
    "    # ---------- predict test ----------\n",
    "    model_final.eval(); preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb in DataLoader(SleepDS(X_test), batch_size=BATCH_SIZE_DEFAULT):\n",
    "            preds.extend(model_final(xb.to(DEVICE)).argmax(1).cpu().numpy())\n",
    "    preds_dict[target] = preds\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Build submission\n",
    "# --------------------------------------------------\n",
    "sub = testset[['subject_id','sleep_date','lifelog_date']].copy()\n",
    "for t in TARGETS:\n",
    "    sub[t] = preds_dict[t]\n",
    "\n",
    "SAVE_PATH = '../data/submission_optuna.csv'\n",
    "sub.to_csv(SAVE_PATH, index=False)\n",
    "print('✅ submission saved ->', SAVE_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wonjun_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
