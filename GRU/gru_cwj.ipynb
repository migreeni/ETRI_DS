{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d155b048",
   "metadata": {},
   "source": [
    "# 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44915715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ merged_df_cwj.csv 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv('../data/merged_df_original.csv')\n",
    "\n",
    "# '캐시워크'부터 '우체국보험'까지의 열 이름 목록 추출\n",
    "start_col = '캐시워크'\n",
    "end_col = '우체국보험'\n",
    "cols_to_sum = df.loc[:, start_col:end_col].columns\n",
    "\n",
    "df['m_usagestats'] = df[cols_to_sum].sum(axis=1)\n",
    "\n",
    "df.drop(columns=cols_to_sum, inplace=True)\n",
    "\n",
    "merged_df_cwj = df.copy()\n",
    "\n",
    "merged_df_cwj.rename(columns={'m_wtb_rssi_x': 'm_wtb_rssi', 'm_wtb_rssi_y': 'm_wtw_rssi'}, inplace=True)\n",
    "\n",
    "# CSV로 내보내기\n",
    "merged_df_cwj.to_csv('../data/merged_df_cwj.csv', index=False)\n",
    "\n",
    "\n",
    "print(\"✅ merged_df_cwj.csv 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad28eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 결측치 개수:\n",
      "met_activity     2690\n",
      "m_wtb_rssi      77337\n",
      "m_wtw_rssi      22842\n",
      "heart_rate      55846\n",
      "distance        22852\n",
      "latitude        16044\n",
      "longitude       16044\n",
      "altitude        16044\n",
      "speed           16044\n",
      "m_usagestats        0\n",
      "w_light         19040\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/merged_df_cwj.csv')\n",
    "\n",
    "# 검사할 열 목록\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# 각 열의 결측치 개수 출력\n",
    "missing_counts = df[cols_to_check].isnull().sum()\n",
    "print(\"📊 결측치 개수:\")\n",
    "print(missing_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157bbe6",
   "metadata": {},
   "source": [
    "### 결측치를 가장 가까운 이웃의 값(앞뒤 기준)으로 채움"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cce709a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 결측치 보간 완료. 남은 결측치 수:\n",
      "met_activity    0\n",
      "m_wtb_rssi      0\n",
      "m_wtw_rssi      0\n",
      "heart_rate      0\n",
      "distance        0\n",
      "latitude        0\n",
      "longitude       0\n",
      "altitude        0\n",
      "speed           0\n",
      "m_usagestats    0\n",
      "w_light         0\n",
      "dtype: int64\n",
      "📁 '../data/merged_df_cwj_filled.csv' 저장 완료\n"
     ]
    }
   ],
   "source": [
    "#대상 열 목록\n",
    "cols_to_check = [\n",
    "    'met_activity', 'm_wtb_rssi', 'm_wtw_rssi', 'heart_rate',\n",
    "    'distance', 'latitude', 'longitude', 'altitude', 'speed', 'm_usagestats', 'w_light'\n",
    "]\n",
    "\n",
    "# 결측치 보간 함수 (앞뒤 평균)\n",
    "def fill_nearest_avg(series):\n",
    "    forward = series.ffill()  # 앞쪽 값으로 채우기\n",
    "    backward = series.bfill()  # 뒤쪽 값으로 채우기\n",
    "    filled = series.copy()\n",
    "    \n",
    "    # 앞뒤 값이 모두 있는 경우 평균으로\n",
    "    for i in series[series.isnull()].index:\n",
    "        f, b = forward[i], backward[i]\n",
    "        if pd.notnull(f) and pd.notnull(b):\n",
    "            filled[i] = (f + b) / 2\n",
    "        elif pd.notnull(f):\n",
    "            filled[i] = f\n",
    "        elif pd.notnull(b):\n",
    "            filled[i] = b\n",
    "    return filled\n",
    "\n",
    "# 'burned_calories' 열의 결측치를 0으로 채우기\n",
    "df['burned_calories'] = df['burned_calories'].fillna(0)\n",
    "\n",
    "# 각 열에 대해 결측치 처리\n",
    "for col in cols_to_check:\n",
    "    df[col] = fill_nearest_avg(df[col])\n",
    "\n",
    "# 확인용 출력 (선택사항)\n",
    "print(\"✅ 결측치 보간 완료. 남은 결측치 수:\")\n",
    "print(df[cols_to_check].isnull().sum())\n",
    "\n",
    "# 저장\n",
    "df.to_csv('../data/merged_df_cwj_filled.csv', index=False)\n",
    "print(\"📁 '../data/merged_df_cwj_filled.csv' 저장 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bda8fcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 [cols_to_check 외] 결측치가 있는 열들:\n",
      "Music                        3717\n",
      "Vehicle                      3717\n",
      "Motor vehicle (road)         3717\n",
      "Outside, urban or manmade    3717\n",
      "Outside, rural or natural    3717\n",
      "                             ... \n",
      "Heavy metal                  3717\n",
      "Double bass                  3717\n",
      "Drum and bass                3717\n",
      "String section               3717\n",
      "Punk rock                    3717\n",
      "Length: 517, dtype: int64\n",
      "✅ 결측치를 0으로 채운 파일 저장 완료: merged_df_cwj_tozero.csv\n",
      "✅ 결측치 행 삭제 파일 저장 완료: merged_df_cwj_delete.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 나머지 열들 선택\n",
    "remaining_cols = [col for col in df.columns if col not in cols_to_check]\n",
    "\n",
    "# 결측치 개수 확인\n",
    "missing_counts = df[remaining_cols].isnull().sum()\n",
    "\n",
    "# 결측치가 존재하는 열만 출력\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "print(\"📊 [cols_to_check 외] 결측치가 있는 열들:\")\n",
    "print(missing_counts)\n",
    "\n",
    "# 결측치를 0으로 채워 저장\n",
    "df_zero_filled = df.copy()\n",
    "df_zero_filled[remaining_cols] = df_zero_filled[remaining_cols].fillna(0)\n",
    "df_zero_filled.to_csv('../data/merged_df_cwj_tozero.csv', index=False)\n",
    "print(\"✅ 결측치를 0으로 채운 파일 저장 완료: merged_df_cwj_tozero.csv\")\n",
    "\n",
    "# 결측치가 있는 행을 삭제하여 저장\n",
    "df_dropna = df.copy()\n",
    "df_dropna = df_dropna.dropna(subset=remaining_cols)\n",
    "df_dropna.to_csv('../data/merged_df_cwj_delete.csv', index=False)\n",
    "print(\"✅ 결측치 행 삭제 파일 저장 완료: merged_df_cwj_delete.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67ea2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_zero_filled 결측치 총 개수: 0\n",
      "df_dropna 결측치 총 개수: 0\n",
      "(99190, 541) (95473, 541)\n"
     ]
    }
   ],
   "source": [
    "# 결측치 개수 세기\n",
    "missing_zero_filled = df_zero_filled.isnull().sum().sum()\n",
    "missing_dropna = df_dropna.isnull().sum().sum()\n",
    "\n",
    "print(f\"df_zero_filled 결측치 총 개수: {missing_zero_filled}\")\n",
    "print(f\"df_dropna 결측치 총 개수: {missing_dropna}\")\n",
    "\n",
    "print(df_zero_filled.shape, df_dropna.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bfc549",
   "metadata": {},
   "source": [
    "# train, test 셋 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14215e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_zero_filled shape: (99190, 541)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_zero_filled = pd.read_csv('../data/merged_df_cwj_tozero.csv')\n",
    "print(\"✅ df_zero_filled shape:\", df_zero_filled.shape)\n",
    "#df_zero_filled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785bf30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "24        id01  2024-07-24   2024-07-23   0   1   1   0   0   1\n",
      "17        id01  2024-07-15   2024-07-14   0   1   0   0   1   1\n",
      "66        id02  2024-08-19   2024-08-18   0   1   1   1   0   1\n",
      "148       id04  2024-09-02   2024-09-01   0   0   1   1   1   0\n",
      "249       id06  2024-07-03   2024-07-02   0   1   1   1   1   1\n",
      "31        id01  2024-08-21   2024-08-20   0   0   1   0   0   1\n",
      "84        id02  2024-09-24   2024-09-23   0   1   1   1   1   1\n",
      "307       id07  2024-08-02   2024-08-01   0   0   0   0   0   0\n",
      "406       id09  2024-08-24   2024-08-23   0   0   0   0   1   0\n",
      "389       id09  2024-07-18   2024-07-17   1   0   0   1   1   1\n",
      "\n",
      "df_val.head():\n",
      "    subject_id  sleep_date lifelog_date  Q1  Q2  Q3  S1  S2  S3\n",
      "407       id09  2024-08-25   2024-08-24   1   0   0   1   1   1\n",
      "444       id10  2024-09-03   2024-09-02   0   0   0   0   0   0\n",
      "117       id03  2024-09-07   2024-09-06   1   1   0   1   0   0\n",
      "30        id01  2024-08-20   2024-08-19   0   1   1   0   1   1\n",
      "415       id09  2024-09-03   2024-09-02   1   1   1   1   0   0\n",
      "157       id04  2024-09-17   2024-09-16   1   0   0   1   1   1\n",
      "325       id08  2024-07-01   2024-06-30   0   1   0   0   1   1\n",
      "447       id10  2024-09-09   2024-09-08   1   1   1   0   1   1\n",
      "268       id06  2024-08-11   2024-08-10   0   1   1   1   1   1\n",
      "297       id07  2024-07-07   2024-07-06   1   0   1   0   0   0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((90, 9), (360, 9))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_df = pd.read_csv('../data/ch2025_metrics_train.csv')\n",
    "\n",
    "df_train, df_val = train_test_split(train_val_df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "print(\"df_train.head():\")\n",
    "print(df_train.head(10))\n",
    "print(\"\\ndf_val.head():\")\n",
    "print(df_val.head(10))\n",
    "\n",
    "\n",
    "df_val.shape, df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e461298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   0\n",
      "\n",
      "df_val_q1 head:\n",
      "    subject_id  sleep_date lifelog_date  Q1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q2 head:\n",
      "    subject_id  sleep_date lifelog_date  Q2\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_q3 head:\n",
      "    subject_id  sleep_date lifelog_date  Q3\n",
      "407       id09  2024-08-25   2024-08-24   0\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   0\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s1 head:\n",
      "    subject_id  sleep_date lifelog_date  S1\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   1\n",
      "30        id01  2024-08-20   2024-08-19   0\n",
      "415       id09  2024-09-03   2024-09-02   1\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "24        id01  2024-07-24   2024-07-23   0\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   0\n",
      "148       id04  2024-09-02   2024-09-01   1\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s2 head:\n",
      "    subject_id  sleep_date lifelog_date  S2\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "df_train_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "24        id01  2024-07-24   2024-07-23   1\n",
      "17        id01  2024-07-15   2024-07-14   1\n",
      "66        id02  2024-08-19   2024-08-18   1\n",
      "148       id04  2024-09-02   2024-09-01   0\n",
      "249       id06  2024-07-03   2024-07-02   1\n",
      "\n",
      "df_val_s3 head:\n",
      "    subject_id  sleep_date lifelog_date  S3\n",
      "407       id09  2024-08-25   2024-08-24   1\n",
      "444       id10  2024-09-03   2024-09-02   0\n",
      "117       id03  2024-09-07   2024-09-06   0\n",
      "30        id01  2024-08-20   2024-08-19   1\n",
      "415       id09  2024-09-03   2024-09-02   0\n",
      "\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define metric columns\n",
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "# Split df_train and df_val for each metric\n",
    "for metric in metrics:\n",
    "    globals()[f'df_train_{metric.lower()}'] = df_train[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "    globals()[f'df_val_{metric.lower()}'] = df_val[['subject_id', 'sleep_date', 'lifelog_date', metric]].copy()\n",
    "\n",
    "# Display head of all 12 dataframes\n",
    "for metric in metrics:\n",
    "    print(f\"df_train_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_train_{metric.lower()}'].head())\n",
    "    print(f\"\\ndf_val_{metric.lower()} head:\")\n",
    "    print(globals()[f'df_val_{metric.lower()}'].head())\n",
    "    print(\"\\n\" + \"-\"*40 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96526449",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b98986c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# total sequences built : 700\n",
      "Train tensor shape : (360, 144, 529) (360, 1)\n",
      "Val   tensor shape : (90, 144, 529) (90, 1)\n",
      "Epoch  1 | Val macro‑F1: 0.4888\n",
      "Epoch  2 | Val macro‑F1: 0.4985\n",
      "Epoch  3 | Val macro‑F1: 0.4493\n",
      "Epoch  4 | Val macro‑F1: 0.4585\n",
      "Epoch  5 | Val macro‑F1: 0.4886\n",
      "Epoch  6 | Val macro‑F1: 0.5222\n",
      "Epoch  7 | Val macro‑F1: 0.5193\n",
      "Epoch  8 | Val macro‑F1: 0.5222\n",
      "Epoch  9 | Val macro‑F1: 0.5324\n",
      "Epoch 10 | Val macro‑F1: 0.5091\n",
      "Epoch 11 | Val macro‑F1: 0.5778\n",
      "Epoch 12 | Val macro‑F1: 0.5776\n",
      "Epoch 13 | Val macro‑F1: 0.5217\n",
      "Epoch 14 | Val macro‑F1: 0.4970\n",
      "Epoch 15 | Val macro‑F1: 0.4857\n",
      "Epoch 16 | Val macro‑F1: 0.5602\n",
      "Epoch 17 | Val macro‑F1: 0.4924\n",
      "Epoch 18 | Val macro‑F1: 0.5465\n",
      "Epoch 19 | Val macro‑F1: 0.5312\n",
      "Epoch 20 | Val macro‑F1: 0.5839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTip ‑ Hyper‑param 수정\\n----------------------\\n* hidden_size, num_layers, dropout 조정\\n* optimizer\\xa0: AdamW, lr_schedule\\n* class imbalance → pos_weight in BCELoss\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "df_merged = df_zero_filled.copy()\n",
    "# timestamp → datetime 변환\n",
    "df_merged[\"timestamp\"] = pd.to_datetime(df_merged[\"timestamp\"])\n",
    "\n",
    "# lifelog_date = 날짜(문자열) ; 2024-06-26 같은 형식\n",
    "df_merged[\"lifelog_date\"] = df_merged[\"timestamp\"].dt.date.astype(str)\n",
    "\n",
    "# -------- 2. feature / sensor column 정의 -----------\n",
    "drop_cols = [\"timestamp\",                # 시간은 예측에 불필요\n",
    "             \"subject_id\",               # 매치용\n",
    "             \"lifelog_date\"] + [c for c in df_merged.columns if c.startswith(\"id\")]  # one-hot id\n",
    "\n",
    "sensor_cols = [c for c in df_merged.columns if c not in drop_cols]\n",
    "# 하루 당 최대 144 타임스텝으로 패딩\n",
    "MAX_SEQ_LEN = 144\n",
    "\n",
    "\n",
    "# -------- 3. 시퀀스 묶는 함수 -----------\n",
    "def build_sequences(df):\n",
    "    \"\"\"(subject_id, lifelog_date) → ndarray(seq_len, n_feat)\"\"\"\n",
    "    seq_dict = {}\n",
    "    for (sid, day), g in df.groupby(['subject_id', 'lifelog_date']):\n",
    "        # 10-분 간격 보장 안될 수도 있으니 timestamp 기준 정렬\n",
    "        g = g.sort_values('timestamp')\n",
    "        x = g[sensor_cols].to_numpy(dtype=np.float32)\n",
    "\n",
    "        # 길이 조정\n",
    "        if len(x) > MAX_SEQ_LEN:          # 잘라내기\n",
    "            x = x[:MAX_SEQ_LEN]\n",
    "        if len(x) < MAX_SEQ_LEN:          # 0-패딩\n",
    "            pad = np.zeros((MAX_SEQ_LEN - len(x), x.shape[1]), np.float32)\n",
    "            x = np.vstack([x, pad])\n",
    "\n",
    "        seq_dict[(sid, day)] = x          # shape = (144, n_feat)\n",
    "    return seq_dict\n",
    "\n",
    "sequence_dict = build_sequences(df_merged)\n",
    "print(\"# total sequences built :\", len(sequence_dict))\n",
    "\n",
    "# ---------- 3. Train / Val Tensor 준비 ----------\n",
    "def rows_to_tensors(df_label):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row['Q1']])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q1)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q1)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "\n",
    "# ---------- 4. Feature 정규화 ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU 모델 ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. 학습 루프 ----------\n",
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        pred = model(xb)\n",
    "        loss = criterion(pred, yb)\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    # --- val ---\n",
    "    model.eval()\n",
    "    preds, trues = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            prob = model(xb).cpu().numpy()\n",
    "            preds.append(prob)\n",
    "            trues.append(yb.numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    trues = np.vstack(trues)\n",
    "    y_hat = (preds >= 0.5).astype(int)\n",
    "    f1 = f1_score(trues, y_hat, average='macro')\n",
    "    print(f\"Epoch {epoch:2d} | Val macro‑F1: {f1:.4f}\")\n",
    "\n",
    "\"\"\"\n",
    "Tip ‑ Hyper‑param 수정\n",
    "----------------------\n",
    "* hidden_size, num_layers, dropout 조정\n",
    "* optimizer : AdamW, lr_schedule\n",
    "* class imbalance → pos_weight in BCELoss\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b6dc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 81\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(h))    \u001b[38;5;66;03m# (B,1)\u001b[39;00m\n\u001b[1;32m     80\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 81\u001b[0m model  \u001b[38;5;241m=\u001b[39m \u001b[43mGRUModel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msensor_cols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     82\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     83\u001b[0m optim     \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/rnn.py:223\u001b[0m, in \u001b[0;36mRNNBase._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weight_refs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 223\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecurse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Resets _flat_weights\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# Note: be v. careful before removing this, as 3rd party device types\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# likely rely on this behavior to properly .to() modules like LSTM.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_flat_weights()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "metrics = ['Q1', 'Q2', 'Q3', 'S1', 'S2', 'S3']\n",
    "\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])             # 원하는 metric 컬럼 사용\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "'''\n",
    "# metric별로 train/val tensor 생성 및 shape 출력\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# ---------- 3. Train / Val Tensor 준비 ----------\n",
    "def rows_to_tensors(df_label, metric):\n",
    "    X, y = [], []\n",
    "    for _, row in df_label.iterrows():\n",
    "        key = (row['subject_id'], row['lifelog_date'])\n",
    "        if key not in sequence_dict:        # 누락된 날짜 skip\n",
    "            continue\n",
    "        X.append(sequence_dict[key])\n",
    "        y.append([row[metric]])               # binary -> shape (1,)\n",
    "    return np.stack(X), np.array(y, dtype=np.float32)\n",
    "\n",
    "X_train, y_train = rows_to_tensors(df_train_q2, metric)\n",
    "X_val,   y_val   = rows_to_tensors(df_val_q2, metric)\n",
    "\n",
    "print(\"Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "print(\"Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "'''\n",
    "\n",
    "# ---------- 4. Feature 정규화 ----------\n",
    "scaler = StandardScaler().fit(X_train.reshape(-1, len(sensor_cols)))\n",
    "def scale(x):\n",
    "    orig_shape = x.shape\n",
    "    x = scaler.transform(x.reshape(-1, len(sensor_cols)))\n",
    "    return x.reshape(orig_shape)\n",
    "\n",
    "X_train = scale(X_train)\n",
    "X_val   = scale(X_val)\n",
    "\n",
    "# ---------- 5. PyTorch Dataset ----------\n",
    "class SleepDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):  return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = SleepDS(X_train, y_train)\n",
    "val_ds   = SleepDS(X_val,   y_val)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- 6. GRU 모델 ----------\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, n_features, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=n_features, hidden_size=hidden,\n",
    "                          num_layers=1, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden, 1)\n",
    "    def forward(self, x):\n",
    "        _, h = self.gru(x)          # h: (1,B,hidden)\n",
    "        h = h.squeeze(0)            # (B,hidden)\n",
    "        return torch.sigmoid(self.fc(h))    # (B,1)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model  = GRUModel(len(sensor_cols)).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optim     = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# ---------- 7. 학습 루프 ----------\n",
    "EPOCHS = 4\n",
    "for metric in metrics:\n",
    "    train_df = globals()[f'df_train_{metric.lower()}']\n",
    "    val_df   = globals()[f'df_val_{metric.lower()}']\n",
    "    X_train, y_train = rows_to_tensors(train_df, metric)\n",
    "    X_val,   y_val   = rows_to_tensors(val_df, metric)\n",
    "    print(f\"[{metric}] Train tensor shape :\", X_train.shape, y_train.shape)\n",
    "    print(f\"[{metric}] Val   tensor shape :\", X_val.shape,   y_val.shape)\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        # --- val ---\n",
    "        model.eval()\n",
    "        preds, trues = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                prob = model(xb).cpu().numpy()\n",
    "                preds.append(prob)\n",
    "                trues.append(yb.numpy())\n",
    "        preds = np.vstack(preds)\n",
    "        trues = np.vstack(trues)\n",
    "        y_hat = (preds >= 0.5).astype(int)\n",
    "        f1 = f1_score(trues, y_hat, average='macro')\n",
    "        print(f\"Epoch {epoch:2d} | Val macro‑F1: {f1:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0fa060",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wonjun_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
